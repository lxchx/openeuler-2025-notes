### 1. 背景与痛点

人工智能领域技术迭代迅速，从业者需要持续关注最新的AI资讯、模型发布和开源工具。然而，信息源分散，手动追踪和学习耗时耗力。为了解决这一痛点，我们开发了一个智能洞察助手（AI Agent），旨在自动化地完成信息收集、分析和总结，从而提升效率。

### 2. 解决方案概述

我们利用开源项目 **Dify** 作为智能体编排与开发平台，结合在 **昇腾（Ascend）** 和 **GPU** 环境中部署的大语言模型推理服务，快速构建了一个AI洞察助手。该助手能够自动从指定信息源获取内容、进行多层次分析，并最终生成洞察报告。

### 3. 技术架构与组件

本方案的核心组件包括Dify编排层、多模型推理服务层以及底层硬件支持。所有服务均基于 **openEuler** 社区发布的软件栈构建。

**软件栈概览：**

| 组件 | 用途 | 部署环境 | 依赖的openEuler镜像 |
| :--- | :--- | :--- | :--- |
| **Dify** | AI Agent编排与工作流定义 | - | - |
| **Qwen2-7B** | 从用户需求中提取信息源链接 | 昇腾服务器 | `openEuler Llama-cpp` 镜像 |
| **Qwen2-72B** | 提取浅层网页内容 | GPU服务器 | `openEuler vLLM` 镜像 |
| **Qwen2-57B-A14B** | 深度分析与内容总结 | GPU服务器 | `openEuler S-Giant` 镜像 |

### 4. 演示步骤与工作流

以下是现场演示的操作流程，展示了从服务启动到获取最终结果的全过程。

1.  **启动依赖服务**
    *   启动 Dify 服务。
    *   在昇腾服务器上启动基于 `Llama-cpp` 的 `Qwen2-7B` 模型推理服务。
    *   在GPU服务器上分别启动基于 `vLLM` 的 `Qwen2-72B` 和基于 `S-Giant` 的 `Qwen2-57B-A14B` 模型推理服务。
    *   检查并确认所有服务均已正常运行。

2.  **执行洞察任务**
    *   登录 Dify 工作界面。
    *   导入预先编排好的工作流DSL（Domain-Specific Language）文件。
    *   触发洞察任务，Agent开始执行。

3.  **Agent 工作流解析**
    *   **步骤一：信息源获取**
        *   Agent 调用 `Qwen2-7B` 模型，根据用户输入的需求，解析并获取相关网页链接。
    *   **步骤二：分层内容提取**
        *   **浅层提取**：Agent 调用 `Qwen2-72B` 模型，访问链接并提取初步信息。
        *   **深度分析**：Agent 将初步信息交由 `Qwen2-57B-A14B` 稀疏模型进行深度的分析、归纳和总结。

4.  **查看结果**
    *   任务执行完毕后，系统自动将生成的洞察报告发送至预设邮箱。
    *   演示中，约一分钟内即可收到包含详细分析内容的邮件，验证了方案的高效性。

### 5. 资源与复现

为方便社区开发者复现和使用，本项目相关的资源已全部开源：

*   **开源内容**：
    *   Dify 工作流 DSL 文件
    *   所有模型部署的脚本
*   **获取方式**：请参考屏幕上方展示的开源链接进行访问和尝试。

通过该方案，我们展示了如何利用 Dify 和 openEuler 生态中的工具链，高效地将强大的大模型能力封装为实用的AI Agent，解决实际业务痛点。