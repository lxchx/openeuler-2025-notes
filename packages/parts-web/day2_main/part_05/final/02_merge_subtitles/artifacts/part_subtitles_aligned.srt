1
00:00:00,380 --> 00:00:10,765
刚才联盟的成立，相信一定为异构融合系统软件释放超强算力，指明方向，也能够凝聚合力。

2
00:00:11,820 --> 00:00:19,915
刚才这一部分讲的都是openEuler在超节点、在新的硬件形态下面，我们去做的改变。

3
00:00:20,650 --> 00:00:27,750
那么下面一部分是关于openEuler在AI领域里面持续深耕、不断突破，我们做的一些新的工作。

4
00:00:29,430 --> 00:00:33,390
大家都知道AI是一个大家都很关注的问题。

5
00:00:33,750 --> 00:00:40,690
刚才熊总也提到，今年的创新项目里面，有超过90%都是和AI相关的。

6
00:00:41,070 --> 00:00:46,910
那么到底openEuler在AI上做了什么？我给大家简单地也做一个回顾。

7
00:00:47,590 --> 00:01:12,420
一方面的话就是，我们看到对于openEuler来说，我们作为一个操作系统，以后操作系统的软件生态，我们认为它会向Agent去转变。所以我们整个欧拉上的软件的开发以及相关的这个栈，我们都希望能够为加速企业Agent开发去做相应的优化。

8
00:01:07,280 --> 00:01:10,480
那么我也去尝试用Dify开发了这么一个工具。

9
00:01:11,160 --> 00:01:12,495
现在给大家演示一下。

10
00:01:12,700 --> 00:01:25,060
所以在这里的话，我们从底下的Agent开发工具链，到数据治理流水线，到上面典型的行业应用Agent，我们都去做了相应的这个准备。

11
00:01:14,720 --> 00:01:16,569
首先启动Dify服务。

12
00:01:18,509 --> 00:01:20,009
OK，它已经正常启动了。

13
00:01:20,400 --> 00:01:25,455
除了使用Dify来编排这个工具以外，我们还用到了大模型的推理能力。

14
00:01:25,500 --> 00:01:37,195
那当然更重要的一点就是，我们在操作系统自身，然后如何让它去成为一个智能的服务型操作系统，这块其实也是整个行业里面大家非常关注的话题。

15
00:01:26,340 --> 00:01:35,375
刚才我已经在昇腾的服务器上和GPU环境分别拉起了模型。大屏幕上也显示了这些推理服务用到的软件栈。

16
00:01:36,210 --> 00:01:38,629
现在检查一下这些服务是否OK。

17
00:01:37,960 --> 00:01:39,825
我们openEuler在这里面做了几件事情。

18
00:01:40,610 --> 00:01:53,775
首先就是从上往下的话，我们在操作系统最关注的一些领域里面，我们通过AI加持的方式去做了进一步的调整和优化，从智能调优到运维到问答。

19
00:01:40,920 --> 00:01:44,215
好的，昇腾服务器上已经正常启动了两个大模型。

20
00:01:40,920 --> 00:01:44,215
好的，那个昇腾服务器上已经正常启动了两个大模型。

21
00:01:45,260 --> 00:01:47,585
然后再来看一下GPU环境的状态。

22
00:01:49,630 --> 00:01:50,385
也没有问题。

23
00:01:51,010 --> 00:01:53,615
那这时候就可以进入到Dify的工作界面。

24
00:01:54,620 --> 00:02:01,840
这是操作系统对于我们的管理员、对用户来说，直观可见的它的能力提升。

25
00:01:57,970 --> 00:02:01,175
接着导入已经编排好的DSL文件。

26
00:02:02,140 --> 00:02:22,240
那第二方面的话，就是我们在操作系统里面去针对系统最基础的一号进程去做了相应的调整。我们在systemd上面也做了大幅度的扩展。我们通过对于系统智能服务的调整，我们的目标就是让大模型、语言大模型可以更好地管理和控制操作系统。

27
00:02:08,600 --> 00:02:09,920
好的，已经导入完成了。

28
00:02:10,180 --> 00:02:12,215
那现场让它完成一个洞察任务。

29
00:02:18,520 --> 00:02:25,985
趁着工具运行的时间，我们回头看一下整个执行过程中用到了哪些openEuler发布的软件。

30
00:02:22,660 --> 00:03:09,165
这里面最值得提出来、最值得说的是，我们通过高质量的操作系统语料微调出了一个我们认为是行业里面第一个服务器操作系统的领域模型。这样一个领域模型可以通过纯的CPU推理，不需要额外其他任何的硬件，就可以实现非常高质量的系统的智能调优，能够和600多亿的商用模型在调优能力上面形成一个可比的效果。那我们在这里的话就邀请我们第一位的演示嘉宾，给大家展示一下openEuler这样一个领域定制模型在智能调优场景里的一个实战应用。有请。

31
00:02:26,760 --> 00:02:33,205
先是从用户需求获取信息源链接的时候，用到了Qwen2-7B的模型。

32
00:02:26,760 --> 00:02:33,205
先是从用户需求获取信息源链接的时候，用到了千问3-8B的模型。

33
00:02:34,270 --> 00:03:04,665
这个模型是用openEuler官方Llama-cpp镜像在昇腾服务器上启动的。

34
00:02:40,640 --> 00:02:44,820
然后，在分层次提取链接内容的过程中，

35
00:02:45,500 --> 00:02:49,935
浅层网页的信息提取用到的是Qwen2-72B模型。

36
00:02:45,500 --> 00:02:49,935
浅层网页的信息提取用到的是千问3-32B模型。

37
00:02:50,800 --> 00:02:57,545
然后深度分析和总结的Agent用到的是Qwen2-57B-A14B稀疏模型。

38
00:02:50,800 --> 00:02:57,545
然后深度分析和总结的Agent用到的是千问3-nice-80B稀疏模型。

39
00:02:58,360 --> 00:03:04,665
这两个模型分别用openEuler发布的vLLM和S-Giant镜像一键启动的，非常方便。

40
00:03:05,660 --> 00:03:14,845
而且在本次会议之前，这个洞察工具的DSL文件和刚才模型部署的所有脚本已经在社区开源了。

41
00:03:10,890 --> 00:03:12,250
好的，谢谢辛总。

42
00:03:12,430 --> 00:03:15,410
大家好，我是openEuler智能调优团队的王树源。

43
00:03:15,580 --> 00:03:19,860
然后开源的链接在屏幕的正上方，大家可以参考一下。

44
00:03:15,670 --> 00:03:20,270
在调优领域，最开始都是依赖我们的调优专家们进行人工调优。

45
00:03:20,280 --> 00:03:23,060
那么现在整个洞察任务已经结束了。

46
00:03:20,290 --> 00:03:24,650
这样带来的问题是人力成本高、调优耗时长、强依赖专家知识。

47
00:03:23,399 --> 00:03:30,100
当然为了方便我随时去查看每天的洞察结果，我也配置了邮件订阅这么一个功能。

48
00:03:24,870 --> 00:03:32,750
之后，我们openEuler推出了A-Tune工具，进行了自动化的调优，但是对于先验知识的强依赖性问题还是没有解决。

49
00:03:30,460 --> 00:03:32,645
然后可以看一下邮箱。

50
00:03:32,990 --> 00:03:46,780
最后，我们在A-Tune的基础上提出了大模型智能调优方案，利用大模型的学习能力与分析能力，自主完成瓶颈分析和参数推荐，节约人力成本，节省调优耗时，摆脱对先验知识的强依赖性。

51
00:03:34,200 --> 00:03:38,075
可以看到邮箱确实已经收到了刚才洞察的结果。

52
00:03:38,750 --> 00:03:44,655
而且在一分钟左右的时间就得到这么多细化的内容，确实效率比人工高多了。

53
00:03:45,390 --> 00:03:49,885
如果大家感兴趣，可以在会后也尝试一下屏幕上方的链接。

54
00:03:46,980 --> 00:03:54,775
基于此背景，我们进一步推出了支持内嵌在openEuler操作系统中的领域模型，它可以在纯CPU环境进行推理部署。

55
00:03:50,560 --> 00:03:51,919
好，谢谢大家。

56
00:03:54,410 --> 00:03:56,879
好，谢谢小伙伴的分享。

57
00:03:55,820 --> 00:04:04,140
下面，我将向大家演示我们领域模型在鲲鹏920B服务器上的使用，纯CPU环境进行部署。

58
00:03:57,500 --> 00:04:15,025
就我们能看到，就是我们的全栈既能够支持在不同的企业环境里面高效地获取AI软件、运行，然后也能够支持在上面高效地基于企业自己的数据去开发Agent以及定制模型。

59
00:03:57,500 --> 00:04:15,025
就我们能看到，我们的全栈既能够支持在不同的企业环境里面高效地获取AI软件、运行，然后也能够支持在上面高效地基于企业自己的数据去开发Agent以及定制模型。

60
00:04:04,260 --> 00:04:07,720
首先，我们将模型文件下载到本地环境。

61
00:04:07,740 --> 00:04:14,250
可以看到我们的OS领域模型模型文件共有9个，总计2.2个G左右。

62
00:04:14,590 --> 00:04:19,965
然后，我们使用Llama-cpp工具直接将我们的领域模型在纯CPU环境上进行部署。

63
00:04:16,750 --> 00:04:27,030
我们都知道openEuler是一个全场景、多样性算力支持的这样一个操作系统社区，所以刚才在讲Intelligence BoM的时候，肯定也有小伙伴在想，

64
00:04:24,630 --> 00:04:29,550
可以看到我们的领域模型已经成功在纯CPU环境完成了部署。

65
00:04:28,290 --> 00:04:33,790
就是Intelligence BoM它是不是只能够在云和服务器上来运行？

66
00:04:29,670 --> 00:04:38,040
接着，我们将我们的领域模型配置到我们的智能调优应用中，主要是LLM URL以及LLM model name这两个参数。

67
00:04:34,150 --> 00:04:37,010
那我们都理解这样一个问题。

68
00:04:37,410 --> 00:04:39,990
所以这是我们的回答。

69
00:04:38,040 --> 00:04:41,355
这边我已经提前配置好。

70
00:04:40,350 --> 00:04:41,750
就并不是。

71
00:04:42,190 --> 00:04:53,800
我们在看Intelligence BoM的话，它虽然是从服务器、从云上的AI软件栈开始，但是它一定会扩展成为一个全场景的AI的参考实现。

72
00:04:44,000 --> 00:04:46,635
然后我们启动我们的智能调优。

73
00:04:48,190 --> 00:04:54,650
由于现在调优还是小时级的，所以我们这里跳过我们的调优过程，直接给大家展示一下我们的调优效果。

74
00:04:54,200 --> 00:05:03,500
所以，那除了云和服务器之外的话，现在在什么场合里面大家对于AI的诉求最强烈呢？就是具身智能机器人。

75
00:04:55,130 --> 00:05:06,750
我们基于MySQL开箱性能作为基线，然后我们预先跑好了，左边是我们的领域模型的调优效果，右边是我们的DeepSeek-V2 671B模型的调优效果。可以看到，从最终的调优效果来对比，

76
00:05:03,500 --> 00:05:18,940
所以我们很高兴地把Intelligence BoM在云上的和边缘上的这样的栈，通过云边协同、端边协同的方式和具身机器人去做一个相应的对接。

77
00:05:06,870 --> 00:05:18,445
我们的领域模型在支持纯CPU环境推理部署的前提下，又能达到DeepSeek-V2 671B模型的调优效果。

78
00:05:19,370 --> 00:05:36,890
最后，我们的领域模型在其他应用的泛化能力，我们总结了一个表格。我们在Spark、Nginx、Ceph、PG-circle、MySQL等应用上均进行了调优效果的验证。最后的结果表明，我们的领域模型效果与DeepSeek-V2 671B的调优效果基本持平。

79
00:05:19,600 --> 00:05:27,240
传统的软件栈的话是以ROS，就是机器人OS这个中间件为基础的，但是它们的智能化能力不足。

80
00:05:27,740 --> 00:05:36,730
我们的AI软件栈虽然在数字世界里面有强大的能力，但是缺的是和物理世界交互的能力。

81
00:05:37,110 --> 00:06:08,740
所以通过Intelligence BoM在云边协同和端侧的具身智能的软件栈的这个对接，我们希望能够去构建一种全新的AI机器人的框架，通过数据感知层来采集数据、筛选数据，通过基础框架和AI栈在云边的协同和联动，实现高效的推理和精准的控制，并且能够针对异构加速、多模型性能调优这些场景去做针对性的优化。

82
00:05:37,310 --> 00:05:39,475
关于领域模型的演示到此结束，谢谢大家。

83
00:05:42,410 --> 00:05:43,650
谢谢树源的介绍。

84
00:05:44,270 --> 00:05:48,290
我相信大家对于这样的一个演示应该都非常的有感触。

85
00:05:48,290 --> 00:06:04,770
因为首先这是基于大模型来替代我们的专家在这里去做一个快速的系统调优，并且在刚才的演示中能看到，它不光在调优的结果，并且在泛化能力上面都有一个非常出色的呈现。

86
00:06:05,110 --> 00:06:15,760
然后另一方面的话，这也是我们第一次能够不通过定制的硬件，只通过CPU来做推理，这使得这样一项技术的可普及性大大提高。

87
00:06:09,060 --> 00:06:31,895
我们整个方案的话，当前还在紧锣密鼓地开发阶段。当前智源、中科院自动化所、AGIROS这些伙伴都以及参与进来，在共同推进整个生态的建设和发展。现场展区的话也有相应的阶段性成果，欢迎各位待会儿会议间隙的时候能够去做参观和交流。

88
00:06:15,780 --> 00:06:33,240
我们相信这样的一个探索也为后续在操作系统里内嵌更多的小的模型，也指出了一个方向。我们相信这会是一个后续操作系统领域里面大家开源探索和学术研究的一个热门的主题。

89
00:06:32,219 --> 00:06:32,549
好。

90
00:06:33,700 --> 00:06:49,610
那么对于openEuler来说，我们为什么今年能够做到这件事情？其实也是有一个很强的相关性，就是我们在今年通过刚才提到的Intelligence BoM这样一个openEuler和我们的伙伴联合的开源解决方案。

91
00:06:49,610 --> 00:07:02,710
通过这样的一个方案的话，我们在其中的模型微调能力，使得我们在openEuler上围绕自己的语料、围绕自己的数据去真训一个模型，成为了可能。

92
00:07:03,470 --> 00:07:33,200
在这里给大家再回顾一下，Intelligence BoM是openEuler和高校、企业以及各个社区联合打造的全栈开源的AI技术软件解决方案。我们在7月份的时候推出了第一个这样的一个参考实现版本，愿景就是通过这样的一个开源的参考实现来打破技术壁垒，通过全栈开源来加速大模型推理技术的普惠，来赋能行业的转型。

93
00:07:33,920 --> 00:07:47,650
在7月份之后的话，实际上整个社区还在快速地往前迭代和演进。我们在生态拓展、异构协同、在简单易用以及模型微调上都有了相应的进展。

94
00:07:47,890 --> 00:08:07,270
所以很高兴地在这里也给大家宣布一下，就是我们在当期发布Intelligence BoM 2511的第二个参考实现版本。我们在最开始设计的时候，就是社区里面基于AI工作组大家讨论，我们希望把每一个参考实现版本的话，用一种中国的地方特色美食来命名。

95
00:08:07,270 --> 00:08:34,245
所以当时第一个版本我们用的名字是河南的烩面，第二个的话是这个建议来自一位我的互联网朋友，他是浙江温州人，所以是温州当地的一种特色美食叫做敲鱼面。大家如果对这个名字不熟的话，可以下来自行那个搜索引擎或者通过豆包去查询一下。

96
00:08:36,360 --> 00:08:52,060
那在这个版本里面的话，我们在低成本、高效率、易落地这些上面其实都有了相应的这个进展，所以通过BoM这种方案的话，也使得更多的企业级AI助手可及。

97
00:08:52,340 --> 00:09:06,095
然后我们下面的话，其实就希望通过小伙伴的演示给大家更直观地了解一下，我们这样一个开源全栈到底怎么能够让Agentic AI的软件生态触手可及、开箱即用。有请。

98
00:09:08,260 --> 00:09:09,240
好的，大家好。

99
00:09:09,380 --> 00:09:11,775
我是来自AI SIG的罗义军。

100
00:09:12,420 --> 00:09:19,135
在正式开始之前，我们可以一块想一个问题，那就是DeepSeek-V2是什么时候发布的？

101
00:09:21,000 --> 00:09:24,800
其实很多时候直觉告诉我，这是一两年前的模型了。

102
00:09:25,280 --> 00:09:33,120
但实际上是今年年初才发布的，之所以有这种错觉是因为现在AI领域的技术实际发展得太快了。

103
00:09:33,480 --> 00:09:47,120
那么作为从业者，为了能够跟上这些前沿的技术，我经常会去一些技术平台学习、总结，比如去看一下最新的AI资讯，然后学习一下最新发布的AI Agent的工具等。

104
00:09:47,620 --> 00:09:52,530
但是类似的信息发布平台非常多，这种事情花费了我很大精力。

105
00:09:52,950 --> 00:09:58,370
那这个时候如果有一个智能洞察的助手能够完成这些事情，那就轻松多了。

106
00:09:59,090 --> 00:10:06,625
恰好现在也出现了像Dify、Cozey这样非常适合新手去搭建智能体的项目。

107
00:10:07,280 --> 00:10:10,480
那么我也去尝试用Dify开发了这么一个工具。

108
00:10:11,160 --> 00:10:12,495
现在给大家演示一下。

109
00:10:15,000 --> 00:10:16,569
首先启动Dify服务。

110
00:10:18,509 --> 00:10:19,675
OK，它已经正常启动了。

111
00:10:20,400 --> 00:10:25,455
除了使用Dify来编排这个工具以外，我们还用到了大模型的推理能力。

112
00:10:26,340 --> 00:10:35,375
刚才我已经在昇腾的服务器上和GPU环境分别拉起了模型。大屏幕上也显示了这些推理服务用到的软件栈。

113
00:10:36,210 --> 00:10:38,629
现在检查一下这些服务是否OK。

114
00:10:40,920 --> 00:10:44,215
好的，昇腾服务器上已经正常启动了两个大模型。

115
00:10:45,260 --> 00:10:47,585
然后再来看一下GPU环境的状态。

116
00:10:49,630 --> 00:10:50,385
也没有问题。

117
00:10:51,010 --> 00:10:53,615
那这时候就可以进入到Dify的工作界面。

118
00:10:57,970 --> 00:11:01,175
接着导入已经编排好的DSL文件。

119
00:11:08,600 --> 00:11:09,920
好的，已经导入完成了。

120
00:11:10,180 --> 00:11:12,215
那现场让它完成一个洞察任务。

121
00:11:18,520 --> 00:11:25,985
趁着工具运行的时间，我们回头看一下整个执行过程中用到了哪些openEuler发布的软件。

122
00:11:26,760 --> 00:11:33,205
先是从用户需求获取信息源链接的时候，用到了Qwen2-7B的模型。

123
00:11:34,270 --> 00:11:39,875
这个模型是用openEuler官方Llama-cpp镜像在昇腾服务器上启动的。

124
00:11:40,640 --> 00:11:44,820
然后，在分层次提取链接内容的过程中，

125
00:11:45,500 --> 00:11:49,935
浅层网页的信息提取用到的是Qwen2-72B模型。

126
00:11:50,800 --> 00:11:57,545
然后深度分析和总结的Agent用到的是Qwen2-57B-A14B稀疏模型。

127
00:11:58,360 --> 00:12:04,665
这两个模型分别用openEuler发布的vLLM和S-Giant镜像一键启动的，非常方便。

128
00:12:05,660 --> 00:12:14,845
而且在本次会议之前，这个洞察工具的DSL文件和刚才模型部署的所有脚本已经在社区开源了。

129
00:12:15,580 --> 00:12:19,860
然后开源的链接在屏幕的正上方，大家可以参考一下。

130
00:12:20,280 --> 00:12:23,060
那么现在整个洞察任务已经结束了。

131
00:12:23,399 --> 00:12:30,100
当然为了方便我随时去查看每天的洞察结果，我也配置了邮件订阅这么一个功能。

132
00:12:30,460 --> 00:12:32,645
然后可以看一下邮箱。

133
00:12:34,200 --> 00:12:38,075
可以看到邮箱确实已经收到了刚才洞察的结果。

134
00:12:38,750 --> 00:12:44,655
而且在一分钟左右的时间就得到这么多细化的内容，确实效率比人工高多了。

135
00:12:45,390 --> 00:12:49,885
如果大家感兴趣，可以在会后也尝试一下屏幕上方的链接。

136
00:12:50,560 --> 00:12:51,919
好，谢谢大家。

137
00:12:55,160 --> 00:12:57,420
好，谢谢小伙伴的分享。

138
00:12:57,500 --> 00:13:15,025
就我们能看到，我们的全栈既能够支持在不同的企业环境里面高效地获取AI软件、运行，然后也能够支持在上面高效地基于企业自己的数据去开发Agent以及定制模型。

139
00:13:16,750 --> 00:13:28,030
我们都知道openEuler是一个全场景、多样性算力支持的这样一个操作系统社区，所以刚才在讲Intelligence BoM的时候，肯定也有小伙伴在想，

140
00:13:28,290 --> 00:13:33,790
就是Intelligence BoM它是不是只能够在云和服务器上来运行？

141
00:13:34,150 --> 00:13:37,010
那我们都理解这样一个问题。

142
00:13:37,410 --> 00:13:39,990
所以这是我们的回答。

143
00:13:40,350 --> 00:13:41,750
就并不是。

144
00:13:42,190 --> 00:13:53,800
我们在看Intelligence BoM的话，它虽然是从服务器、从云上的AI软件栈开始，但是它一定会扩展成为一个全场景的AI的参考实现。

145
00:13:54,200 --> 00:14:03,500
所以，那除了云和服务器之外的话，现在在什么场合里面大家对于AI的诉求最强烈呢？就是具身智能机器人。

146
00:14:03,500 --> 00:14:18,940
所以我们很高兴地把Intelligence BoM在云上的和边缘上的这样的栈，通过云边协同、端边协同的方式和具身机器人去做一个相应的对接。

147
00:14:19,600 --> 00:14:27,240
传统的软件栈的话是以ROS，就是机器人OS这个中间件为基础的，但是它们的智能化能力不足。

148
00:14:27,740 --> 00:14:36,730
我们的AI软件栈虽然在数字世界里面有强大的能力，但是缺的是和物理世界交互的能力。

149
00:14:37,110 --> 00:15:08,740
所以通过Intelligence BoM在云边协同和端侧的具身智能的软件栈的这个对接，我们希望能够去构建一种全新的AI机器人的框架，通过数据感知层来采集数据、筛选数据，通过基础框架和AI栈在云边的协同和联动，实现高效的推理和精准的控制，并且能够针对异构加速、多模型性能调优这些场景去做针对性的优化。

150
00:15:09,060 --> 00:15:31,895
我们整个方案的话，当前还在紧锣密鼓地开发阶段。当前智源、中科院自动化所、AGIROS这些伙伴都以及参与进来，在共同推进整个生态的建设和发展。现场展区的话也有相应的阶段性成果，欢迎各位待会儿会议间隙的时候能够去做参观和交流。

151
00:15:32,219 --> 00:15:32,549
好。
