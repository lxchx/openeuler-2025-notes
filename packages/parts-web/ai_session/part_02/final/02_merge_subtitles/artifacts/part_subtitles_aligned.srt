1
00:00:00,870 --> 00:00:07,690
下面有请招商证券股份有限公司核心系统架构师黄俊先生，

2
00:00:07,690 --> 00:00:15,005
为我们带来《沙上聚塔：基于LLM的核心系统软件栈智能调优探索》。大家欢迎。

3
00:00:24,590 --> 00:00:27,590
大家好，我是来自招商证券的黄俊。

4
00:00:28,010 --> 00:00:40,664
这次我给大家带来的分享，是想基于大模型技术，在我们一些核心系统，也就是国产化替代，即信创领域，做一些性能差距的补齐探索。

5
00:00:42,960 --> 00:00:45,480
首先，照例还是来一个自我介绍。

6
00:00:45,760 --> 00:00:52,760
我在招商证券目前共职，担任核心系统架构师，也兼任公司的DevOps效能教练。

7
00:00:53,500 --> 00:01:03,780
主要技术标签列了五个：一是有幸参与开放原子开源基金会openEuler社区AI联合工作组，成为其中一员。

8
00:01:04,840 --> 00:01:10,340
二是过去三年，有幸在创原会当选年度云原生MVP。

9
00:01:10,960 --> 00:01:19,830
创原会是全球云原生计算基金会、华为和信通院组织的云原生技术交流组织。

10
00:01:20,210 --> 00:01:22,886
如果大家有兴趣也可以参加。

11
00:01:24,210 --> 00:01:34,470
在软件栈方面，我一开始是做前端开发出身，做过安卓、iOS、H5，后来也从事过.NET、Java后台开发。

12
00:01:35,270 --> 00:01:45,160
在DevOps方面，拥有十多年的带队经验，对过去的瀑布式模型，以及现在的Scrum敏捷双速团队，都有实践经验。

13
00:01:45,820 --> 00:01:52,205
最后，我是公司的内部培训认证讲师，也曾受邀担任TWT企业IT社区特邀技术主持。

14
00:01:54,410 --> 00:01:56,350
本次分享分为四块。

15
00:01:56,570 --> 00:02:03,590
首先会讲一下在国产化替代全面落地的过程中，我们遇到的一些痛点与挑战。

16
00:02:03,690 --> 00:02:13,170
第二，讲解大模型技术出现后，为我们的技术方案层面带来的新机遇。

17
00:02:13,770 --> 00:02:22,950
第三，分享我们设计的智能调优方案的细节与创新点。

18
00:02:23,800 --> 00:02:30,465
第四，基于信创技术生态，想与大家一起做一些创议和共建。

19
00:02:33,480 --> 00:02:38,760
这部分因为时间有限，我们就不细讲了，字和面都比较多。

20
00:02:39,040 --> 00:02:53,360
做信创的同学应该比较熟悉，硬件层面，如CPU、网卡、低时延交换机、存储等，与国外竞品相比，差距仍不小。

21
00:02:53,500 --> 00:03:01,300
因为涉及底层先进的制程工艺，短期内需要突破。

22
00:03:02,000 --> 00:03:27,510
在软件层面，差距相对更小，特别是操作系统、JDK等。在国内，开源与商业的有效闭环模式已经跑通。以openEuler为例，它是一个操作系统的生态底座，上面有统信、麒麟等商业发行版进行增强。

23
00:03:27,810 --> 00:03:37,500
通过商业增强发行版获得持续的商业收益后，可以反馈给开源社区，从而使整个生态全面开花。

24
00:03:37,560 --> 00:03:51,940
这在JDK领域也已初现苗头。毕昇JDK的底座是开源的，上面有ISV厂商做增强发行，比如宝兰德、东方通等，都有这方面的方案。

25
00:03:52,660 --> 00:03:59,840
下面我只讲一下数据库。我们从两个方面看，一是分布式场景。

26
00:03:59,840 --> 00:04:09,750
以OceanBase为例，它在“双十一”等超高并发场景下，已经历练多年。

27
00:04:09,750 --> 00:04:17,850
其技术水平放到国际上比较，即便不领先，也至少是并驾齐驱的。

28
00:04:18,510 --> 00:04:24,230
但在集中式场景，若以替代Oracle为目标，可能仍有差距。

29
00:04:24,390 --> 00:04:41,275
虽然性能可以通过软硬件协同研调基本抹平，但在产品成熟度、上下游工具链以及社区生态规模方面，确实还有比较大的欠缺。

30
00:04:42,990 --> 00:04:49,390
上面这些虽然有差距，但是你如果要做0到1的替代，我觉得还是至少有产品可用。

31
00:04:50,090 --> 00:05:08,890
这一块是我们发现，如果你要做全面的铺广推广，就是你要做到从1到10，甚至从10到100的效果，这一块我觉得是一个组织层面的差距。就是在传统我们的企业架构之中，开发跟运维它是严格分离的。

32
00:05:09,070 --> 00:05:17,420
所以一般看到系统开发可能侧重业务开发，运维会注重整个系统的运行稳定性。

33
00:05:17,800 --> 00:05:23,280
所以这里面，如果要做信创替代，中间就有一块调优的职责没人做。

34
00:05:24,240 --> 00:05:39,555
这其实就是我们想做这个方案的初衷。我们希望借助这种大模型的技术，能把这些专家经验沉淀到模型库中，然后来加以复用，由此来弥补中间这一块调优职责的缺失问题。

35
00:05:42,670 --> 00:05:54,250
当然，因为大模型技术也非常新，它还是有一些不确定性因素的，表现最突出的其实就是幻觉。

36
00:05:54,570 --> 00:06:01,010
我这里恰好那天我想把这个主题让豆包AI生成一个胶片。

37
00:06:01,670 --> 00:06:11,940
其实整体效果大家可以看到，应该已经达到至少我这边80%到90%的预期了。但是很不幸，就是下面这里有两个地方还是有些谬误。

38
00:06:12,160 --> 00:06:25,040
就第一个就是，它把忠实性幻觉下面一些说明内容跟这个忠实性幻觉这个一级标题做了一个并列。其实我们知道，这种是不是一个并列，是一个从属的关系。

39
00:06:25,240 --> 00:06:34,240
而这种逻辑关系，其实你通过语言模型，你那些检索算法其实是不一定能识别到的，因为它已经超出了常规的这种语言逻辑了。

40
00:06:34,540 --> 00:06:46,110
第二块呢就是说，就比较明显了，就是一个内容的一个重复。这种相对来说就对我们就比较致命了，因为这是一个非常明显，普通用户也能识别出来的错误。

41
00:06:47,110 --> 00:06:54,970
但是我们就是说，我们既想用这个大模型的一些优势，同时呢，还是得想办法要摒弃它的一些劣势。

42
00:06:55,270 --> 00:07:01,670
所以我们就想了一些办法。当然，首先还是想讲讲大模型我们用到的它的一些长处吧。

43
00:07:02,090 --> 00:07:13,960
当然，在介绍大模型之前，还是稍微做一点导入，就是整个系统调优这一块，其实业界其实已经也有很多年了，也有各种工具，不管是数据库的也好，还是Linux的也好。

44
00:07:14,280 --> 00:07:24,560
我们这边拿的一个对标的就是目前其实在应用大模型之前，应该算是一个最先进的一个优化框架，就是贝叶斯迭代模型这一套。

45
00:07:25,180 --> 00:07:34,460
它基本上其实也，它的思路其实也是希望基于贝叶斯定理来找到这个函数的一些，就是调优函数的最大值或者最小值。

46
00:07:34,760 --> 00:07:45,430
然后呢，在这个过程中，它是可以做一轮轮的迭代，上一轮迭代它拿到的这些历史的运行信息，可以指导下一次的优化，来调整你那个参数集的一些取值。

47
00:07:46,290 --> 00:08:05,590
整体的逻辑其实差不多就是这个图吧，就你将负载特征化之后，你就可以通过那个专家来做一些关键调优参数的一些选型，选了这个范围之后，就可以来训练你这个代理模型。然后代理模型训练出来之后，它就具备了智能来生成这些参数配置方案的一个能力。

48
00:08:05,890 --> 00:08:23,080
一旦通过它生成这个方案，你就可以通过Agent下发到你的应用节点去做执行，同时通过Agent来采集你这个运行数据。有这个运行数据，其实你又可以传回给后端的这个代理模型来做下一轮的一个校正。

49
00:08:23,280 --> 00:08:37,965
由此呢，就又可以做新一轮的那个参数下发，就可以形成一个持续的一个自循环，然后最终可以达到就是说你整个环境的那个性能效果比较稳定之后，你的这个调优方案相对来说就应该是比较成熟了。

50
00:08:40,050 --> 00:08:53,630
刚刚我们说了，那个是我们就是说对标的一个方案，那我们所以还是把，如果我们用这种大模型来做一个综合性的智能调优方案，跟它贝叶斯会有的一些比较性的优势。

51
00:08:54,030 --> 00:09:20,420
就我们从第二条开始，就是负载感知这一块，其实刚刚那个贝叶斯的整个循环，我们觉得其实它已经初步有一些这种系统感知的能力了，当然它可能没有自动化的能力。所以这一块也是我们想用大模型的一个最大的优势，它就可以实现这种细粒度跟多维度的一些负载的分析，然后由此来识别一些关键的一个性能瓶颈点，就不只是基于一些规则项了。

52
00:09:20,720 --> 00:09:33,500
参数识别这一块呢，刚刚其实贝叶斯这边刚刚特地提过了，就它可能还是需要专家来筛选这些关键参数，就是你还是依赖，摆脱不了人工的这个束缚。

53
00:09:34,086 --> 00:09:52,210
而大模型方案这一块其实是我们最看重的，我们希望就是把这些知识形成领域知识沉淀到大模型，不管是基于RAG库还是说基于调优、微调，然后结合这个工作负载，能够自动识别一些关键参数来形成这个参数调优的方案。

54
00:09:52,730 --> 00:10:03,990
由上面其实就决定下面这个调优范围了，就肯定贝叶斯这边呢，它只能做部分参数的这种，它没办法做到大规模的这种调优。

55
00:10:04,310 --> 00:10:15,020
而在信创这种全栈调优下面，基本上现在大概有13000多个参数了，所以你如果用人工来调优，其实是做不到的。

56
00:10:15,180 --> 00:10:21,780
所以我们就还是希望就是说，结合这种领域知识的辅助，能够用大模型来做这种模型方案的输出。

57
00:10:22,620 --> 00:10:28,020
可解释性我觉得就不太说了，因为两者其实都还是有些解释性问题。

58
00:10:28,460 --> 00:10:41,370
下面四个方面呢，就最终你会发现就是说，我们贝叶斯的模型，它调优效率，我们觉得已经其实已经算比较快了，就以天级别算吧，那最终你全生命周期算起来应该就是以周了。

59
00:10:41,790 --> 00:10:50,390
而大模型这边可能相对来说就会快很多，就基本上只要你方案出来，基本上就可以小时级，例如你跑一个晚上基本上就能出来了。

60
00:10:50,490 --> 00:10:57,086
那整个全生命周期就从你开始调到最终出方案，基本上可能天级别吧。

61
00:10:59,260 --> 00:11:14,780
下面呢，我们就稍微快速地过一下大模型，我们用到的一些比较技术长处的地方，或者觉得它能有价值的地方。RAG库这个技术应该大家就用得非常熟了，也是最早大模型技术出来想解决这个模型幻觉的一个技术。

62
00:11:15,140 --> 00:11:24,900
这一块就一方面它非常成熟简单，就各个开发框架基本上都能一件接入，所以它的那个接入成本很低，就成本效益比相对很高。

63
00:11:25,260 --> 00:11:30,045
也是和企业这种来接入它的一些私有数据嘛，就数据不会跑出去。

64
00:11:31,230 --> 00:11:49,930
可解释性跟可信度就因为它依赖本身这个知识库，所以相对来说这个是可以想到的。我们还是稍微说一下它的劣势，劣势这里面列到第四块，其实我觉得是最重要的，就是说它比较难以处理这种，还是比较难以处理这种复杂的这种跨领域的这种关联的问题的推理。

65
00:11:50,070 --> 00:11:56,930
因为它其实本质上还是一个向量检索技术嘛，它并没有太多的利用这个模型的一个推理能力。

66
00:12:05,590 --> 00:12:21,335
模型微调呢，这是我们第一卷的方案，还是想重点去抓的一个，就是说我们希望通过我们的一些知识的沉淀，能把这个微调的这个模型给训练出来，然后来真正产生这种专业化的价值。

67
00:12:25,850 --> 00:12:42,170
蒸馏这一块其实是DeepSeek带火的，我们这其实主要是成本方面的一个优势，就如果你把一个大模型通用大模型的能力，有个80%、90%的能够蒸馏到这个小模型，然后让你的部署成本，那就可以大规模的一个降低了。

68
00:12:42,390 --> 00:12:47,425
你部署成本降低了，那你应用的數量啊、场景啊，自然就可以极大地扩展了。

69
00:12:48,960 --> 00:13:02,335
MCP的话，应该就我觉得它更多的是帮助大模型来融入整个现有的我们的一些专业的领域能力也好，工具能力也好，把它形成一个整体的方案，所以其实这是一种融合性的技术。

70
00:13:03,190 --> 00:13:13,175
在第三块我们的方案这边呢，我其实画了一个回环，但这个因为时间比较少了，所以就不展开聊了，刚刚其实已经讲过了。

71
00:13:15,180 --> 00:13:49,070
基于上面那些技术的优势，我们就整体就大概做了一个多模型的一个架构方案的构想。其实我们希望就是说，整个通用大模型它只是在上面一层，我们可以叫路由解析层，它依托一些输入，你可以对简单问题或者结合RAG库，你可以把一些相对比较简单的或者解答类的问题，把它做一些输出。但是如果更深层次的一些通用的调优的这种任务，我们还是希望它能路由到我们后端的这些领域模型来。

72
00:13:49,550 --> 00:13:58,685
根据不同的领域模型，来做专业的事情。当然这个其实只是一个示例，实践的目前我们其实还只调优了一个OS的一个领域模型。

73
00:14:00,440 --> 00:14:13,100
最终方案呢，我们也放了一张图，左边呢是我们想做的一个叫AITune的一个智能的调优系统，它大概分三层，一个上层就是基于这个Copilot其实做的。

74
00:14:13,100 --> 00:14:26,020
就我们跟华为在联创，就通过它一方面就是说，做一个基准测试跟一个性能采集，然后有了性能采集之后，你的性能指标数据就可以收集给到中间这个Copilot的服务端。

75
00:14:26,020 --> 00:14:39,310
由它来做一个负载感知，还有参数的一些汇总、瓶颈的分析。但是最终呢，你会汇总这些参数之后，还是得要传到大模型这边，由大模型来生成它的专业的这个参数方案。

76
00:14:39,810 --> 00:14:53,810
当然这一块我也还是把那个RAG库的这一块也放上来了，因为这一块还是它模型的一个能力，但我们真正核心的其实还是希望大模型能跟小模型形成一个联动，由领域的小模型来输出一个调优的策略。

77
00:14:54,490 --> 00:15:08,586
右边是我们画的我们自己的一个核心交易系统简图，就除了刚刚那个Tuner Agent和跟我们这个模型层做交互之外，它本身我们也还有一个应用的一个运维的监控平台，来实现整个一个运行的监控跟应用的部署。

78
00:15:08,586 --> 00:15:28,735
当然后续呢，我们还是希望这个监控平台能跟我们的模型能做更多的一个交互，例如你把你的运行信息能输出到这边，那引擎层这边如果收到这些信息之后，就除了做调优，你还可以做问题诊断、各种系统的调度之类的，这是远期的一个场景。

79
00:15:58,850 --> 00:16:19,390
这个是我们规划的一个RAG库的一个资产库吧，资产库列表吧，反正就包括一般就是核心系统的一些运维的或者说技术类的资产，然后另外就是数据库的，还有鲲鹏的计算类的，还有MQ啊，包括一些其他中间件，就Redis、JDK，后面这一块肯定应该还是会继续拓展。

80
00:16:20,830 --> 00:16:43,315
这个是属于组织赋能上面的吧，就我们目前这个其实还是有1.0版本的输出，目前已经把它赋能到我们的研发团队，就恒生电子那边，然后让他们就是说在研发阶段呢，就参与这个调优跟评测，交给我们交付的时候就交付整个调优资产库，就这一块。

81
00:16:46,460 --> 00:16:55,040
这个就是我们近期做的一些调优的一个效果了，这个是个接入网关的，在开箱调优的场景下，大概提升13.6%的样子。

82
00:16:55,900 --> 00:16:58,586
主要起作用的参数其实就是那些。

83
00:17:00,586 --> 00:17:16,410
这个是最近我们调的，也是相对来说看效果还可以的，就是我们期权的系统的调优。当然这里面其实除了就是操作系统层的一些调优啊，也还有就是JDK跟那个数据库的。

84
00:17:16,650 --> 00:17:22,810
整体来说效果还可以，就特别是跟那个基线，就是我们不调优跟调优，它的提升还是比较明显的。

85
00:17:23,470 --> 00:17:42,355
跟X86其实我们也做了一版对比，当然这个涉及到那个硬件的性能它不完全一致，就对得起，所以数据还是仅供参考。而且也看到了，哪怕是这样，下面还是有蛮多接口，它的性能还是不及X86的，只能说我们通过调优是可以尽可能的弥补这些差距。

86
00:17:45,720 --> 00:18:04,020
分享最后呢，其实还是想跟大家同步一下，就是我们的一些下一阶段的一些研究计划，也欢迎大家一起来共建。就一方面我们是希望就是把我们公司专有领域调优模型做进一步的输出，然后使平台它的那个方案能够输出得更专业、精准。

87
00:18:04,620 --> 00:18:16,730
同时呢，也希望这种高频度的使用这个intelligence，能够跟华为团队一起联创，把整个平台的产品易用性做起来，然后能深度赋能研发团队。

88
00:18:16,830 --> 00:18:25,870
第三块其实也是一个呼吁，就是希望看看国内同行有没有对这块感兴趣，我们希望就是持续探索一下持续在线学习这个方向。

89
00:18:26,150 --> 00:18:49,485
它整个方向我觉得它在通用大模型的落地方面应该是一个战略性的一个可行性点，如果这个点不解决的话，通用大模型你要真正做商用落地，你始终没办法解决那些幻觉问题，因为你模型能力没办法持续地适应你落地场景去持续地强化，那你始终就是一个离线训练的一种模式。

90
00:18:50,980 --> 00:19:12,320
第四块是，我觉得还是可以有比较大的落地价值，然后后续呢应该是也可以可实现性比较高的，就就是现在我们企业有大量的数据嘛，但是企业呢，你如果要去堆卡，特别是中小企业来说，毕竟它单卡那么贵，你不可能堆很多。

91
00:19:12,500 --> 00:19:17,120
但你如果要作模型的训练，资源消耗也挺大的。

92
00:19:17,260 --> 00:20:12,760
所以我觉得最终可能比较理想的状态下，你企业应该是主要就负责推理这一侧的人算力资源的积累，你训练这类的资源积累，应该还是依托远端的这种算力平台或者云平台。但是你要能达成这种愿景，你一定要解决你企业那个知识的信息安全的问题。所以我们就跟华为的光猫AI团队，也在探讨这一块的方案，就是说看是不是说它知识库，我们做一些脱敏，当然这个具体技术方案其实也还只是一个意向性想法，就例如可能做了向量化，然后做一些脱敏之后，再远端会有一个专有的算力池给到企业这边，然后企业就在基于这个专有算力池来做训练，训练完之后，你再把这个模型传回给你公司本地，这样就确保你整个数据看上去是没有泄露的。

93
00:20:13,080 --> 00:20:18,040
当然这个是其实还是个非常宽，初级跟那个开放性的一个构想。

94
00:20:18,280 --> 00:20:23,155
只是说我们觉得它应该是后续有比较大的商业运行价值。

95
00:20:24,900 --> 00:20:38,820
这张图如果跟我比较熟的同学应该看过好多次了，因为我每一次分享都会把它放到最后面。就因为整个信创替代确实做得很艰难，然后也有很很多年了，然后差距确实也在不断地缩小。

96
00:20:38,860 --> 00:20:46,680
但是毕竟国外的平台它积累了几十年嘛，所以你一下子要要赶上他们，其实我觉得也没那么快。

97
00:20:47,200 --> 00:21:04,915
所以这种情况下只能就是说大家就是说一起来做，然后步骤尽可能地保持一致，就因为你大家都来用，你那些可能的坑啊，或者场景啊方面，你就能使产品能得到更多的锤炼，那就能更快地加速它的一个成熟化。

98
00:21:08,890 --> 00:21:13,505
行，我的分享就到这里，时间好像也超了一些，感谢大家。

99
00:21:14,520 --> 00:21:16,500
黄老师请留步，请允许我，

100
00:21:16,500 --> 00:21:22,660
我看到你刚才那个问题，我特别想，那个，可能你上面的那页的第三个问题，4.1。

101
00:21:22,660 --> 00:21:29,920
老师能帮我回放一下吗？那个，就是或者没关系，也可以，耽误一分钟时间。

102
00:21:30,140 --> 00:21:40,860
就是您刚才提到的第三个点，就是说那个在线去调优，持续在线去学习的方案的可行性。

103
00:21:41,260 --> 00:22:06,650
这里面其实我们花一两分钟时间很想请教一下，就是说这个问题它本质上来讲的话，它其实又在于就是你的数据或者环境，它第一个调优会带来一些底座的问题，第二个呢就是说可能会对你的系统里面可能会有一定的这个底噪，就是你必须要运行这个在线的能力的时候，其实这个地方在一些探索的时候，我们是一个可接受的方案吗？就是这个问题可能想请问一下。

104
00:22:06,850 --> 00:22:12,270
对，其实我觉得业界目前也有一些在往这方面走的。

105
00:22:12,470 --> 00:22:18,800
我觉得初步可以预想的呢，就是说是第一步走增量学习的一个路线。

106
00:22:19,360 --> 00:22:26,460
所以增量学习其实，它本质上还是一种离线学习模式，就是离线的训练是训练，推理是推理。

107
00:22:26,960 --> 00:22:31,320
但它它可以做到就是说，你可能频度上会比较加快。

108
00:22:31,620 --> 00:22:41,660
就例如我用我们这个调优的这个这个例子来说，我觉得也还可以，就是说你例如你持续地采集线上的这些生产数据。

109
00:22:42,020 --> 00:22:49,710
那你采集，例如你持续采集一天，你肯定有一定量的积累，哪怕一天的量如果不够，我采集一周，你的量也是可以的。

110
00:22:50,050 --> 00:22:59,990
然后在你晚上的时候，例如就是非生产时间，你就可以切换这个角色，来做那个，我们叫微调也好，叫预训练也好。

111
00:23:00,230 --> 00:23:06,990
然后把这个事情做完之后，因为你模型我们目前说我们想调的还是小模型嘛，你时间整体应该可控。

112
00:23:07,070 --> 00:23:11,310
可控之后，再把这个能力更新，更新之后你第二天就可以继续用了。

113
00:23:11,970 --> 00:23:24,500
但其实但后续呢，我觉得可能在这个基础上应该把平台的一个能力自动化之后，这个应该效率应该可以转得越来越快，就越来越顺畅，就不需要人工来介入太多。

114
00:23:25,240 --> 00:23:33,000
当然也还有一个，好像是武汉那边有个家厂商，他给的一个思路，那个我觉得成本有点高。

115
00:23:33,120 --> 00:23:51,330
他其实就生产，他是会布两个，布两个卡，其实就是布两个模型，一个是用来线上在线的推理，另外一个呢，就是来做后那个持续的这个学习，等这个学习的

116
00:23:51,330 --> 00:23:59,586
或者叫微调，微调之后能力具备了，再替换那个线上的模型。当然这个我觉得它的算力的耗损耗，你至少就是两倍了。

117
00:23:59,586 --> 00:24:01,586
远期来说我觉得可能不一定具备太强的这种可落地性。

118
00:24:01,586 --> 00:24:02,086
就这么多。

119
00:24:02,086 --> 00:24:02,586
好。

120
00:24:02,586 --> 00:24:03,086
非常感谢。

121
00:24:03,086 --> 00:24:03,586
黄老师。

122
00:24:03,586 --> 00:24:06,435
请就坐。谢谢。再次欢迎。

123
00:24:09,350 --> 00:24:14,790
好，那个耽误了一点时间，我觉得这个问题可能都是大家希望看到的一个这个探讨的一个问题。

124
00:24:15,030 --> 00:24:54,900
那我们非常感谢黄老师给我们带来的那个精彩分享，其实黄老师从这个贝叶斯的调优跟那个模型的调优，以及RAG的方案跟那个微调的方案呢，给我们做了一个详细的阐述，并且结合MCP Server或者领域的，刚才我也看到了一个OS领域大模型这个视角，就是说对我们操作系统相关的这个13000多个参数的一个调优，做了一些相关的这个那个，不管是從效率上还是这个这个成果上面都能得到一定的提升，然后可以看到后面这个也规划了很多的相关的工作，也带来一些性能还有一些高低的那个问题，可能还有很多问题要去解决。

125
00:24:55,060 --> 00:25:00,680
我们也会欢迎后面一起的讨论，怎么能更好的去解决这类似的问题，好，谢谢。
