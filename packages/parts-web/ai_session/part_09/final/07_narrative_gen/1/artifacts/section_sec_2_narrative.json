{
  "id": "sec_2",
  "index": 1,
  "title": "技术概述：FP8及其应用价值",
  "time_range": "00:01:42,355 - 00:08:36,845",
  "script": "首先，我会介绍什么是FP8，以及它能带来什么价值。\n第二，我会汇报我们团队基于FP8所做的算子设计与优化工作。\n第三，分享我们团队对未来的一些思考。\n在开始之前，先介绍一下我们团队正在构建的从上到下的整体软件栈。\n首先在模型层，我们目前主要支持Qwen和DeepSeek这两个系列的模型。\n在中间的推理框架层，我们主要集中在vLLM-Ascend和vLLM，并在此基础上开发了自研的推理框架。\n再往下是算子层。我们会针对业界常用的算子进行特定调优。\n例如LSTM，我们目前的调优成果比业界已知方案的整体性能提升了约30%。\n整体性能表现很好。但今天重点不是LSTM，而是软FP8算子。\n在下层，我们会在不同的操作系统上运行我们的算子。今天主要是在我们基于openEuler的FusionOS操作系统上进行了一些系统层面的调优。\n再往下是硬件层，支持各种国产加速卡，例如燧原、昆仑芯、天数、昇腾，以及我们最近正在适配的壁仞。\n接下来，我们开始介绍FP8技术本身。\nFP8是一种新兴的数值表示格式，专为AI计算优化。\n它能在保持高精度的同时，显著降低计算资源需求。\n与传统的FP16和BF16相比，FP8能将显存需求减半。与目前流行的INT8相比，FP8的推理精度更高。因此，它完美平衡了效率与精度。\n我们从三个方面来看。第一，显存需求减半。与BF16相比，权重占用的显存减半，因此能大幅缓解硬件资源压力。\n例如DeepSeek模型，之前需要16张卡，现在用8张卡就能运行，硬件资源直接减半。\n第二，更高的推理精度。社区（如Hugging Face）上已有的INT8权重文件，其精度略有下降。我们在此基础上推出FP8，将精度重新提升了回来。\n最终，基于以上两点，我们实现了效率与精度的完美平衡。\n在当前的AI应用场景中，无论是从社区还是大模型的训练与推理结果来看，FP8已成为大模型领域的“新宠”，越来越多的AI模型开始采用FP8格式进行训练和推理。\n而且，长期以来，FP8精度的支持高度依赖国外芯片。目前，越来越多的国产芯片也开始支持FP8，甚至FP4，严重制约了相关技术的自主发展与广泛应用。\n在2025年，FP8低精度类型已成为大模型主流选择，许多企业和研究机构都在尝试如何在不同硬件平台上实现极高的FP8计算。\n当前FP8技术面临的主要挑战是对特定硬件的高度依赖。为解决这一问题，河南昆仑技术有限公司基于昇腾易用的Ascend C算子开发框架，成功研发出软FP8解决方案。\n该解决方案主要分为三个层面。第一，软FP8助力硬件支持FP8权重。我们将FP8权重模型输入昇腾硬件，通过我们自研的反量化算子，将其转化为BF16格式参与计算。\n这在保障计算过程的准确性，同时也为后续使用FP8权重模型进行快速适配预留了灵活空间。\n无需权重格式的多次切换，提高了部署效率。主要的核心是算子级优化和框架级优化。\n在算子创新方面，我们进行了多项优化。由于时间关系，这里只列举几点。\n例如，创新性地在Kernel内提出专家动态反量化方案。\n将Vector与Cube两部分算子进行融合，高效调度两类核心的计算任务。\n通过探寻分块策略、数据预取与乱序等，彻底消除计算流水线气泡。\n在框架级优化方面，通过PyTorch的Meta函数实现入图，使得自研FP8反量化算子基于懒加载下发，避免了单个算子依次下发带来的性能开销。\n利用模型特征的智能感知，实现计算路径的动态调整。\n端到端推理效率提升32%。\n该技术在不同硬件配置上均表现出色。在模型精度几乎无损的前提下，单台设备即可流畅运行满血版DeepSeek V3.1。\n在板卡机型上，能实现模型参数规模翻番，运行更大规模的模型。\n大幅提升并发处理能力，让不同硬件配置的用户都能享受到FP8推理的技术红利。\n该方案实现了广泛的模型兼容性，全面兼容DeepSeek V3.1、DeepSeek-V3/R1、Qwen3等主流FP8量化模型。\n右侧是软浮点推理精度测试结果，相较于DeepSeek-R1的原始格式测试结果，FP8与INT8+INT4的精度相当。",
  "summary": "本节首先介绍了FP8技术，它是一种专为AI计算优化的新兴数值格式，能以减半的显存需求和比INT8更高的精度，完美平衡效率与精度。为解决FP8对特定硬件的高度依赖问题，讲者团队基于昇腾Ascend C框架研发了软FP8解决方案。该方案通过创新的反量化算子和框架级优化，实现了32%的端到端推理效率提升，并能在精度几乎无损的情况下，让现有硬件运行更大规模的模型。"
}