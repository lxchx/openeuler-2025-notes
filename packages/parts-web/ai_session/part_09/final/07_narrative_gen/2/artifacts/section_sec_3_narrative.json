{
  "id": "sec_3",
  "index": 2,
  "title": "算子调优：FP8 Matmul/GroupedMatmul 优化实践",
  "time_range": "00:08:38,500 - 00:16:33,180",
  "script": "下面，我将汇报我们团队关于软FP8软浮点算子的设计。\n我们针对不同场景进行特定优化，这里介绍其中三点。\n我们自研FP8算子整体方案，核心在于动态反量化。\n昇腾官方的原始路径是直接使用BF16的激活值和权重，通过Matmul或GroupedMatmul进行计算，最终输出BF16的结果。\n但这种方案需要较多的硬件资源。\n因此，我们首先在Vector Core对FP8权重进行动态反量化，将其转换为昇腾芯片支持的BF16格式。\n接着，在Cube Core对反量化后的权重和输入激活值进行BF16矩阵乘法。\n最后，Vector Core再对FP32的矩阵乘法结果作类型转换。\n我们针对不同场景进行了特定优化，下面介绍其中三点。\n优化一：基于MOE专家激活信息的选择性反量化。\n优化二：针对大Shape的带宽优化。\n优化三：针对特定模型Shape亲和性优化。\n它与昇腾标卡的核数比较亲和。\n我们可以完全复用，且没有任何尾核处理。\n首先是基于专家激活信息的选择性反量化。我们在算子中增加了一个group_list参数，用于描述模型中专家的激活情况。\n根据group_list，可以动态调度激活专家的计算基本块，对未激活的专家不进行反量化计算，以减少耗时。\n我们可以看一下左侧的图示。\n当专家0被激活时，我们会将专家0相关的数据分配到对应的核上进行运算。\n可以看到，专家1未被激活，因此我们不处理其对应的数据。\n当专家2被激活时，其对应的数据会在第五个核处理完后，继续由第六个核进行划分处理。\n这个过程的具体实现如下。\n优化二：针对大Shape的带宽优化。\n我们通过设置更大的基本块，将数据分成若干小块。\n然后将这些小块与scalar进行计算。\n具体来说，AI CORE依次对每一行进行反量化计算，循环128次。依次读取FP8权重和FP32反量化系数，权重矩阵与融合系数在L0C上融合，最后将权重数据转换成FP32-BF16。\n昇腾原本主要采用行优先的调度策略。\n我们则提出了列优先的调度策略，先调度基本块，再调度尾块。\n这使得基本块和尾块都能在AI CORE之间达到负载均衡。\n下面是一些测试数据。\n在特定Shape下，我们对比了带宽和反量化性能。\n结果显示，在大Shape下，显存带宽提升50%-65%。\n反量化性能提升51%-66%。而在小Shape下，提升不明显。\n但仍有提升，例如从32.4提升到35.61。\n提升幅度不明显。\n优化三：Qwen3-32B-FP8 Shape亲和优化。在适配Qwen3-32B-FP8模型时，我们发现其down反量化系数的行恰好等于300个A2标卡的Vector Core数量。\n其权重和系数矩阵可以按行将计算任务均匀分配给每个AI CORE。\n因此，我们按行将计算任务均匀分配给每个AI CORE，既达到了完美的负载均衡，又改善了每个AI CORE的访存空间局部性。\n下面是具体实现，我们通过代数等价替换，调整了转置操作与反量化的顺序。\n这可以令反量化系数矩阵达到40的权重复用，且列数大大增加，适合利用优化二提到的大Shape优化策略，加速计算。\n对比上下两图，当40为行时，我们先进行反量化，再进行转置和Matmul计算。\n当40为列时，我们先进行转置，将列转换为行，再进行反量化和Matmul计算。\n这是我们整体的端到端性能对比。\n在GroupedMatmul down层，FP8的吞吐量相比开源版本平均提升40%。\n在普通Matmul gate和up层，吞吐量相比开源版本平均提升25%。",
  "summary": "该部分介绍了团队自研的FP8算子优化方案，核心在于动态反量化。报告重点阐述了三项优化实践：基于MoE专家激活信息的选择性反量化、针对大Shape的带宽优化，以及利用特定模型Shape与硬件核心数亲和性的优化。这些优化最终在GroupedMatmul和普通Matmul上分别带来了40%和25%的吞吐量提升。"
}