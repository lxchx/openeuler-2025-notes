{
  "graph_data": {
    "nodes": [
      {
        "id": "p1",
        "label": "FP8 技术与软硬件挑战",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_1",
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_002",
            "chunk_00/slide_004",
            "chunk_00/slide_005"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_005",
            "primary_time_ts": "00:03:33,000",
            "primary_time_ms": 213000
          }
        },
        "parent": null,
        "summary": "介绍大模型面临的算力与内存瓶颈，以及昇腾硬件不支持原生FP8计算的挑战。同时，阐述了FP8技术作为一种新兴数值格式，在平衡效率与精度方面的核心价值与行业趋势。"
      },
      {
        "id": "d1_1",
        "label": "挑战：算力与内存瓶颈",
        "category": "Problem",
        "references": {
          "section_ids": [
            "sec_1"
          ],
          "material_ids": [
            "chunk_00/slide_002"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_002",
            "primary_time_ts": "00:01:10,000",
            "primary_time_ms": 70000
          }
        },
        "parent": "p1",
        "summary": "随着大模型规模持续增长，传统的BF16或FP16精度因高资源消耗，导致算力与内存成为制约模型扩展和性能发挥的关键瓶颈。"
      },
      {
        "id": "d1_2",
        "label": "挑战：硬件缺乏原生FP8支持",
        "category": "Problem",
        "references": {
          "section_ids": [
            "sec_1",
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_002",
            "chunk_00/slide_006"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_002",
            "primary_time_ts": "00:01:25,000",
            "primary_time_ms": 85000
          }
        },
        "parent": "p1",
        "summary": "当前的昇腾芯片（如910B系列）计算单元尚不支持FP8数据类型的原生计算，这制约了FP8技术在这些硬件上的直接应用和自主发展。"
      },
      {
        "id": "d1_3",
        "label": "概念：FP8 技术价值",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_005"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_005",
            "primary_time_ts": "00:03:33,000",
            "primary_time_ms": 213000
          }
        },
        "parent": "p1",
        "summary": "FP8是一种专为AI计算优化的数值格式，其核心价值在于能将模型显存需求减半，同时提供比INT8更高的推理精度，实现了效率与精度的完美平衡。"
      },
      {
        "id": "p2",
        "label": "软 FP8 解决方案与优化",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_2",
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_006",
            "chunk_00/slide_008",
            "chunk_00/slide_009",
            "chunk_00/slide_010",
            "chunk_00/slide_011",
            "chunk_00/slide_012"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_009",
            "primary_time_ts": "00:08:46,500",
            "primary_time_ms": 526500
          }
        },
        "parent": null,
        "summary": "为解决硬件限制，团队基于昇腾Ascend C框架研发了软FP8解决方案。核心是通过动态反量化算子在软件层面实现FP8支持，并结合选择性反量化、带宽优化、Shape亲和性优化等多项技术，显著提升了算子性能。"
      },
      {
        "id": "d2_1",
        "label": "方案：软FP8解决方案",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_006"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_006",
            "primary_time_ts": "00:05:39,000",
            "primary_time_ms": 339000
          }
        },
        "parent": "p2",
        "summary": "基于昇腾Ascend C框架，在软件层面实现对FP8权重的支持。方案通过自研的反量化算子，在计算时将FP8权重动态转换为BF16格式参与运算，从而让不支持FP8的硬件也能高效运行FP8模型。"
      },
      {
        "id": "d2_2",
        "label": "核心技术：动态反量化",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_009"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_009",
            "primary_time_ts": "00:08:50,000",
            "primary_time_ms": 530000
          }
        },
        "parent": "p2",
        "summary": "自研FP8算子的核心方案。在Vector Core上对FP8权重进行动态反量化，转换为BF16格式，再送入Cube Core进行矩阵乘法计算，规避了官方BF16路径资源占用过多的问题。"
      },
      {
        "id": "d2_3",
        "label": "优化一：选择性反量化 (MoE)",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_010"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_010",
            "primary_time_ts": "00:10:34,000",
            "primary_time_ms": 634000
          }
        },
        "parent": "p2",
        "summary": "针对MoE模型，根据专家激活信息（group_list），仅对被激活专家的权重执行反量化和计算，跳过未激活专家，显著减少不必要的计算开销。"
      },
      {
        "id": "d2_4",
        "label": "优化二：大Shape带宽优化",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_011"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_011",
            "primary_time_ts": "00:13:00,000",
            "primary_time_ms": 780000
          }
        },
        "parent": "p2",
        "summary": "针对大尺寸矩阵，提出列优先（Column-major）调度策略，并设置更大的基本块，优化了AI Core间的负载均衡，使显存带宽和反量化性能提升50%以上。"
      },
      {
        "id": "d2_5",
        "label": "优化三：Shape亲和性优化",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_012"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_012",
            "primary_time_ts": "00:14:30,000",
            "primary_time_ms": 870000
          }
        },
        "parent": "p2",
        "summary": "利用Qwen3-32B模型特定权重Shape与昇腾硬件核心数的天然亲和性，通过调整运算顺序，实现完美的负载均衡和40倍的权重复用，并有效利用了大Shape带宽优化策略。"
      },
      {
        "id": "d2_6",
        "label": "框架级优化",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_006"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_006",
            "primary_time_ts": "00:06:50,000",
            "primary_time_ms": 410000
          }
        },
        "parent": "p2",
        "summary": "通过PyTorch的meta函数实现算子懒加载（Lazy Loading）下发，避免性能开销，并利用模型特征智能感知实现计算路径的动态调整。"
      },
      {
        "id": "p3",
        "label": "成果与未来展望",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_2",
            "sec_3",
            "sec_4"
          ],
          "material_ids": [
            "chunk_00/slide_007",
            "chunk_00/slide_013",
            "chunk_00/slide_015"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_013",
            "primary_time_ts": "00:16:16,000",
            "primary_time_ms": 976000
          }
        },
        "parent": null,
        "summary": "总结软FP8解决方案带来的显著性能成果，包括端到端推理效率和算子吞吐量的提升。并展望了未来在软FP4技术研发、深度挖掘昇腾算力及生态扩展等方向的探索计划。"
      },
      {
        "id": "d3_1",
        "label": "成果：显著性能提升",
        "category": "Result",
        "references": {
          "section_ids": [
            "sec_2",
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_007",
            "chunk_00/slide_013"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_013",
            "primary_time_ts": "00:16:16,000",
            "primary_time_ms": 976000
          }
        },
        "parent": "p3",
        "summary": "通过一系列优化，实现了32%的端到端推理效率提升。在算子层面，GroupedMatmul和普通Matmul的FP8吞吐量分别平均提升了40%和25%。"
      },
      {
        "id": "d3_2",
        "label": "未来探索方向",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_4"
          ],
          "material_ids": [
            "chunk_00/slide_014",
            "chunk_00/slide_015"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_015",
            "primary_time_ts": "00:16:41,500",
            "primary_time_ms": 1001500
          }
        },
        "parent": "p3",
        "summary": "团队未来的探索主要包括三个方向：研发软FP4技术以探索更低比特；持续深度挖掘昇腾芯片的算力潜力；将生态扩展作为明年重点，与社区建立更紧密联系。"
      }
    ],
    "edges": [
      {
        "source": "p1",
        "target": "p2",
        "relation_type": "sequential",
        "label": "引出方案"
      },
      {
        "source": "p2",
        "target": "p3",
        "relation_type": "sequential",
        "label": "展示成果"
      },
      {
        "source": "d1_1",
        "target": "d1_2",
        "relation_type": "parallel",
        "label": "并列挑战"
      },
      {
        "source": "d1_2",
        "target": "d2_1",
        "relation_type": "causal",
        "label": "促使研发"
      },
      {
        "source": "d1_3",
        "target": "d2_1",
        "relation_type": "causal",
        "label": "奠定理论基础"
      },
      {
        "source": "d2_1",
        "target": "d2_2",
        "relation_type": "compositional",
        "label": "包含算子级优化"
      },
      {
        "source": "d2_1",
        "target": "d2_6",
        "relation_type": "compositional",
        "label": "包含框架级优化"
      },
      {
        "source": "d2_2",
        "target": "d2_3",
        "relation_type": "compositional",
        "label": "具体技术"
      },
      {
        "source": "d2_3",
        "target": "d2_4",
        "relation_type": "parallel",
        "label": "并列优化技术"
      },
      {
        "source": "d2_4",
        "target": "d2_5",
        "relation_type": "parallel",
        "label": "并列优化技术"
      },
      {
        "source": "d2_1",
        "target": "d3_1",
        "relation_type": "causal",
        "label": "实现"
      },
      {
        "source": "d3_1",
        "target": "d3_2",
        "relation_type": "sequential",
        "label": "展望未来"
      }
    ]
  },
  "sections": [
    {
      "id": "sec_1",
      "index": 0,
      "title": "开场：软硬协同，效能飞跃",
      "time_range": "00:00:00,000 - 00:01:42,355",
      "summary": "讲者孙亚楠开场，介绍了本次分享的主题：在昇腾平台上针对FP8精度算子的调优工作。他指出现有大模型面临算力与内存瓶颈，并提出本次分享将要解决的核心问题，即如何在不支持FP8的硬件上高效运行FP8权重。",
      "refined_script": "# 开场：软硬协同，实现 FP8 算子性能飞跃\n\n## 1. 背景与挑战\n\n随着大模型规模的持续增长，算力与内存已成为制约其进一步扩展和性能发挥的关键瓶颈。在昇腾平台上，若要追求极致性能，传统的 BF16 或 FP16 精度存在以下挑战：\n\n- **高资源消耗**：模型加载和运行时资源占用巨大。\n- **硬件限制**：当前的昇腾芯片（如 910B、910B3 及更早版本）的计算单元尚不支持 FP8 数据类型的原生计算。\n\n## 2. 核心问题\n\n基于上述背景，本次分享旨在解决一个核心技术问题：\n\n> 如何在不支持 FP8 原生计算的昇腾硬件上，高效地运行使用 FP8 权重的模型？\n\n## 3. 分享大纲\n\n本文将从三个部分展开，介绍我们团队在昇腾平台上针对此问题的调优实践。",
      "original_script": "下面，有请河南昆仑技术有限公司算子开发高级工程师孙亚楠先生，\n为我们带来河南昆仑基于昇腾软件FP8的极致调优分享。大家欢迎。\n大家好，我是河南昆仑技术有限公司的孙亚楠。\n刚才前面的老师讲了很多关于算子的内容，但没有展开讲解算子的优化具体是如何实现的。\n今天，我有幸在这里和大家分享我们团队在昇腾平台上，针对FP8精度算子的调优工作。\n在开始之前，我想问大家一个问题：大家是否遇到过以下情况？\n我们知道，现在的模型越来越大，对精度的要求也越来越高。\n然而，当前的算力与内存瓶颈，就像孙悟空的紧箍咒，\n牢牢地限制了模型的扩展和算力的发挥。\n如果想在昇腾平台上跑出极致性能，\n传统的BF16或FP16精度，在加载和资源耗用方面都非常大。\n针对当前昇腾910B、910B3或之前的芯片，其计算单元目前尚不支持FP8数据类型。\n那么，我们如何让FP8的权重在这些不支持FP8的硬件上运行呢？\n下面我将从三个部分为大家分享。",
      "start_ms": 0,
      "end_ms": 102355,
      "material_ids": [
        "chunk_00/slide_001",
        "chunk_00/slide_002"
      ],
      "summary_hint": "讲者孙亚楠开场，介绍本次分享的主题是关于在昇腾平台上，针对FP8精度算子的调优工作。",
      "notes": "本节包含主持人开场白与演讲者开场部分。",
      "lead_text": "下面，有请河南昆仑技术有限公司算子开发高级工程师孙亚楠先生，",
      "tail_text": "下面我将从三个部分为大家分享。",
      "text_span": {
        "start": 0,
        "end": 13
      }
    },
    {
      "id": "sec_2",
      "index": 1,
      "title": "技术概述：FP8及其应用价值",
      "time_range": "00:01:42,355 - 00:08:36,845",
      "summary": "本节首先介绍了FP8技术，它是一种专为AI计算优化的新兴数值格式，能以减半的显存需求和比INT8更高的精度，完美平衡效率与精度。为解决FP8对特定硬件的高度依赖问题，讲者团队基于昇腾Ascend C框架研发了软FP8解决方案。该方案通过创新的反量化算子和框架级优化，实现了32%的端到端推理效率提升，并能在精度几乎无损的情况下，让现有硬件运行更大规模的模型。",
      "refined_script": "# 技术概述：FP8及其应用价值\n\n## 一、引言：团队技术栈概览\n\n为更好地理解软FP8算子的背景，首先介绍我们团队构建的自上而下的全栈软件解决方案：\n\n- **模型层**: 主要支持 `Qwen` 和 `DeepSeek` 系列模型。\n- **推理框架层**: 聚焦 `vLLM-Ascend` 和 `vLLM`，并在此基础上开发了自研推理框架。\n- **算子层**: 针对业界常用算子进行深度优化。例如，`LSTM` 算子经调优后整体性能比业界已知方案提升约30%。本次分享的重点是**软FP8算子**。\n- **系统层**: 在基于 openEuler 的 `FusionOS` 操作系统上进行系统级调优。\n- **硬件层**: 支持多种国产加速卡，包括昇腾、燧原、昆仑芯、天数，并正在适配壁仞。\n\n## 二、FP8 技术简介\n\n### 什么是 FP8？\n\nFP8（8-bit Floating Point）是一种专为AI计算优化的新兴数值表示格式。它旨在保持高精度的同时，显著降低计算与存储资源的消耗。\n\n### FP8 的核心价值\n\nFP8 在效率与精度之间取得了卓越的平衡，其核心价值体现在以下三个方面：\n\n1.  **显存需求减半**: 与 `FP16` 和 `BF16` 相比，FP8 可将模型权重占用的显存减半，大幅缓解硬件资源压力。例如，原需16张卡运行的 `DeepSeek` 模型，使用FP8后仅需8张卡即可运行。\n\n2.  **更高的推理精度**: 相较于目前流行的 `INT8` 量化方案，FP8 能够提供更高的推理精度，有效减少量化带来的精度损失。\n\n3.  **效率与精度的完美平衡**: 综合以上两点，FP8 成为了大模型领域兼顾性能与效果的理想选择。\n\n### 行业趋势与挑战\n\n- **行业趋势**: FP8 已成为大模型训练与推理领域的“新宠”，越来越多的AI模型开始采用FP8格式。预计到2025年，FP8将成为大模型的主流低精度技术选型。\n- **核心挑战**: FP8 的高效计算长期以来高度依赖特定硬件（尤其是国外芯片）的原生支持，这严重制约了相关技术的自主发展与广泛应用。\n\n## 三、基于昇腾 Ascend C 的软 FP8 解决方案\n\n为解决FP8对特定硬件的依赖问题，我们基于昇腾 `Ascend C` 算子开发框架，成功研发出**软FP8解决方案**。\n\n### 方案概述\n\n该方案的核心思想是在软件层面实现对FP8权重的支持，其工作流程如下：\n\n1.  **输入**: 直接加载FP8格式的权重模型到昇腾硬件。\n2.  **处理**: 通过自研的**反量化算子**，在计算时将FP8权重动态转换为 `BF16` 格式参与运算。\n3.  **优势**: \n    - 保障了计算过程的准确性。\n    - 避免了部署过程中权重格式的多次转换，提高了效率。\n    - 为未来硬件原生支持FP8时快速适配预留了灵活空间。\n\n### 核心优化策略\n\n方案的性能提升源于算子级和框架级的深度优化。\n\n#### 1. 算子级优化\n\n- **专家动态反量化**: 在Kernel内创新性地提出动态反量化方案。\n- **算子融合**: 将Vector与Cube两类算子进行融合，高效调度不同计算核心的任务。\n- **流水线优化**: 通过探寻最优分块策略、数据预取与乱序执行等技术，彻底消除计算流水线气泡。\n\n#### 2. 框架级优化\n\n- **高效算子下发**: 通过PyTorch的 `meta` 函数实现算子入图，并基于**懒加载（Lazy Loading）**方式下发，避免了单个算子依次下发带来的性能开销。\n- **动态路径调整**: 利用模型特征的智能感知能力，实现计算路径的动态调整，选择最优执行策略。\n\n## 四、应用成果与价值\n\n该软FP8解决方案在多个维度取得了显著成果。\n\n- **性能与效率**: \n    - 实现 **32% 的端到端推理效率提升**。\n    - 在模型精度几乎无损的前提下，单台设备即可流畅运行完整版 `DeepSeek V3.1`。\n    - 在板卡机型上，可支持的模型参数规模翻倍，从而运行更大规模的模型。\n    - 大幅提升并发处理能力，让不同硬件配置的用户都能享受到FP8的技术红利。\n\n- **模型兼容性**: \n    - 全面兼容 `DeepSeek V3.1`、`DeepSeek-V3/R1`、`Qwen3` 等主流FP8量化模型。\n\n- **精度表现**: \n    - 精度测试结果显示，该方案的推理精度与 `INT8+INT4` 混合精度方案相当，实现了近乎无损的性能优化。",
      "original_script": "首先，我会介绍什么是FP8，以及它能带来什么价值。\n第二，我会汇报我们团队基于FP8所做的算子设计与优化工作。\n第三，分享我们团队对未来的一些思考。\n在开始之前，先介绍一下我们团队正在构建的从上到下的整体软件栈。\n首先在模型层，我们目前主要支持Qwen和DeepSeek这两个系列的模型。\n在中间的推理框架层，我们主要集中在vLLM-Ascend和vLLM，并在此基础上开发了自研的推理框架。\n再往下是算子层。我们会针对业界常用的算子进行特定调优。\n例如LSTM，我们目前的调优成果比业界已知方案的整体性能提升了约30%。\n整体性能表现很好。但今天重点不是LSTM，而是软FP8算子。\n在下层，我们会在不同的操作系统上运行我们的算子。今天主要是在我们基于openEuler的FusionOS操作系统上进行了一些系统层面的调优。\n再往下是硬件层，支持各种国产加速卡，例如燧原、昆仑芯、天数、昇腾，以及我们最近正在适配的壁仞。\n接下来，我们开始介绍FP8技术本身。\nFP8是一种新兴的数值表示格式，专为AI计算优化。\n它能在保持高精度的同时，显著降低计算资源需求。\n与传统的FP16和BF16相比，FP8能将显存需求减半。与目前流行的INT8相比，FP8的推理精度更高。因此，它完美平衡了效率与精度。\n我们从三个方面来看。第一，显存需求减半。与BF16相比，权重占用的显存减半，因此能大幅缓解硬件资源压力。\n例如DeepSeek模型，之前需要16张卡，现在用8张卡就能运行，硬件资源直接减半。\n第二，更高的推理精度。社区（如Hugging Face）上已有的INT8权重文件，其精度略有下降。我们在此基础上推出FP8，将精度重新提升了回来。\n最终，基于以上两点，我们实现了效率与精度的完美平衡。\n在当前的AI应用场景中，无论是从社区还是大模型的训练与推理结果来看，FP8已成为大模型领域的“新宠”，越来越多的AI模型开始采用FP8格式进行训练和推理。\n而且，长期以来，FP8精度的支持高度依赖国外芯片。目前，越来越多的国产芯片也开始支持FP8，甚至FP4，严重制约了相关技术的自主发展与广泛应用。\n在2025年，FP8低精度类型已成为大模型主流选择，许多企业和研究机构都在尝试如何在不同硬件平台上实现极高的FP8计算。\n当前FP8技术面临的主要挑战是对特定硬件的高度依赖。为解决这一问题，河南昆仑技术有限公司基于昇腾易用的Ascend C算子开发框架，成功研发出软FP8解决方案。\n该解决方案主要分为三个层面。第一，软FP8助力硬件支持FP8权重。我们将FP8权重模型输入昇腾硬件，通过我们自研的反量化算子，将其转化为BF16格式参与计算。\n这在保障计算过程的准确性，同时也为后续使用FP8权重模型进行快速适配预留了灵活空间。\n无需权重格式的多次切换，提高了部署效率。主要的核心是算子级优化和框架级优化。\n在算子创新方面，我们进行了多项优化。由于时间关系，这里只列举几点。\n例如，创新性地在Kernel内提出专家动态反量化方案。\n将Vector与Cube两部分算子进行融合，高效调度两类核心的计算任务。\n通过探寻分块策略、数据预取与乱序等，彻底消除计算流水线气泡。\n在框架级优化方面，通过PyTorch的Meta函数实现入图，使得自研FP8反量化算子基于懒加载下发，避免了单个算子依次下发带来的性能开销。\n利用模型特征的智能感知，实现计算路径的动态调整。\n端到端推理效率提升32%。\n该技术在不同硬件配置上均表现出色。在模型精度几乎无损的前提下，单台设备即可流畅运行满血版DeepSeek V3.1。\n在板卡机型上，能实现模型参数规模翻番，运行更大规模的模型。\n大幅提升并发处理能力，让不同硬件配置的用户都能享受到FP8推理的技术红利。\n该方案实现了广泛的模型兼容性，全面兼容DeepSeek V3.1、DeepSeek-V3/R1、Qwen3等主流FP8量化模型。\n右侧是软浮点推理精度测试结果，相较于DeepSeek-R1的原始格式测试结果，FP8与INT8+INT4的精度相当。",
      "start_ms": 102355,
      "end_ms": 516845,
      "material_ids": [
        "chunk_00/slide_003",
        "chunk_00/slide_004",
        "chunk_00/slide_005",
        "chunk_00/slide_006",
        "chunk_00/slide_007"
      ],
      "summary_hint": "介绍FP8技术概念，它能缓解显存压力、提供更高推理精度。并阐述了昆仑基于昇腾研发的软FP8解决方案及其在不同硬件上的应用价值。",
      "notes": "规划的首句与上一节尾句重合，为保证语义独立，将本节起始句调整为字幕 #15 '首先，我会介绍什么是FP8...'",
      "lead_text": "下面我将从三个部分为大家分享。",
      "tail_text": "右侧是软浮点推理精度测试结果，相较于DeepSeek-R1的原始格式测试结果，FP8与INT8+INT4的精度相当。",
      "text_span": {
        "start": 13,
        "end": 51
      }
    },
    {
      "id": "sec_3",
      "index": 2,
      "title": "算子调优：FP8 Matmul/GroupedMatmul 优化实践",
      "time_range": "00:08:38,500 - 00:16:33,180",
      "summary": "该部分介绍了团队自研的FP8算子优化方案，核心在于动态反量化。报告重点阐述了三项优化实践：基于MoE专家激活信息的选择性反量化、针对大Shape的带宽优化，以及利用特定模型Shape与硬件核心数亲和性的优化。这些优化最终在GroupedMatmul和普通Matmul上分别带来了40%和25%的吞吐量提升。",
      "refined_script": "# 算子调优：FP8 Matmul/GroupedMatmul 优化实践\n\n## 1. 核心方案：基于动态反量化的 FP8 算子\n\n为优化昇腾芯片上的 FP8 算子性能，我们自研了一套以动态反量化为核心的解决方案。此方案旨在规避官方原生路径因直接使用 BF16 计算而占用过多硬件资源的问题。\n\n**原生计算路径 (Ascend):**\n\n1.  输入：BF16 激活值、BF16 权重\n2.  计算：在 Cube Core 上直接进行 BF16 Matmul/GroupedMatmul\n3.  输出：BF16 结果\n\n**自研优化路径:**\n\n1.  **动态反量化 (Vector Core):** 首先在 Vector Core 上对 FP8 权重进行动态反量化，将其转换为昇腾芯片支持的 BF16 格式。\n2.  **矩阵乘法 (Cube Core):** 接着，在 Cube Core 上使用反量化后的 BF16 权重与输入的 BF16 激活值进行矩阵乘法计算。\n3.  **类型转换 (Vector Core):** 最后，在 Vector Core 上对 FP32 的矩阵乘法中间结果进行类型转换，得到最终输出。\n\n## 2. 专项优化实践\n\n我们针对不同场景设计了特定的优化策略，以下为三项关键实践。\n\n### 2.1 优化一：基于 MoE 专家激活信息的选择性反量化\n\n在 MoE (Mixture of Experts) 模型中，并非所有专家都会在一次前向传播中被激活。利用这一特性，我们为算子增加了 `group_list` 参数，用于传递当前激活的专家信息。\n\n- **优化逻辑**：算子根据 `group_list` 动态调度计算任务，仅对被激活专家的权重数据执行反量化和矩阵乘法计算，而完全跳过未激活专家的处理流程，从而显著减少不必要的计算开销。\n- **执行示例**：\n  - 当专家 0 被激活时，其相关数据被分配至指定 AI Core 进行运算。\n  - 若专家 1 未被激活，则其对应数据不进行任何处理。\n  - 当专家 2 被激活时，其数据块可能由一个或多个 AI Core 接力处理，以保证计算资源的有效利用。\n\n### 2.2 优化二：针对大 Shape 的带宽优化\n\n针对大尺寸矩阵（Large Shape）的计算场景，我们通过改进调度策略来优化显存带宽和反量化性能。\n\n- **原调度策略**：昇腾原生算子主要采用行优先（Row-major）的调度策略。\n- **优化调度策略**：我们提出了列优先（Column-major）的调度策略。该策略优先调度基本块（Basic Block），再处理尾块（Tail Block），使得计算任务在 AI Core 之间能达到更好的负载均衡。\n\n**实现细节**：\n1.  通过设置更大的基本块，将数据切分为若干小块。\n2.  AI Core 以列为单位，依次对每一行进行反量化计算。\n3.  在 L0 Cache 上融合 FP8 权重与 FP32 反量化系数，完成计算后将数据转换为 BF16 格式。\n\n**性能对比**：\n\n| Shape 类型 | 显存带宽提升 | 反量化性能提升 |\n| :--- | :--- | :--- |\n| 大 Shape | 50% - 65% | 51% - 66% |\n| 小 Shape | 提升不明显 | 提升不明显 |\n\n该优化在大 Shape 场景下效果显著，而在小 Shape 场景下虽有提升，但幅度有限。\n\n### 2.3 优化三：针对 Qwen3-32B-FP8 的 Shape 亲和性优化\n\n在适配 Qwen3-32B-FP8 模型时，我们发现其 `down` 权重矩阵的反量化系数行数（300）恰好等于昇腾 A2 标卡的 Vector Core 数量。\n\n- **核心思路**：利用模型 Shape 与硬件核心数的天然亲和性，将计算任务按行均匀分配给每个 AI Core。这不仅实现了完美的负载均衡，还改善了每个 AI Core 的访存空间局部性。\n\n- **实现技巧**：通过代数等价替换，我们调整了转置（Transpose）与反量化（Dequantization）的运算顺序。\n  - **优化前**：先对一个行为 40 的矩阵进行反量化，再进行转置和 Matmul 计算。\n  - **优化后**：先进行转置，将 40 列转换为 40 行，再进行反量化和 Matmul 计算。\n\n此项调整使得反量化系数矩阵的列数大幅增加，从而可以有效利用前述的“大 Shape 带宽优化”策略。同时，该方法使得反量化系数达到了 40 倍的权重复用，进一步加速了计算过程。\n\n## 3. 端到端性能总结\n\n通过上述一系列优化，我们自研的 FP8 算子在端到端性能上取得了显著提升。\n\n- **GroupedMatmul (`down` 层)**：FP8 吞吐量相较于开源版本平均提升 **40%**。\n- **普通 Matmul (`gate` 和 `up` 层)**：FP8 吞吐量相较于开源版本平均提升 **25%**。",
      "original_script": "下面，我将汇报我们团队关于软FP8软浮点算子的设计。\n我们针对不同场景进行特定优化，这里介绍其中三点。\n我们自研FP8算子整体方案，核心在于动态反量化。\n昇腾官方的原始路径是直接使用BF16的激活值和权重，通过Matmul或GroupedMatmul进行计算，最终输出BF16的结果。\n但这种方案需要较多的硬件资源。\n因此，我们首先在Vector Core对FP8权重进行动态反量化，将其转换为昇腾芯片支持的BF16格式。\n接着，在Cube Core对反量化后的权重和输入激活值进行BF16矩阵乘法。\n最后，Vector Core再对FP32的矩阵乘法结果作类型转换。\n我们针对不同场景进行了特定优化，下面介绍其中三点。\n优化一：基于MOE专家激活信息的选择性反量化。\n优化二：针对大Shape的带宽优化。\n优化三：针对特定模型Shape亲和性优化。\n它与昇腾标卡的核数比较亲和。\n我们可以完全复用，且没有任何尾核处理。\n首先是基于专家激活信息的选择性反量化。我们在算子中增加了一个group_list参数，用于描述模型中专家的激活情况。\n根据group_list，可以动态调度激活专家的计算基本块，对未激活的专家不进行反量化计算，以减少耗时。\n我们可以看一下左侧的图示。\n当专家0被激活时，我们会将专家0相关的数据分配到对应的核上进行运算。\n可以看到，专家1未被激活，因此我们不处理其对应的数据。\n当专家2被激活时，其对应的数据会在第五个核处理完后，继续由第六个核进行划分处理。\n这个过程的具体实现如下。\n优化二：针对大Shape的带宽优化。\n我们通过设置更大的基本块，将数据分成若干小块。\n然后将这些小块与scalar进行计算。\n具体来说，AI CORE依次对每一行进行反量化计算，循环128次。依次读取FP8权重和FP32反量化系数，权重矩阵与融合系数在L0C上融合，最后将权重数据转换成FP32-BF16。\n昇腾原本主要采用行优先的调度策略。\n我们则提出了列优先的调度策略，先调度基本块，再调度尾块。\n这使得基本块和尾块都能在AI CORE之间达到负载均衡。\n下面是一些测试数据。\n在特定Shape下，我们对比了带宽和反量化性能。\n结果显示，在大Shape下，显存带宽提升50%-65%。\n反量化性能提升51%-66%。而在小Shape下，提升不明显。\n但仍有提升，例如从32.4提升到35.61。\n提升幅度不明显。\n优化三：Qwen3-32B-FP8 Shape亲和优化。在适配Qwen3-32B-FP8模型时，我们发现其down反量化系数的行恰好等于300个A2标卡的Vector Core数量。\n其权重和系数矩阵可以按行将计算任务均匀分配给每个AI CORE。\n因此，我们按行将计算任务均匀分配给每个AI CORE，既达到了完美的负载均衡，又改善了每个AI CORE的访存空间局部性。\n下面是具体实现，我们通过代数等价替换，调整了转置操作与反量化的顺序。\n这可以令反量化系数矩阵达到40的权重复用，且列数大大增加，适合利用优化二提到的大Shape优化策略，加速计算。\n对比上下两图，当40为行时，我们先进行反量化，再进行转置和Matmul计算。\n当40为列时，我们先进行转置，将列转换为行，再进行反量化和Matmul计算。\n这是我们整体的端到端性能对比。\n在GroupedMatmul down层，FP8的吞吐量相比开源版本平均提升40%。\n在普通Matmul gate和up层，吞吐量相比开源版本平均提升25%。",
      "start_ms": 518500,
      "end_ms": 993180,
      "material_ids": [
        "chunk_00/slide_008",
        "chunk_00/slide_009",
        "chunk_00/slide_010",
        "chunk_00/slide_011",
        "chunk_00/slide_012",
        "chunk_00/slide_013"
      ],
      "summary_hint": "详细介绍FP8算子的三项核心优化技术：基于专家激活信息的选择性反量化、针对大Shape的带宽优化、以及针对特定模型的Shape亲和优化，并展示了超过25%的性能提升。",
      "notes": "讲者详细介绍了针对FP8 Matmul/GroupedMatmul算子的三项核心优化技术，并展示了最终的性能提升数据。",
      "lead_text": "下面，我将汇报我们团队关于软FP8软浮点算子的设计。",
      "tail_text": "在普通Matmul gate和up层，吞吐量相比开源版本平均提升25%。",
      "text_span": {
        "start": 52,
        "end": 95
      }
    },
    {
      "id": "sec_4",
      "index": 3,
      "title": "未来探索方向",
      "time_range": "00:16:36,260 - 00:17:15,895",
      "summary": "讲者分享了团队未来的三个探索方向：首先是进行软FP4技术研发；其次是继续深度挖掘昇腾芯片的算力潜力；最后，将生态扩展作为明年的工作重点，与生态建立更紧密的联系。",
      "refined_script": "# 未来探索方向\n\n团队未来的技术探索将主要围绕以下三个方向展开：\n\n1.  **软FP4技术研发**\n    *   **背景**：昇腾（Ascend）硬件即将支持 FP8 格式。\n    *   **计划**：为进一步探索低比特技术，团队将启动软 FP4 技术的研发。\n\n2.  **深度挖掘昇腾芯片算力**\n    *   **目标**：持续深度挖掘昇腾芯片的算力潜力，以实现极致的性能表现。\n\n3.  **生态扩展**\n    *   **定位**：此项将作为下一年度的工作重点。\n    *   **目标**：与生态伙伴建立更紧密的联系，为社区发展做出更多贡献。",
      "original_script": "最后，我将汇报我们团队未来的探索方向。\n主要有三个方向。第一，软FP4技术研发。\n因为昇腾硬件即将支持FP8，所以我们需要向下探索。\n也就是研发软FP4技术。\n第二，我们将继续深度挖掘昇腾芯片的算力潜力。\n第三，也是我们明年的工作重点，即生态扩展。\n我们将与生态产生更紧密的联系，做出更多贡献。我的分享到此结束，谢谢大家。",
      "start_ms": 996260,
      "end_ms": 1035895,
      "material_ids": [
        "chunk_00/slide_014",
        "chunk_00/slide_015"
      ],
      "summary_hint": "讲者分享了团队未来的三个探索方向：软FP4技术研发、深度挖掘昇腾芯片算力潜力以及生态扩展。",
      "lead_text": "最后，我将汇报我们团队未来的探索方向。",
      "tail_text": "我们将与生态产生更紧密的联系，做出更多贡献。我的分享到此结束，谢谢大家。",
      "text_span": {
        "start": 96,
        "end": 102
      }
    },
    {
      "id": "sec_5",
      "index": 4,
      "title": "结束语",
      "time_range": "00:17:15,895 - 00:17:47,430",
      "summary": "主持人感谢孙老师的分享，并总结了其核心内容。通过FP8技术降低显存占用，并对特定形状的张量进行亲和处理，实现了高达500%的性能提升，效果显著。",
      "refined_script": "## 技术总结\n\n本次分享的核心技术是通过结合以下两种方法，实现了显著的性能优化。\n\n### 1. 核心优化方法\n\n*   **FP8 技术**：应用 FP8 数据类型，以降低显存占用。\n*   **张量亲和处理**：针对特定形状（Shape）的张量进行亲和性处理，优化计算流程。\n\n### 2. 性能提升\n\n*   综合上述技术，最终实现了高达 **500%** 的性能提升。",
      "original_script": "非常感谢孙老师的精彩分享。通过FP8技术，降低了显存占用，同时对特定Shape的张量进行亲和处理，带来了高达500%的性能提升，效果非常震撼。\n这是一个令人期待的技术，谢谢。",
      "start_ms": 1035895,
      "end_ms": 1067430,
      "material_ids": [
        "chunk_00/slide_016"
      ],
      "summary_hint": "主持人总结分享内容并致谢。",
      "notes": "本节为主持人总结陈词。",
      "lead_text": "非常感谢孙老师的精彩分享。通过FP8技术，降低了显存占用，同时对特定Shape的张量进行亲和处理，带来了高达500%的性能提升，效果非常震撼。",
      "tail_text": "这是一个令人期待的技术，谢谢。",
      "text_span": {
        "start": 103,
        "end": 104
      }
    }
  ]
}