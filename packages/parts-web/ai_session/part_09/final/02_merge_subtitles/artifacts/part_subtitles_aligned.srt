1
00:00:00,000 --> 00:00:07,890
下面，有请河南昆仑技术有限公司算子开发高级工程师孙亚楠先生，

2
00:00:07,890 --> 00:00:14,500
为我们带来河南昆仑基于昇腾软件FP8的极致调优分享。大家欢迎。

3
00:00:15,750 --> 00:00:19,270
大家好，我是河南昆仑技术有限公司的孙亚楠。

4
00:00:19,650 --> 00:00:27,750
刚才前面的老师讲了很多关于算子的内容，但没有展开讲解算子的优化具体是如何实现的。

5
00:00:27,930 --> 00:00:37,355
今天，我有幸在这里和大家分享我们团队在昇腾平台上，针对FP8精度算子的调优工作。

6
00:00:38,120 --> 00:00:43,180
在开始之前，我想问大家一个问题：大家是否遇到过以下情况？

7
00:00:43,440 --> 00:00:49,600
我们知道，现在的模型越来越大，对精度的要求也越来越高。

8
00:00:49,800 --> 00:00:55,620
然而，当前的算力与内存瓶颈，就像孙悟空的紧箍咒，

9
00:00:55,880 --> 00:01:03,700
牢牢地限制了模型的扩展和算力的发挥。

10
00:01:03,880 --> 00:01:09,230
如果想在昇腾平台上跑出极致性能，

11
00:01:09,590 --> 00:01:18,510
传统的BF16或FP16精度，在加载和资源耗用方面都非常大。

12
00:01:18,950 --> 00:01:27,150
针对当前昇腾910B、910B3或之前的芯片，其计算单元目前尚不支持FP8数据类型。

13
00:01:27,350 --> 00:01:36,700
那么，我们如何让FP8的权重在这些不支持FP8的硬件上运行呢？

14
00:01:36,700 --> 00:01:42,355
下面我将从三个部分为大家分享。

15
00:01:42,355 --> 00:01:50,450
首先，我会介绍什么是FP8，以及它能带来什么价值。

16
00:01:50,810 --> 00:02:00,270
第二，我会汇报我们团队基于FP8所做的算子设计与优化工作。

17
00:02:00,510 --> 00:02:04,585
第三，分享我们团队对未来的一些思考。

18
00:02:06,000 --> 00:02:13,560
在开始之前，先介绍一下我们团队正在构建的从上到下的整体软件栈。

19
00:02:13,880 --> 00:02:21,720
首先在模型层，我们目前主要支持Qwen和DeepSeek这两个系列的模型。

20
00:02:22,160 --> 00:02:32,340
在中间的推理框架层，我们主要集中在vLLM-Ascend和vLLM，并在此基础上开发了自研的推理框架。

21
00:02:32,340 --> 00:02:45,230
再往下是算子层。我们会针对业界常用的算子进行特定调优。

22
00:02:45,750 --> 00:02:55,890
例如LSTM，我们目前的调优成果比业界已知方案的整体性能提升了约30%。

23
00:02:55,890 --> 00:03:02,830
整体性能表现很好。但今天重点不是LSTM，而是软FP8算子。

24
00:03:03,210 --> 00:03:19,940
在下层，我们会在不同的操作系统上运行我们的算子。今天主要是在我们基于openEuler的FusionOS操作系统上进行了一些系统层面的调优。

25
00:03:20,200 --> 00:03:30,885
再往下是硬件层，支持各种国产加速卡，例如燧原、昆仑芯、天数、昇腾，以及我们最近正在适配的壁仞。

26
00:03:32,550 --> 00:03:38,650
接下来，我们开始介绍FP8技术本身。

27
00:03:39,030 --> 00:03:45,450
FP8是一种新兴的数值表示格式，专为AI计算优化。

28
00:03:45,730 --> 00:03:50,215
它能在保持高精度的同时，显著降低计算资源需求。

29
00:03:50,940 --> 00:04:09,595
与传统的FP16和BF16相比，FP8能将显存需求减半。与目前流行的INT8相比，FP8的推理精度更高。因此，它完美平衡了效率与精度。

30
00:04:10,360 --> 00:04:23,200
我们从三个方面来看。第一，显存需求减半。与BF16相比，权重占用的显存减半，因此能大幅缓解硬件资源压力。

31
00:04:23,460 --> 00:04:31,215
例如DeepSeek模型，之前需要16张卡，现在用8张卡就能运行，硬件资源直接减半。

32
00:04:32,030 --> 00:04:53,445
第二，更高的推理精度。社区（如Hugging Face）上已有的INT8权重文件，其精度略有下降。我们在此基础上推出FP8，将精度重新提升了回来。

33
00:04:54,070 --> 00:04:58,410
最终，基于以上两点，我们实现了效率与精度的完美平衡。

34
00:04:58,830 --> 00:05:18,150
在当前的AI应用场景中，无论是从社区还是大模型的训练与推理结果来看，FP8已成为大模型领域的“新宠”，越来越多的AI模型开始采用FP8格式进行训练和推理。

35
00:05:18,470 --> 00:05:35,915
而且，长期以来，FP8精度的支持高度依赖国外芯片。目前，越来越多的国产芯片也开始支持FP8，甚至FP4，严重制约了相关技术的自主发展与广泛应用。

36
00:05:35,915 --> 00:05:45,710
在2025年，FP8低精度类型已成为大模型主流选择，许多企业和研究机构都在尝试如何在不同硬件平台上实现极高的FP8计算。

37
00:05:45,710 --> 00:05:54,150
当前FP8技术面临的主要挑战是对特定硬件的高度依赖。为解决这一问题，河南昆仑技术有限公司基于昇腾易用的Ascend C算子开发框架，成功研发出软FP8解决方案。

38
00:05:54,490 --> 00:06:14,725
该解决方案主要分为三个层面。第一，软FP8助力硬件支持FP8权重。我们将FP8权重模型输入昇腾硬件，通过我们自研的反量化算子，将其转化为BF16格式参与计算。

39
00:06:15,770 --> 00:06:23,030
这在保障计算过程的准确性，同时也为后续使用FP8权重模型进行快速适配预留了灵活空间。

40
00:06:23,510 --> 00:06:32,230
无需权重格式的多次切换，提高了部署效率。主要的核心是算子级优化和框架级优化。

41
00:06:32,830 --> 00:06:44,590
在算子创新方面，我们进行了多项优化。由于时间关系，这里只列举几点。

42
00:06:45,010 --> 00:06:50,815
例如，创新性地在Kernel内提出专家动态反量化方案。

43
00:06:51,870 --> 00:06:59,095
将Vector与Cube两部分算子进行融合，高效调度两类核心的计算任务。

44
00:06:59,930 --> 00:07:12,710
通过探寻分块策略、数据预取与乱序等，彻底消除计算流水线气泡。

45
00:07:13,190 --> 00:07:25,275
在框架级优化方面，通过PyTorch的Meta函数实现入图，使得自研FP8反量化算子基于懒加载下发，避免了单个算子依次下发带来的性能开销。

46
00:07:26,650 --> 00:07:35,175
利用模型特征的智能感知，实现计算路径的动态调整。

47
00:07:36,000 --> 00:07:41,295
端到端推理效率提升32%。

48
00:07:42,350 --> 00:07:59,130
该技术在不同硬件配置上均表现出色。在模型精度几乎无损的前提下，单台设备即可流畅运行满血版DeepSeek V3.1。

49
00:07:59,130 --> 00:08:09,945
在板卡机型上，能实现模型参数规模翻番，运行更大规模的模型。

50
00:08:10,630 --> 00:08:20,635
大幅提升并发处理能力，让不同硬件配置的用户都能享受到FP8推理的技术红利。

51
00:08:21,450 --> 00:08:28,245
该方案实现了广泛的模型兼容性，全面兼容DeepSeek V3.1、DeepSeek-V3/R1、Qwen3等主流FP8量化模型。

52
00:08:29,010 --> 00:08:36,845
右侧是软浮点推理精度测试结果，相较于DeepSeek-R1的原始格式测试结果，FP8与INT8+INT4的精度相当。

53
00:08:38,500 --> 00:08:44,985
下面，我将汇报我们团队关于软FP8软浮点算子的设计。

54
00:08:45,740 --> 00:08:53,075
我们针对不同场景进行特定优化，这里介绍其中三点。

55
00:08:56,200 --> 00:09:03,200
我们自研FP8算子整体方案，核心在于动态反量化。

56
00:09:03,420 --> 00:09:20,200
昇腾官方的原始路径是直接使用BF16的激活值和权重，通过Matmul或GroupedMatmul进行计算，最终输出BF16的结果。

57
00:09:20,200 --> 00:09:24,520
但这种方案需要较多的硬件资源。

58
00:09:24,900 --> 00:09:41,510
因此，我们首先在Vector Core对FP8权重进行动态反量化，将其转换为昇腾芯片支持的BF16格式。

59
00:09:41,850 --> 00:09:50,930
接着，在Cube Core对反量化后的权重和输入激活值进行BF16矩阵乘法。

60
00:09:51,330 --> 00:09:56,630
最后，Vector Core再对FP32的矩阵乘法结果作类型转换。

61
00:09:56,630 --> 00:10:03,770
我们针对不同场景进行了特定优化，下面介绍其中三点。

62
00:10:04,130 --> 00:10:09,770
优化一：基于MOE专家激活信息的选择性反量化。

63
00:10:10,210 --> 00:10:14,130
优化二：针对大Shape的带宽优化。

64
00:10:14,690 --> 00:10:19,870
优化三：针对特定模型Shape亲和性优化。

65
00:10:20,290 --> 00:10:24,490
它与昇腾标卡的核数比较亲和。

66
00:10:24,970 --> 00:10:31,755
我们可以完全复用，且没有任何尾核处理。

67
00:10:33,700 --> 00:10:45,840
首先是基于专家激活信息的选择性反量化。我们在算子中增加了一个group_list参数，用于描述模型中专家的激活情况。

68
00:10:46,680 --> 00:10:59,545
根据group_list，可以动态调度激活专家的计算基本块，对未激活的专家不进行反量化计算，以减少耗时。

69
00:11:00,290 --> 00:11:03,310
我们可以看一下左侧的图示。

70
00:11:03,810 --> 00:11:08,870
当专家0被激活时，我们会将专家0相关的数据分配到对应的核上进行运算。

71
00:11:09,210 --> 00:11:12,870
可以看到，专家1未被激活，因此我们不处理其对应的数据。

72
00:11:13,450 --> 00:11:21,245
当专家2被激活时，其对应的数据会在第五个核处理完后，继续由第六个核进行划分处理。

73
00:11:22,190 --> 00:12:59,400
这个过程的具体实现如下。

74
00:12:59,400 --> 00:13:08,385
优化二：针对大Shape的带宽优化。

75
00:13:09,150 --> 00:13:13,070
我们通过设置更大的基本块，将数据分成若干小块。

76
00:13:13,070 --> 00:13:19,290
然后将这些小块与scalar进行计算。

77
00:13:19,450 --> 00:13:32,470
具体来说，AI CORE依次对每一行进行反量化计算，循环128次。依次读取FP8权重和FP32反量化系数，权重矩阵与融合系数在L0C上融合，最后将权重数据转换成FP32-BF16。

78
00:13:32,790 --> 00:13:38,695
昇腾原本主要采用行优先的调度策略。

79
00:13:39,180 --> 00:13:46,540
我们则提出了列优先的调度策略，先调度基本块，再调度尾块。

80
00:13:46,740 --> 00:13:51,465
这使得基本块和尾块都能在AI CORE之间达到负载均衡。

81
00:13:52,100 --> 00:13:54,340
下面是一些测试数据。

82
00:13:54,880 --> 00:14:01,060
在特定Shape下，我们对比了带宽和反量化性能。

83
00:14:01,560 --> 00:14:09,700
结果显示，在大Shape下，显存带宽提升50%-65%。

84
00:14:10,260 --> 00:14:16,240
反量化性能提升51%-66%。而在小Shape下，提升不明显。

85
00:14:16,300 --> 00:14:23,950
但仍有提升，例如从32.4提升到35.61。

86
00:14:24,670 --> 00:14:26,295
提升幅度不明显。

87
00:14:28,800 --> 00:14:49,245
优化三：Qwen3-32B-FP8 Shape亲和优化。在适配Qwen3-32B-FP8模型时，我们发现其down反量化系数的行恰好等于300个A2标卡的Vector Core数量。

88
00:14:49,970 --> 00:15:17,010
其权重和系数矩阵可以按行将计算任务均匀分配给每个AI CORE。

89
00:15:17,010 --> 00:15:31,055
因此，我们按行将计算任务均匀分配给每个AI CORE，既达到了完美的负载均衡，又改善了每个AI CORE的访存空间局部性。

90
00:15:31,860 --> 00:15:41,760
下面是具体实现，我们通过代数等价替换，调整了转置操作与反量化的顺序。

91
00:15:42,180 --> 00:15:45,245
这可以令反量化系数矩阵达到40的权重复用，且列数大大增加，适合利用优化二提到的大Shape优化策略，加速计算。

92
00:15:45,890 --> 00:16:02,700
对比上下两图，当40为行时，我们先进行反量化，再进行转置和Matmul计算。

93
00:16:02,700 --> 00:16:13,705
当40为列时，我们先进行转置，将列转换为行，再进行反量化和Matmul计算。

94
00:16:15,780 --> 00:16:20,860
这是我们整体的端到端性能对比。

95
00:16:20,860 --> 00:16:27,480
在GroupedMatmul down层，FP8的吞吐量相比开源版本平均提升40%。

96
00:16:27,480 --> 00:16:33,180
在普通Matmul gate和up层，吞吐量相比开源版本平均提升25%。

97
00:16:36,260 --> 00:16:41,840
最后，我将汇报我们团队未来的探索方向。

98
00:16:41,920 --> 00:16:48,960
主要有三个方向。第一，软FP4技术研发。

99
00:16:49,220 --> 00:16:53,320
因为昇腾硬件即将支持FP8，所以我们需要向下探索。

100
00:16:53,660 --> 00:16:56,945
也就是研发软FP4技术。

101
00:16:57,780 --> 00:17:02,840
第二，我们将继续深度挖掘昇腾芯片的算力潜力。

102
00:17:03,200 --> 00:17:11,100
第三，也是我们明年的工作重点，即生态扩展。

103
00:17:11,100 --> 00:17:15,895
我们将与生态产生更紧密的联系，做出更多贡献。我的分享到此结束，谢谢大家。

104
00:17:15,895 --> 00:17:43,170
非常感谢孙老师的精彩分享。通过FP8技术，降低了显存占用，同时对特定Shape的张量进行亲和处理，带来了高达500%的性能提升，效果非常震撼。

105
00:17:43,330 --> 00:17:47,430
这是一个令人期待的技术，谢谢。

106
00:17:47,430 --> 00:17:48,000
[音乐]
