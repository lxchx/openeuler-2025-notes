# 算子调优：FP8 Matmul/GroupedMatmul 优化实践

## 1. 核心方案：基于动态反量化的 FP8 算子

为优化昇腾芯片上的 FP8 算子性能，我们自研了一套以动态反量化为核心的解决方案。此方案旨在规避官方原生路径因直接使用 BF16 计算而占用过多硬件资源的问题。

**原生计算路径 (Ascend):**

1.  输入：BF16 激活值、BF16 权重
2.  计算：在 Cube Core 上直接进行 BF16 Matmul/GroupedMatmul
3.  输出：BF16 结果

**自研优化路径:**

1.  **动态反量化 (Vector Core):** 首先在 Vector Core 上对 FP8 权重进行动态反量化，将其转换为昇腾芯片支持的 BF16 格式。
2.  **矩阵乘法 (Cube Core):** 接着，在 Cube Core 上使用反量化后的 BF16 权重与输入的 BF16 激活值进行矩阵乘法计算。
3.  **类型转换 (Vector Core):** 最后，在 Vector Core 上对 FP32 的矩阵乘法中间结果进行类型转换，得到最终输出。

## 2. 专项优化实践

我们针对不同场景设计了特定的优化策略，以下为三项关键实践。

### 2.1 优化一：基于 MoE 专家激活信息的选择性反量化

在 MoE (Mixture of Experts) 模型中，并非所有专家都会在一次前向传播中被激活。利用这一特性，我们为算子增加了 `group_list` 参数，用于传递当前激活的专家信息。

- **优化逻辑**：算子根据 `group_list` 动态调度计算任务，仅对被激活专家的权重数据执行反量化和矩阵乘法计算，而完全跳过未激活专家的处理流程，从而显著减少不必要的计算开销。
- **执行示例**：
  - 当专家 0 被激活时，其相关数据被分配至指定 AI Core 进行运算。
  - 若专家 1 未被激活，则其对应数据不进行任何处理。
  - 当专家 2 被激活时，其数据块可能由一个或多个 AI Core 接力处理，以保证计算资源的有效利用。

### 2.2 优化二：针对大 Shape 的带宽优化

针对大尺寸矩阵（Large Shape）的计算场景，我们通过改进调度策略来优化显存带宽和反量化性能。

- **原调度策略**：昇腾原生算子主要采用行优先（Row-major）的调度策略。
- **优化调度策略**：我们提出了列优先（Column-major）的调度策略。该策略优先调度基本块（Basic Block），再处理尾块（Tail Block），使得计算任务在 AI Core 之间能达到更好的负载均衡。

**实现细节**：
1.  通过设置更大的基本块，将数据切分为若干小块。
2.  AI Core 以列为单位，依次对每一行进行反量化计算。
3.  在 L0 Cache 上融合 FP8 权重与 FP32 反量化系数，完成计算后将数据转换为 BF16 格式。

**性能对比**：

| Shape 类型 | 显存带宽提升 | 反量化性能提升 |
| :--- | :--- | :--- |
| 大 Shape | 50% - 65% | 51% - 66% |
| 小 Shape | 提升不明显 | 提升不明显 |

该优化在大 Shape 场景下效果显著，而在小 Shape 场景下虽有提升，但幅度有限。

### 2.3 优化三：针对 Qwen3-32B-FP8 的 Shape 亲和性优化

在适配 Qwen3-32B-FP8 模型时，我们发现其 `down` 权重矩阵的反量化系数行数（300）恰好等于昇腾 A2 标卡的 Vector Core 数量。

- **核心思路**：利用模型 Shape 与硬件核心数的天然亲和性，将计算任务按行均匀分配给每个 AI Core。这不仅实现了完美的负载均衡，还改善了每个 AI Core 的访存空间局部性。

- **实现技巧**：通过代数等价替换，我们调整了转置（Transpose）与反量化（Dequantization）的运算顺序。
  - **优化前**：先对一个行为 40 的矩阵进行反量化，再进行转置和 Matmul 计算。
  - **优化后**：先进行转置，将 40 列转换为 40 行，再进行反量化和 Matmul 计算。

此项调整使得反量化系数矩阵的列数大幅增加，从而可以有效利用前述的“大 Shape 带宽优化”策略。同时，该方法使得反量化系数达到了 40 倍的权重复用，进一步加速了计算过程。

## 3. 端到端性能总结

通过上述一系列优化，我们自研的 FP8 算子在端到端性能上取得了显著提升。

- **GroupedMatmul (`down` 层)**：FP8 吞吐量相较于开源版本平均提升 **40%**。
- **普通 Matmul (`gate` 和 `up` 层)**：FP8 吞吐量相较于开源版本平均提升 **25%**。