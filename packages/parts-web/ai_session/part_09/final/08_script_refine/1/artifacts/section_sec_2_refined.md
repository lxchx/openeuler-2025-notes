# 技术概述：FP8及其应用价值

## 一、引言：团队技术栈概览

为更好地理解软FP8算子的背景，首先介绍我们团队构建的自上而下的全栈软件解决方案：

- **模型层**: 主要支持 `Qwen` 和 `DeepSeek` 系列模型。
- **推理框架层**: 聚焦 `vLLM-Ascend` 和 `vLLM`，并在此基础上开发了自研推理框架。
- **算子层**: 针对业界常用算子进行深度优化。例如，`LSTM` 算子经调优后整体性能比业界已知方案提升约30%。本次分享的重点是**软FP8算子**。
- **系统层**: 在基于 openEuler 的 `FusionOS` 操作系统上进行系统级调优。
- **硬件层**: 支持多种国产加速卡，包括昇腾、燧原、昆仑芯、天数，并正在适配壁仞。

## 二、FP8 技术简介

### 什么是 FP8？

FP8（8-bit Floating Point）是一种专为AI计算优化的新兴数值表示格式。它旨在保持高精度的同时，显著降低计算与存储资源的消耗。

### FP8 的核心价值

FP8 在效率与精度之间取得了卓越的平衡，其核心价值体现在以下三个方面：

1.  **显存需求减半**: 与 `FP16` 和 `BF16` 相比，FP8 可将模型权重占用的显存减半，大幅缓解硬件资源压力。例如，原需16张卡运行的 `DeepSeek` 模型，使用FP8后仅需8张卡即可运行。

2.  **更高的推理精度**: 相较于目前流行的 `INT8` 量化方案，FP8 能够提供更高的推理精度，有效减少量化带来的精度损失。

3.  **效率与精度的完美平衡**: 综合以上两点，FP8 成为了大模型领域兼顾性能与效果的理想选择。

### 行业趋势与挑战

- **行业趋势**: FP8 已成为大模型训练与推理领域的“新宠”，越来越多的AI模型开始采用FP8格式。预计到2025年，FP8将成为大模型的主流低精度技术选型。
- **核心挑战**: FP8 的高效计算长期以来高度依赖特定硬件（尤其是国外芯片）的原生支持，这严重制约了相关技术的自主发展与广泛应用。

## 三、基于昇腾 Ascend C 的软 FP8 解决方案

为解决FP8对特定硬件的依赖问题，我们基于昇腾 `Ascend C` 算子开发框架，成功研发出**软FP8解决方案**。

### 方案概述

该方案的核心思想是在软件层面实现对FP8权重的支持，其工作流程如下：

1.  **输入**: 直接加载FP8格式的权重模型到昇腾硬件。
2.  **处理**: 通过自研的**反量化算子**，在计算时将FP8权重动态转换为 `BF16` 格式参与运算。
3.  **优势**: 
    - 保障了计算过程的准确性。
    - 避免了部署过程中权重格式的多次转换，提高了效率。
    - 为未来硬件原生支持FP8时快速适配预留了灵活空间。

### 核心优化策略

方案的性能提升源于算子级和框架级的深度优化。

#### 1. 算子级优化

- **专家动态反量化**: 在Kernel内创新性地提出动态反量化方案。
- **算子融合**: 将Vector与Cube两类算子进行融合，高效调度不同计算核心的任务。
- **流水线优化**: 通过探寻最优分块策略、数据预取与乱序执行等技术，彻底消除计算流水线气泡。

#### 2. 框架级优化

- **高效算子下发**: 通过PyTorch的 `meta` 函数实现算子入图，并基于**懒加载（Lazy Loading）**方式下发，避免了单个算子依次下发带来的性能开销。
- **动态路径调整**: 利用模型特征的智能感知能力，实现计算路径的动态调整，选择最优执行策略。

## 四、应用成果与价值

该软FP8解决方案在多个维度取得了显著成果。

- **性能与效率**: 
    - 实现 **32% 的端到端推理效率提升**。
    - 在模型精度几乎无损的前提下，单台设备即可流畅运行完整版 `DeepSeek V3.1`。
    - 在板卡机型上，可支持的模型参数规模翻倍，从而运行更大规模的模型。
    - 大幅提升并发处理能力，让不同硬件配置的用户都能享受到FP8的技术红利。

- **模型兼容性**: 
    - 全面兼容 `DeepSeek V3.1`、`DeepSeek-V3/R1`、`Qwen3` 等主流FP8量化模型。

- **精度表现**: 
    - 精度测试结果显示，该方案的推理精度与 `INT8+INT4` 混合精度方案相当，实现了近乎无损的性能优化。