# AI通信技术趋势演进和通信诉求

AI通信技术与AI负载强相关，并随着AI负载的演进而发展。当前，AI负载正呈现出新的趋势，对底层通信技术提出了更高的要求。

## 一、 AI负载演进带来的通信挑战

AI负载正从以文本为主的LLM（大语言模型）向多模态及Agent场景演进，同时模型本身也在持续迭代优化。这些变化导致流量负载特性发生改变，为底层通信带来一系列新挑战。

### 1. 超低时延推理需求
推理服务的响应时延是关键指标。随着模型优化和应用场景的演进，时延要求愈发严苛：
- **原有标准**: 典型Token-per-Token时延约为50ms。
- **当前水平**: 以“豆包”、“Kimi”为代表的国内主流模型，时延已下探至10ms级别。
- **前沿水平**: 部分国外先进模型的时延要求低于5ms。

### 2. 负载的高度动态化与多样化
随着AI Agent和多模态技术的发展，AI业务的流量负载呈现出高度多样化和动态化的特性。此外，超长序列（Sequence Length）处理已成为常态，序列长度可达兆字节（MB）级别，对通信的灵活性和效率提出了挑战。

## 二、 AI通信范式的演进路径

为应对上述负载变化，AI通信范式经历了以下演进过程：

1.  **单机通信**
    -   **场景**: 模型参数量较小，可在单台服务器内部完成计算。

2.  **多机集合通信 (Collective Communication)**
    -   **场景**: 模型规模增大，需采用张量并行（TP）、数据并行（DP）、专家并行（EP）等多种并行策略进行分布式训练和推理。
    -   **模式**: 以All-Reduce、All-Gather等集合操作为主。

3.  **点对点通信 (Point-to-Point, P2P)**
    -   **场景**: 在推理优化架构（如Prefill/Decode分离）中，Prefill节点与Decode节点之间需要高效传输KV Cache。

4.  **M2N 通信**
    -   **固定M2N**: 在更细粒度的AFD（Attention-FFN Decode）分离场景中，Decode阶段的Attention和FFN计算被进一步解耦，产生了多对多（M2N）的通信关系，但节点关系相对固定。
    -   **弹性异步M2N**: 未来，AI系统将向完全分离（Full Disaggregation）的架构演进。这种系统要求通信具备弹性和异步特性，以支持任意计算节点间的按需通信，形成更为灵活的M2N通信范式。

## 三、 基础设施演进与核心通信诉求

从基础设施（南向）来看，AI集群正从传统的服务器形态向基于ED-Mesh、2D-Mesh等拓扑的超异构形态演进。结合北向的AI负载变化，可以总结出对未来AI通信的几点核心诉求：

### 1. 极致性能
- **基于内存语义的优化**: 针对超低时延推理场景，需采用基于内存语义（如RDMA）的方式进行极致的性能优化。
- **计算与通信深度融合**: 实现计算与通信的协同调度和深度融合，隐藏通信开销。
- **拓扑亲和的通信**: 针对超异构、大规模互联场景，通信库必须具备拓扑感知能力，以充分利用底层硬件的大带宽优势。

### 2. 生态与易用性
- **开箱即用**: 提供无需复杂配置的“开箱即用”体验。
- **无缝集成**: 确保通信加速库能与vLLM、SGLang等主流推理框架无缝集成，方便社区和开发者快速应用。