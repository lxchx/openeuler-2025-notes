1
00:00:00,000 --> 00:00:06,830
下面有请华为技术有限公司AI通信专家韩远坤先生，

2
00:00:07,010 --> 00:00:12,490
为我们带来《昇腾亲和的通信加速库CAM关键技术和生态介绍》。

3
00:00:13,030 --> 00:00:13,685
大家欢迎。

4
00:00:23,540 --> 00:00:26,160
大家好，我是华为公司的韩远坤。

5
00:00:26,700 --> 00:00:29,440
现在负责AI通信这一块。

6
00:00:29,800 --> 00:00:36,895
下面我分享一下昇腾亲和的通信加速库CAM的关键技术和生态介绍。

7
00:00:39,770 --> 00:00:41,750
今天主要分三个部分。

8
00:00:41,810 --> 00:00:46,590
第一部分是AI通信技术的趋势演进和通信诉求。

9
00:00:47,310 --> 00:00:52,610
第二部分是灵雀通信软件UMDK的总体架构介绍。

10
00:00:52,730 --> 00:00:59,235
最后一部分是昇腾亲和的通信加速库CAM的关键技术介绍。

11
00:01:00,740 --> 00:01:07,380
我们可以看到，AI通信技术与AI负载强相关，

12
00:01:07,740 --> 00:01:10,840
并随着AI负载的变化而变化。

13
00:01:11,020 --> 00:01:19,020
从趋势中可以看到，以文本为主的LLM场景，正在向多模态以及Agent场景演进。

14
00:01:19,560 --> 00:01:24,320
同时，模型也在不断地优化和更新当中。

15
00:01:24,540 --> 00:01:31,955
这两者会导致流量负载的变化，对底层通信提出不同的挑战。

16
00:01:32,970 --> 00:01:35,970
例如，超低时延推理场景。

17
00:01:36,390 --> 00:01:39,870
原本的TPOT是50毫秒。

18
00:01:40,290 --> 00:01:47,630
现在以豆包、Kimi为主的国内模型，已经下探到10毫秒级别，

19
00:01:47,950 --> 00:01:52,250
甚至国外的模型会到5毫秒以下。

20
00:01:52,730 --> 00:01:57,450
同时，随着AI Agent以及多模态的发展，

21
00:01:57,690 --> 00:02:06,660
AI业务的负载呈现出高度多样化和动态化的特性。

22
00:02:07,120 --> 00:02:09,359
超长序列已成为常态。

23
00:02:09,919 --> 00:02:14,085
序列长度甚至达到兆级别。

24
00:02:14,870 --> 00:02:22,350
针对这些变化，我们看一下AI通信是如何演进的。

25
00:02:23,310 --> 00:02:26,830
参数量较小时，我们以单机通信为主。

26
00:02:26,990 --> 00:02:34,250
模型变大后，会产生不同的并行策略，如TP、DP、EP。

27
00:02:34,530 --> 00:02:37,550
这时以多机的集合通信为主。

28
00:02:38,010 --> 00:02:48,930
在PD分离的场景下，P和D节点之间要传输KV Cache，这时就产生了P2P通信。

29
00:02:49,430 --> 00:02:55,570
在AFD场景，Decode的Attention和FFN之间进一步分离，

30
00:02:55,950 --> 00:03:04,155
产生了M2N这种通信范式，这还是一种比较固定的关系。

31
00:03:05,160 --> 00:03:11,340
我们认为，未来AI系统会是一个全分离的系统，

32
00:03:11,700 --> 00:03:15,760
它必须具备弹性、异步这样一种特征。

33
00:03:15,940 --> 00:03:23,505
对于通信而言，也是这种异步、任意的点对点通信，即M2N的通信范式。

34
00:03:24,780 --> 00:03:31,980
南向的变化，从Server形态到超异构形态。

35
00:03:32,220 --> 00:03:42,620
超异构又有ED Mesh、2D Mesh等，这种变化对通信也有不同的诉求。

36
00:03:42,920 --> 00:03:53,665
我们结合来看，从南向的基础设施变化到北向的AI负载变化，总结出对通信的三种不同诉求。

37
00:03:54,520 --> 00:04:04,820
首先是高性能方面，要基于内存语义做极致的性能优化，应对超低时延的推理场景。

38
00:04:05,120 --> 00:04:09,200
同时要做计算与通信的深度融合。

39
00:04:09,680 --> 00:04:20,380
针对超异构大规模互联的场景，要做拓扑亲和的通信，将超异构的大带宽优势发挥出来。

40
00:04:20,580 --> 00:04:32,445
在生态与应用性上，我们强调开箱即用，让vLLM、SGLang等主流社区可以无缝使用我们的通信加速库。

41
00:04:35,310 --> 00:04:44,430
UMDK是灵雀的高性能通信基础底座，基于URMA统一内存语义来构建。

42
00:04:44,430 --> 00:04:51,790
南向能够屏蔽不同硬件的差异，构建智算、通算场景下的互联底座。

43
00:04:52,290 --> 00:04:58,319
面向智算场景，我们构建了超异构通信加速库CAM。

44
00:04:58,319 --> 00:05:10,040
它能基于内存语义通信，突破大EP通信的低时延，并做细粒度流水并行，实现计算和通信的掩盖。

45
00:05:10,460 --> 00:05:24,445
在生态上，UMDK支持DeepEP、Mooncake等主流社区接口，能让AI框架在不修改代码的情况下，无缝使用灵雀超异构通信加速的能力。

46
00:05:26,710 --> 00:05:36,290
通信加速库CAM的定位是做昇腾亲和的通信加速库，构建不同场景的通信加速能力。

47
00:05:36,490 --> 00:05:45,570
包括MoE场景下的大EP通信库、大通算融合加速库、M2N场景以及KV Cache传输的加速库。

48
00:05:45,990 --> 00:05:55,310
对于大EP通信库，我们提供了Prefill和Decode阶段的MoE通信算子，包括Dispatch和Combine。

49
00:05:56,050 --> 00:05:58,115
同时支持不同的代际。

50
00:05:59,060 --> 00:06:06,939
虽然我们针对MoE通信做了极致的性能优化，但通信和计算仍是串行的。

51
00:06:07,100 --> 00:06:20,760
所以我们做了MoE大融合加速库FusedDeepMoE，通过Token的动态分组和细粒度流水并行，实现计算和通信的掩盖。

52
00:06:21,919 --> 00:06:26,980
M2N场景下，通信库需具备弹性可靠的通信能力。

53
00:06:26,980 --> 00:06:32,340
同时我们也在探索异步的M2N通信范式。

54
00:06:32,700 --> 00:06:42,585
在KV Cache传输场景下，我们通过自动聚合技术，提升PD分离下KV Cache的传输加速。

55
00:06:44,980 --> 00:06:50,300
我们看一下在大EP通信库下做了哪些技术手段的优化。

56
00:06:50,320 --> 00:06:56,020
可以看到，MoE通信占比将近30%，包括Dispatch和Combine。

57
00:06:57,090 --> 00:07:05,770
通信时延主要包括传输时延和静态时延，即静态下发的控制面开销和算子启动开销。

58
00:07:06,070 --> 00:07:18,310
我们以Dispatch算子为例进行打点，分析各流程的时延，可以看到主要包含三部分时延。

59
00:07:18,310 --> 00:07:24,790
一是通信数据准备。因为Dispatch阶段要把Token分发到不同专家，

60
00:07:25,110 --> 00:07:37,400
而昇腾NPU是多核的，多核并行分发Token时，如果都发给同一专家，为避免冲突，需要计算地址偏移。

61
00:07:37,560 --> 00:07:41,960
计算时Scalar能力较弱，计算时间长。

62
00:07:42,020 --> 00:07:52,725
所以我们的优化措施是让它以Vector方式计算，从而加速数据准备过程。

63
00:07:53,820 --> 00:08:05,599
二是传输数据时间较长，主要是带宽利用率低，原因是网络冲突或端口负载不均。

64
00:08:05,599 --> 00:08:13,720
我们通过通信时序编排，让共享专家Token通信时，从2/6打1变成8打1，

65
00:08:14,020 --> 00:08:15,860
减少网络冲突。

66
00:08:16,300 --> 00:08:28,270
同时通过地址偏移计算，让哈希分布更均匀，散列在不同NPU端口上，使负载更均衡。

67
00:08:28,790 --> 00:08:35,630
最后是同步耗时较长。通信有同步等待过程，

68
00:08:35,970 --> 00:08:41,359
特别是在大EP通信下，各卡间需做一次全量同步。

69
00:08:41,359 --> 00:08:56,045
这里的优化技术包括EPLB负载均衡，以及构建大通算融合算子，进一步融合通信和计算，降低同步的影响。

70
00:08:58,100 --> 00:09:02,240
我们如何做大EP的通算融合加速库呢？

71
00:09:05,300 --> 00:09:13,679
可以看到，通信和计算是串行执行的，所以通信耗时没有被掩盖。

72
00:09:16,920 --> 00:09:24,320
另外，在做Cube矩阵运算时，Vector算子是串行执行的，算力有浪费。

73
00:09:24,320 --> 00:09:32,890
而且MoE层算子个数多，下发次数多，Kernel Launch耗时也较长。

74
00:09:33,450 --> 00:09:48,270
我们的思路是，将Decode中的整个MoE层融合成一个大算子，通过细粒度流水并行，实现通信与计算的掩盖。

75
00:09:48,690 --> 00:09:55,250
融合的范围包括红框内。

76
00:09:55,450 --> 00:10:11,350
从Dispatch开始，到两次GEMM计算（一次升维、一次降维），再到Combine，整个阶段融合成一个大算子。

77
00:10:11,650 --> 00:10:17,085
如何实现呢？包含两部分的深度融合。

78
00:10:18,080 --> 00:10:22,120
首先是Dispatch和第一个GEMM的融合。

79
00:10:22,380 --> 00:10:34,920
我们可以对Token进行动态分组，收到一组Token就做一组的计算，同时接收下一组Token，实现流水线作业。

80
00:10:35,440 --> 00:10:41,520
第二部分是第二个GEMM与Combine的深度融合。

81
00:10:41,520 --> 00:10:58,260
即计算出一部分Token，就可以对这部分Token进行通信和聚合（Combine），同时计算第二组Token，实现计算与通信的流水线掩盖。

82
00:10:58,760 --> 00:11:07,215
当前这个能力已接入SGLang社区，同时也在和vLLM社区合作。

83
00:11:09,830 --> 00:11:14,170
这个场景是AFD分离场景的M2N通信加速。

84
00:11:14,410 --> 00:11:19,570
前面讲过，大家都说Decode是访存bound的。

85
00:11:19,570 --> 00:11:30,850
那究竟是哪里bound呢？主要是在Attention阶段，它要读取Prefill阶段产生的KV Cache。

86
00:11:31,190 --> 00:11:42,319
实际上，由于FFN阶段的BS较小，其算力利用率较低。

87
00:11:42,319 --> 00:11:51,145
这时我们能否加大FFN阶段的BS？因为加大BS就能提升整个系统的吞吐。

88
00:11:51,760 --> 00:11:58,560
所以这就是AFD的核心思想：将Attention与FFN分离，

89
00:11:58,959 --> 00:12:13,700
让它们可以独立组batch。这样FFN就可以组一个较大的batch，从而提升整个系统的吞吐。

90
00:12:14,020 --> 00:12:26,250
但是，A和F分离后，产生了额外的M2N通信，即Attention和FFN之间。

91
00:12:28,030 --> 00:12:34,770
这时，AI框架上就需要做多batch流水，将通信掩盖掉。

92
00:12:35,050 --> 00:12:41,190
在该场景下，我们也构建了自己的M2N通信加速库。

93
00:12:41,190 --> 00:12:48,079
首先，通过NPU直驱通信，减少控制面的时间开销。

94
00:12:48,079 --> 00:13:00,670
NPU直驱是指在NPU侧发起AI集合通信，而非从CPU侧发起，这样可以减少控制面开销。

95
00:13:00,790 --> 00:13:11,240
同时，做通信融合与编排调度，将A2F及F2A通信与本地计算融合，消除冗余通信。同时，A和F分离要求整个系统是弹性的。

96
00:13:11,599 --> 00:13:23,580
Attention和FFN节点各有其弹性机制。这时通信库也需具备弹性能力。

97
00:13:23,760 --> 00:13:37,895
随意增减节点，通信域都能弹性地进行通信，而无需销毁整个通信域，后者是无法接受的。

98
00:13:39,430 --> 00:13:48,570
最后介绍一下PD分离场景。在PD分离下，KV Cache的传输是重要环节。

99
00:13:49,090 --> 00:13:57,350
通过我们构建的推理传输加速库MIXL，相比现有能力可提升30%。

100
00:13:57,350 --> 00:14:08,610
MIXL是一个中间件，南向屏蔽不同传输通道，北向对接不同AI服务平台及框架。

101
00:14:08,610 --> 00:14:13,740
我们通过两种技术实现。一是分片消息聚合。

102
00:14:14,200 --> 00:14:32,000
通过对多层KV Cache数据进行聚合（这里有不同聚合策略），再一起传输数据，提升带宽，从而提升系统吞吐。

103
00:14:32,220 --> 00:14:34,480
二是异构多径聚合。

104
00:14:34,620 --> 00:14:47,870
昇腾架构中，RoCE网卡嵌入NPU侧，NPU侧能看到多种传输通道，包括UB和RoCE。

105
00:14:48,290 --> 00:14:53,790
当前我们只使用单一类型的传输通道。

106
00:14:54,290 --> 00:15:03,330
通过多径聚合技术，我们可以使用多个传输通道，包括其他空闲卡的通道。

107
00:15:03,510 --> 00:15:10,625
基于这种多路径传输，提升整体传输效率。

108
00:15:11,970 --> 00:15:13,970
以上是我的分享，谢谢大家。

109
00:15:14,250 --> 00:15:16,930
更多信息可以访问openEuler社区。

110
00:15:17,170 --> 00:15:26,490
如果大家对UMDK的技术希望有更多了解，也可以加入我们的微信群。

111
00:15:26,670 --> 00:15:29,310
好，谢谢大家。

112
00:15:33,060 --> 00:15:34,940
好，感谢韩老师带来的分享。

113
00:15:34,940 --> 00:15:47,940
做过推理的人应该很清楚，随着大EP方案的出现，原来对计算的要求很高。

114
00:15:48,000 --> 00:15:53,000
后来通信的开销越来越大。所以刚才韩老师给我们带来了，

115
00:15:53,000 --> 00:16:14,570
包括异构多平面的传输，将点对点通信变为M2N的多对多通信，通过多通道叠加，同时在计算和通信时做掩盖来提升性能。

116
00:16:14,570 --> 00:16:31,850
这种基础通信库能帮助我们实现reduce、all-gather、all-to-all等通信语义，加速这部分的能力，这是一个非常好的情况。

117
00:16:31,910 --> 00:16:42,220
后面也希望跟这个SIG组，结合之前刘宇振老师交流的能力，可能会变成一个更有性价比的推理方案。好，谢谢。
