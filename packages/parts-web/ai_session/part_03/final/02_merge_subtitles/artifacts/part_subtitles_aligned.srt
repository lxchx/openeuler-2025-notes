1
00:00:00,000 --> 00:00:04,340
下面有请Linaro的工程师刘新良先生，

2
00:00:04,340 --> 00:00:11,010
为我们带来JuiceFS MLPerf Storage v2.0性能评测与调优的分享。

3
00:00:11,210 --> 00:00:12,500
大家欢迎。

4
00:00:18,340 --> 00:00:20,220
大家下午好，我是来自Linaro的刘新良。

5
00:00:20,480 --> 00:00:30,655
今天给大家分享的主题是JuiceFS MLPerf Storage v2.0的性能评测与调优。

6
00:00:34,570 --> 00:00:37,590
今天分享的内容主要分为以下几部分。

7
00:00:37,850 --> 00:00:39,310
首先是介绍。

8
00:00:39,550 --> 00:00:42,945
第二部分是测试与分析。

9
00:00:43,680 --> 00:00:45,185
然后最后进行总结。

10
00:00:48,100 --> 00:00:52,360
我们首先介绍一下JuiceFS。它是什么东西呢？

11
00:00:52,920 --> 00:01:04,655
它是由JuiceData公司开源的一款高性能、基于对象存储、低成本的分布式文件系统，是现在AI领域可能用得比较多的一个文件系统。

12
00:01:05,810 --> 00:01:16,450
它的特性主要有支持POSIX、HDFS SDK、Python SDK，以及S3接口的兼容性。

13
00:01:18,310 --> 00:01:26,655
它的企业版还有更多的特性，比如分布式的Distributed Data Cache。

14
00:01:28,460 --> 00:01:32,225
它更多用于云原生的环境上。

15
00:01:33,120 --> 00:01:37,900
应用场景主要是AI训练、推理、大数据等。

16
00:01:37,900 --> 00:01:44,425
跟它有类似的些文件系统有Alluxio，还有S3FS。

17
00:01:46,670 --> 00:01:55,985
可以看一下右边这个图，它是一个典型的元数据与文件数据分离的架构。

18
00:01:56,660 --> 00:02:00,160
上面的大方框是在计算节点上。

19
00:02:00,420 --> 00:02:03,440
所以你看到它其实它的架构还是非常简单的。

20
00:02:03,640 --> 00:02:15,920
因为这个架构，数据库可以适配很多种，文件存储也可以适配很多种对象存储。

21
00:02:16,080 --> 00:02:27,895
这就说明我可以复用已有的一些数据库和对象存储，尤其你在云的环境上使用起来，你会发现它是非常好用、简单的架构。

22
00:02:30,450 --> 00:02:34,210
那么，什么是MLPerf Storage呢？

23
00:02:37,480 --> 00:02:47,215
MLPerf Storage是全球权威的AI工程联盟MLCommons开发的ML存储benchmark。

24
00:02:48,290 --> 00:02:55,550
他们开发的其他benchmark还包括自动驾驶(Automotive)、训练(Training)、推理(Inference)和安全(AILuminate)等。

25
00:02:58,010 --> 00:03:27,875
MLPerf Storage的目的是测量机器学习工作负载在存储方面的性能，包括数据摄取(中期目标)、训练(短期目标)和推理(长期目标)。目前，短期目标“训练”已经实现。

26
00:03:29,090 --> 00:03:32,590
那么为什么需要测量AI环境中的存储呢？

27
00:03:32,830 --> 00:03:39,400
因为我们要了解机器学习工作负载中的存储瓶颈是什么。

28
00:03:40,780 --> 00:03:58,985
还有就是帮助AI/ML从业者做出明智的存储决策。例如，选择何种存储系统能保证GPU利用率达到90%以上，达到资源的有效利用。

29
00:03:59,900 --> 00:04:13,485
还有帮助存储供应商、AI/ML框架优化者等针对机器学习工作负载进行优化。

30
00:04:14,230 --> 00:04:31,590
帮助AI/ML框架和系统获得更好的存储性能。

31
00:04:31,930 --> 00:04:39,710
右边的图是MLPerf Storage benchmark的一个简单框架。

32
00:04:42,160 --> 00:05:10,365
AI训练主要是从存储(Dataset)读取数据集，加载(Load)到内存(System Memory)中。在内存中可能会做一些预处理，比如转换为张量(tensors)。然后以批次(batch)的形式将数据发送到GPU/ASIC进行模型训练。

33
00:05:11,270 --> 00:05:33,370
对于存储的benchmark而言，我们主要关心左边的部分：数据如何加载、转换，并发送到GPU。GPU内部如何训练，我们不太关心。

34
00:05:33,790 --> 00:05:59,895
因此，在做benchmark时，可以简化GPU的训练过程。我们测量出实际训练所需的时间，然后用一个等时的sleep操作来模拟它。这样就大大简化了benchmark的实现工作。

35
00:06:03,380 --> 00:06:07,800
那么，这个v2.0是目前最新的一个版本。

36
00:06:07,800 --> 00:06:25,080
它支持的工作负载(Task)主要有：图像分割(Image segmentation)、图像分类(Image classification)、科学参数预测(Scientific parameter prediction)以及最新的Llama3 Checkpointing。

37
00:06:25,560 --> 00:06:32,860
这些任务模拟了AI训练中对存储系统的各种IO pattern。

38
00:06:32,860 --> 00:06:56,955
可以看到，不同的模型，其样本大小(Sample Size)和批次大小(Batch Size)各不相同。这些模拟数据的设置，都是基于对实际系统的测量得出的，因此非常贴合实际应用场景。

39
00:06:57,990 --> 00:07:20,135
它会评估存储系统的多方面能力(Evaluate Storage Capability)，比如：带宽、IOPS、时延、并发能力。训练模型主要考察读性能，而Checkpointing则考察对超大文件的并发写性能。

40
00:07:29,270 --> 00:07:38,530
接下来，我们从存储的角度看一下分布式训练的概览。

41
00:07:38,530 --> 00:08:16,235
首先，对于我们关心的存储，在分布式训练中，数据集(Dataset)会被分割成很多个分区(Partition)。每个进程(Process)，也就是每个GPU，会读取其中一个分区的数据进行训练。训练完成后，无论是同一台机器内的多块GPU，还是不同机器上的GPU，它们之间需要进行同步(Sync)。这大概就是其基本流程。

42
00:08:19,710 --> 00:08:27,790
这是MLPerf Storage的内部流程图，也是刚才流程的一个具体实现。

43
00:08:28,450 --> 00:09:18,005
右边的JuiceFS存储着被分区的Dataset。每个GPU/Accelerator worker的主线程会运行一个benchmark runner。它内部的Data Loader会从JuiceFS读取一个数据分区(Partition x)。Data Loader以多线程方式，在每个step/cycle中读取一个批次(batch)的样本数据，并转换为张量。之后，用一个sleep操作来模拟实际的计算(Accelerator compute)过程。因此，对于存储测试而言，我们主要关心Data Loader的行为，因为它决定了系统的IO pattern。

44
00:09:23,420 --> 00:09:33,330
为了分析问题，我们需要对整个系统有一个心智模型。

45
00:09:33,430 --> 00:10:23,985
这张图展示了一个简化的例子。可以看到，应用层有多个IO线程(App Threads)，比如mlp-storage dataloader threads。它们发起读写请求。JuiceFS是一个FUSE文件系统，它有处理IO请求的主goroutines。它会向后端的Meta client和ObjectStore client发送请求，这些client内部也有异步的goroutines。这个过程中涉及多个步骤，有些是同步的，有些是异步的，还有一些关键的数据拷贝(copy)操作。这些都是我们需要了解的。

46
00:10:26,320 --> 00:10:32,075
接下来我们看一下性能测试与分析。这是我们的测试环境。

47
00:10:34,290 --> 00:10:42,545
包括硬件和软件。测试是在openEuler 24.03 LTS SP2版本上进行的。

48
00:10:46,180 --> 00:10:48,840
环境部署过程就不再赘述。

49
00:10:54,270 --> 00:10:59,550
首先，跑测试需要生成数据。

50
00:10:59,690 --> 00:11:27,715
数据的生成也是根据各种实际参数进行的。这里要注意，为了保证测试的公平性，数据量的大小要大于系统可用内存的5倍，以排除Page Cache的影响。

51
00:11:28,530 --> 00:11:37,555
另外，JuiceFS支持将数据预热到本地，这可以加快数据访问速度。

52
00:11:42,910 --> 00:11:44,850
首先看unet3d模型。

53
00:11:44,850 --> 00:11:50,715
我们测出的最好结果是单机最高支持5块GPU，满足GPU利用率大于90%的要求。

54
00:11:52,020 --> 00:12:07,705
其中GPU利用率为98%，带宽达到14.8 GB/s。关键的参数调整包括：将数据读取线程(reader.read_threads)从4加到16，以加大并发；以及启用直接IO(reader.odirect=True)，以减少buffer IO和内存拷贝的开销。

55
00:12:10,240 --> 00:12:12,040
为什么支持不到6块GPU呢？

56
00:12:12,040 --> 00:12:29,795
我们分析发现，当使用6块GPU时，其利用率降至83%，带宽为15.1 GB/s。我们用FIO测试JuiceFS的带宽，发现其极限也约为15.1 GB/s。因此，瓶颈在于JuiceFS的带宽。

57
00:12:30,670 --> 00:12:37,270
那为什么JuiceFS的带宽上不去呢？瓶颈在于CPU。

58
00:12:37,470 --> 00:13:38,775
通过绑核(CPU)和使用top、devkit tuner numafast等工具查看，发现48核CPU已被用满。通过devkit tuner的memory/miss工具查看，发现瓶颈是Memory Bound，主要耗时在内存copy。具体来说，是由于互联和内存的时延导致CPU级联。LLC Miss Rate很高，耗时主要在runtime.memove和arch_copy_to_user等内存拷贝函数上。

59
00:13:41,070 --> 00:13:46,430
不绑定CPU时，为什么JuiceFS带宽更上不去呢？

60
00:13:46,470 --> 00:13:50,540
瓶颈为跨NUMA内存访问带来的时延。用top工具查看，CPU未使用满。

61
00:13:50,540 --> 00:14:13,325
用devkit tuner numafast工具查看，发现remote内存访问比例较高，超过80%。而跨片物理带宽仅有60GB/s。因此，瓶颈在于跨NUMA内存访问的时延。

62
00:14:15,460 --> 00:14:17,220
接下来看resnet50模型。

63
00:14:26,230 --> 00:14:34,200
单机最高可支持50块GPU，满足GPU利用率大于90%的要求。此时GPU利用率为95%，带宽为9.2 GB/s。

64
00:14:34,480 --> 00:14:44,905
关键参数调整：由于每次读取的batch数据量为58.5MiB，相对较小，我们将读取线程(reader.read_threads)从8降为1，单线程足以应付。

65
00:14:46,630 --> 00:14:48,330
为什么支持不了更多的GPU呢？

66
00:14:48,330 --> 00:15:01,655
瓶颈为JuiceFS带宽。当增加到55块GPU时，利用率降至86%，带宽同样为9.2 GB/s。瓶颈依然是JuiceFS的后端带宽。

67
00:15:04,100 --> 00:15:16,000
为什么JuiceFS带宽从15.1GB/s掉到9.2GB/s了呢？

68
00:15:16,180 --> 00:15:18,980
瓶颈为内存copy总带宽。

69
00:15:19,000 --> 00:15:28,895
resnet50为Buffer IO，读取和处理dataset时会消耗一部分内存带宽。通过stream测算内存带宽发现，系统内存copy带宽越大，JuiceFS的吞吐带宽越大。

70
00:15:32,750 --> 00:15:56,730
我们测试了不同机器的stream单CPU内存copy带宽和JuiceFS单机部署的read带宽，发现JuiceFS吞吐带宽本身很取决于内存带宽大小。

71
00:15:56,730 --> 00:15:57,990
最后总结一下。

72
00:15:58,030 --> 00:16:23,335
MLPerf Storage unet3d和resnet50模型支持的GPU数量取决于JuiceFS吞吐带宽。这主要考察了文件并发、大/中块顺序读能力，以及提供的总读带宽。

73
00:16:25,180 --> 00:16:33,600
Benchmark是个系统工程，需要系统各个部件配合好，以达到最优性能。

74
00:16:34,040 --> 00:16:43,235
系统内存和互联带宽和时延对系统的吞吐性能至关重要。

75
00:16:43,920 --> 00:16:48,500
内存copy比较消耗内存带宽。

76
00:16:48,580 --> 00:16:57,835
内存copy比较消耗CPU，由于互联和内存的时延导致CPU级联。

77
00:16:57,900 --> 00:16:59,900
JuiceFS吞吐带宽取决于内存带宽大小。

78
00:16:59,920 --> 00:17:11,980
Go没有numa awareness感知，对于跑大规模CPU核数，性能可能没有小规模核数好。

79
00:17:11,980 --> 00:17:18,515
对于多numa系统来说，尽量避免跨numa尤其是跨CPU socket。

80
00:17:19,340 --> 00:17:27,375
谢谢大家，我的分享完了。

81
00:17:31,260 --> 00:17:34,540
非常感谢刘老师的精彩分享。

82
00:17:34,540 --> 00:17:47,800
这个分享很有感触，除了性能评测，还给出了针对性的、专业的分析，以及对workload的建模分析，非常好。

83
00:17:47,820 --> 00:17:59,040
这套工程化的思想和思路，除了JuiceFS，应该也能用于其他场景。感谢。

84
00:18:00,850 --> 00:18:09,190
这也填补了我们社区在相关工具链上的一些空白，后续我们会联系您，获得更多相关的处理方法。好，下面...
