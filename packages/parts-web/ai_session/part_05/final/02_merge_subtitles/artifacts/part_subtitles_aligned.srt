1
00:00:00,000 --> 00:00:12,604
下面，有请华为技术有限公司AI工程师李宇振先生，为我们带来《通算/智算大模型推理加速协同探索与实践》。大家欢迎。

2
00:00:19,580 --> 00:00:20,440
大家好。

3
00:00:20,440 --> 00:00:28,075
我是来自华为的李宇振。今天给大家带来的是《通算/智算大模型推理加速协同探索与实践》。

4
00:00:30,590 --> 00:00:33,210
首先，我们来看一下业务场景。

5
00:00:33,610 --> 00:00:41,080
在传统的推理场景中，大部分模型的计算都会落在GPU上。

6
00:00:41,740 --> 00:00:57,100
因为公众普遍认为CPU的算力较小，所以以往的推理一般都在GPU上进行。这时，CPU的算力和内存就会被闲置，也就是被浪费了。

7
00:00:57,100 --> 00:01:02,460
这个场景中有两个痛点。

8
00:01:02,500 --> 00:01:07,260
第一个痛点是XPU内存容量瓶颈。

9
00:01:07,260 --> 00:01:13,240
这里的XPU是指GPU和NPU。

10
00:01:13,560 --> 00:01:27,260
XPU的内存容量瓶颈如何理解？市面上常见的XPU，其内存容量通常是16GB到80GB。

11
00:01:27,260 --> 00:01:34,870
这个容量会限制模型推理的上下文长度和并发量。

12
00:01:35,630 --> 00:01:48,670
以Qwen-32B模型和10 batch的并发请求为例，这些XPU最多能存储3.2k到16k token的KV Cache。

13
00:01:48,690 --> 00:01:55,270
这意味着在一个请求中，它只能处理这么多token。

14
00:01:55,830 --> 00:02:14,720
而鲲鹏920 CPU的内存能达到512GB，最多能存储1024k token的KV Cache。由此可见，CPU的大内存可以处理更长的请求。

15
00:02:15,040 --> 00:02:19,520
第二个痛点是整机利用率不高。

16
00:02:19,640 --> 00:02:28,420
这里的整机是指CPU加XPU。利用率不高的主要原因是CPU在此过程中被闲置。

17
00:02:29,360 --> 00:02:42,410
例如，在前向计算过程中，XPU承担了95%以上的计算量，而CPU的算力和内存利用率都非常低。

18
00:02:44,240 --> 00:02:52,640
为了利用CPU的算力，我们提出了CPU+XPU算力协同方案。

19
00:02:53,400 --> 00:03:04,050
在此之前，我先介绍一个概念：PD分离。

20
00:03:05,430 --> 00:03:13,550
在大模型推理过程中，会涉及两个步骤：第一个是Prefill，第二个是Decode。

21
00:03:13,570 --> 00:03:22,690
文本生成的第一个token的处理过程就是Prefill，后续生成的每一个token的处理过程则对应Decode。

22
00:03:23,330 --> 00:03:47,560
在这个过程中，Prefill是计算密集型的，而Decode是内存密集型，其计算量非常小。根据这个特性，我们考虑将Prefill放在XPU上，而Decode可以放在CPU上，当然也可以放在XPU上。

23
00:03:47,920 --> 00:04:20,350
可能会有同学疑惑，CPU的算力明显小于XPU，用CPU辅助推理是否会导致整体吞吐量降低？关于这一点，举个例子，在峡谷中即使有十万大军，横向能展开的士兵也有限。这里同理。

24
00:04:20,510 --> 00:04:44,300
大模型推理的Decode过程所需计算量较小，且是串行执行的，即必须推理完一个token才能推理下一个。这使得算力强大的XPU在Decode过程中有点“有力无处使”，因此，在这个场景下使用CPU也是可行的。

25
00:04:45,000 --> 00:04:53,530
我们的CPU+XPU算力协同方案主要包含两部分。第一部分是异构算力调度。

26
00:04:55,150 --> 00:05:22,260
它会代理用户请求，当请求到达调度端后，我们会对其进行PD分离，即Prefill和Decode的分离。我们选择将Prefill放在XPU上，而将一部分Decode放在CPU上，另一部分放在XPU上。

27
00:05:22,560 --> 00:05:31,060
如何衡量哪些Decode应该放在CPU，哪些放在XPU，就涉及到动态调度。

28
00:05:31,360 --> 00:05:56,805
协同方案的第二部分是通算推理加速，主要使用了三项技术：NUMA亲和、多线程并行和算子优化。这项技术目前主要应用于两大类模型：稠密模型的推理加速和MoE模型的推理加速。

29
00:06:00,320 --> 00:06:03,500
我们先来看异构算力调度。

30
00:06:03,500 --> 00:06:12,800
为什么要进行异构算力调度？它能解决什么问题？

31
00:06:12,820 --> 00:06:38,430
大模型推理服务的提供商希望提高并发量，而用户则希望响应速度快，不能因为并发量提高而影响单个请求的时延。

32
00:06:38,650 --> 00:06:43,930
基于这个背景，我们设计了异构算力调度。

33
00:06:44,030 --> 00:07:03,660
它面临两大挑战：第一，Prefill-Decode分离会引入KV Cache的传输开销。因为在XPU上完成Prefill后，需要将KV Cache传输到CPU，CPU才能继续进行Decode推理。

34
00:07:04,380 --> 00:07:23,320
第二，如何动态决定将Decode任务分配到CPU还是XPU？这个问题比较复杂，因为它会因设备、应用场景的不同而变化。

35
00:07:25,320 --> 00:07:38,210
为解决这些挑战，我们做了两方面工作：首先，优化vLLM调度模块，使其支持在异构算力上进行PD分离。

36
00:07:38,750 --> 00:07:43,750
其次，构建基于实时负载感知的动态调度引擎。

37
00:07:44,110 --> 00:07:54,810
负载感知可以指计算负载、显存占用等。

38
00:07:59,170 --> 00:08:38,060
一个简单的做法是，实时收集CPU和XPU上每个batch的吞吐量，然后判断哪个设备的吞吐量更高，就将Decode请求分配到该设备，以期提高整体吞吐量。但这个方案存在一个问题：如果两个Decode请求到达的时间间隔过短，动态调度可能会因来不及反应而失效。

39
00:08:38,280 --> 00:09:12,115
为解决这个问题，我们可以收集历史调度信息，包括每次调度时的负载情况和吞吐量等。通过学习这些历史数据，我们可以预测在当前状态下，将Decode任务发送到某个设备后其吞吐量的变化，从而进行更精准的动态调度。

40
00:09:17,320 --> 00:09:27,560
关于通算推理加速，主要涉及三点：NUMA亲和、多线程并行和算子优化。

41
00:09:28,040 --> 00:10:21,780
首先是NUMA亲和。现在的CPU大多采用多NUMA内存架构，这种设计的一个特点是跨NUMA访存的开销可能是本NUMA内访存的2到4倍，非常昂贵。因此，当我们将Decode请求放在CPU端时，需要尽量降低跨NUMA访存的开销。我们的做法是将完整的模型权重（例如，纵向切割成四份，对应四个NUMA），以及计算所需的hidden state，分别拷贝到各个NUMA上进行本地化计算，最后再合并结果。

42
00:10:22,920 --> 00:10:37,040
其次是多线程并行。现代CPU多采用多核设计，提供了并行算力。

43
00:10:37,280 --> 00:10:49,530
为充分利用多核算力，我们采用多线程并行来处理大模型推理中的计算，例如对矩阵计算进行并行化处理。

44
00:10:55,590 --> 00:11:44,500
第三是算子优化。传统的CPU指令，一条乘法指令可能只能处理一次乘法。但在ARMv8架构之后，出现了许多可以加速计算的指令集。例如，NEON指令集中的一条指令可以一次完成四次浮点数乘法。而I8MM指令集扩展，其一条指令可以计算32个int8的乘法。这些都极大地加速了CPU的推理速度。

45
00:11:47,450 --> 00:11:58,030
通算推理加速目前主要应用于两个场景：稠密模型推理加速和MoE模型推理加速。

46
00:11:58,130 --> 00:12:23,620
对于稠密模型，如果我们将Decode放在CPU上执行，倾向于将注意力计算和FFN都放在CPU上完成，以避免与XPU之间频繁的数据交换产生开销。

47
00:12:23,720 --> 00:12:37,085
对于MoE模型，我们目前的策略是在XPU上完成注意力计算，而在CPU上进行大量的专家计算。

48
00:12:42,050 --> 00:12:46,190
关于业务效果，我们分两个场景来看。

49
00:12:46,190 --> 00:13:13,060
第一个是实时在线推理场景。在此场景下，与纯XPU的原始方案相比，我们的推理协同方案在Qwen-32B模型上可将并发量提升12.5%，在Qwen-7B模型上可提升8.8%。

50
00:13:13,060 --> 00:13:30,565
在非实时离线批量推理场景下，与原始方案相比，协同方案在Qwen-32B模型上可将总吞吐量提升20.1%，在Qwen-7B模型上可提升7.1%。

51
00:13:32,940 --> 00:13:57,380
我们下一步的工作将重点聚焦于MoE模型。我们会继续探索其在多机多卡场景下的异构调度，以及AF分离（Attention和FFN/Expert分离）的优化，还将探索Expert的分布式调度方案。

52
00:13:57,380 --> 00:13:59,415
好，谢谢大家，我的演讲完毕。

53
00:14:04,190 --> 00:14:29,000
感谢李老师为我们带来的通算与智算协同推理技术，非常精彩。刚才看到的性能提升效果令人印象深刻，我们特别期待后续在AF分离以及Expert分布式调度等面向集群能力方面的进展。好，谢谢。
