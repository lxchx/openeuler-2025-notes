{
  "graph_data": {
    "nodes": [
      {
        "id": "p1",
        "label": "传统推理痛点",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_003"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_003",
            "primary_time_ts": "00:00:31,000",
            "primary_time_ms": 31000
          }
        },
        "parent": null,
        "summary": "传统推理模式将计算集中在GPU，导致CPU资源闲置，并引发两大核心痛点：XPU内存容量瓶颈限制了模型上下文与并发，以及整机资源利用率低下。"
      },
      {
        "id": "d1_1",
        "label": "痛点一：XPU内存瓶颈",
        "category": "Problem",
        "references": {
          "section_ids": [
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_003"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_003",
            "primary_time_ts": "00:01:09,590",
            "primary_time_ms": 69590
          }
        },
        "parent": "p1",
        "summary": "XPU（GPU/NPU）的显存容量（通常16-80GB）有限，直接限制了大模型能处理的上下文长度和并发量，成为性能的关键瓶颈。"
      },
      {
        "id": "d1_2",
        "label": "痛点二：整机利用率低",
        "category": "Problem",
        "references": {
          "section_ids": [
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_003"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_003",
            "primary_time_ts": "00:02:08,410",
            "primary_time_ms": 128410
          }
        },
        "parent": "p1",
        "summary": "在传统的“CPU+XPU”异构服务器中，由于前向计算95%以上的负载都在XPU上，导致CPU的算力和内存被大量闲置和浪费。"
      },
      {
        "id": "p2",
        "label": "CPU+XPU协同方案",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_3",
            "sec_4",
            "sec_5"
          ],
          "material_ids": [
            "chunk_00/slide_004",
            "chunk_00/slide_005",
            "chunk_00/slide_006",
            "chunk_00/slide_007"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_004",
            "primary_time_ts": "00:02:46,000",
            "primary_time_ms": 166000
          }
        },
        "parent": null,
        "summary": "为解决传统推理痛点，提出CPU+XPU算力协同方案。该方案以PD分离为核心，通过异构算力调度和通算推理加速技术，实现对稠密和MoE模型的推理加速。"
      },
      {
        "id": "d2_1",
        "label": "核心概念：PD分离",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_004"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_004",
            "primary_time_ts": "00:02:50,240",
            "primary_time_ms": 170240
          }
        },
        "parent": "p2",
        "summary": "PD分离（Prefill-Decode Separation）利用Prefill阶段计算密集、Decode阶段内存密集的特性，将Prefill任务固定在XPU，部分Decode任务卸载至CPU，实现异构算力协同。"
      },
      {
        "id": "d2_2",
        "label": "方案组件：异构算力调度",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_3",
            "sec_4"
          ],
          "material_ids": [
            "chunk_00/slide_004",
            "chunk_00/slide_005"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_005",
            "primary_time_ts": "00:06:01,000",
            "primary_time_ms": 361000
          }
        },
        "parent": "p2",
        "summary": "为平衡高并发与低时延，设计了异构算力调度模块。它代理用户请求，进行PD分离，并通过动态调度引擎将任务灵活分配给CPU或XPU。"
      },
      {
        "id": "d2_3",
        "label": "方案组件：通算推理加速",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_3",
            "sec_5"
          ],
          "material_ids": [
            "chunk_00/slide_004",
            "chunk_00/slide_006",
            "chunk_00/slide_007"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_006",
            "primary_time_ts": "00:09:18,000",
            "primary_time_ms": 558000
          }
        },
        "parent": "p2",
        "summary": "为提升CPU执行推理任务的性能，集成了一套通算推理加速技术，主要包括NUMA亲和、多线程并行和算子优化，以充分利用现代CPU的架构特性。"
      },
      {
        "id": "d2_3_1",
        "label": "技术一：NUMA亲和",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_5"
          ],
          "material_ids": [
            "chunk_00/slide_006",
            "chunk_00/slide_007"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_006",
            "primary_time_ts": "00:09:23,320",
            "primary_time_ms": 563320
          }
        },
        "parent": "d2_3",
        "summary": "通过将模型权重和计算数据拷贝到各NUMA节点进行本地化计算，避免昂贵的跨NUMA节点内存访问开销，从而降低访存延迟。"
      },
      {
        "id": "d2_3_2",
        "label": "技术二：多线程并行",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_5"
          ],
          "material_ids": [
            "chunk_00/slide_006"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_006",
            "primary_time_ts": "00:10:20,320",
            "primary_time_ms": 620320
          }
        },
        "parent": "d2_3",
        "summary": "利用现代CPU的多核设计，采用多线程技术对大模型推理中的计算密集型任务（如矩阵计算）进行并行化处理，以充分利用CPU的并行算力。"
      },
      {
        "id": "d2_3_3",
        "label": "技术三：算子优化",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_5"
          ],
          "material_ids": [
            "chunk_00/slide_006",
            "chunk_00/slide_007"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_007",
            "primary_time_ts": "00:11:48,000",
            "primary_time_ms": 708000
          }
        },
        "parent": "d2_3",
        "summary": "利用现代CPU架构提供的SIMD指令集（如NEON, I8MM），用单条指令处理多个数据，大幅加速矩阵乘法等关键算子的计算速度。"
      },
      {
        "id": "p3",
        "label": "业务效果与展望",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_6",
            "sec_7"
          ],
          "material_ids": [
            "chunk_00/slide_008",
            "chunk_00/slide_009",
            "chunk_00/slide_010"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_008",
            "primary_time_ts": "00:12:43,000",
            "primary_time_ms": 763000
          }
        },
        "parent": null,
        "summary": "协同方案在业务上取得了显著效果，包括提升实时在线推理的并发量和离线批量推理的总吞吐量。未来工作将聚焦于MoE模型的多机多卡异构调度和分布式优化。"
      },
      {
        "id": "d3_1",
        "label": "业务效果：性能提升",
        "category": "Result",
        "references": {
          "section_ids": [
            "sec_6"
          ],
          "material_ids": [
            "chunk_00/slide_008"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_008",
            "primary_time_ts": "00:12:43,000",
            "primary_time_ms": 763000
          }
        },
        "parent": "p3",
        "summary": "与纯XPU方案相比，协同方案在实时场景下提升了8.8%-12.5%的并发量，在离线场景下提升了7.1%-20.1%的总吞吐量。"
      },
      {
        "id": "d3_2",
        "label": "未来工作：MoE模型优化",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_7"
          ],
          "material_ids": [
            "chunk_00/slide_009"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_009",
            "primary_time_ts": "00:13:34,000",
            "primary_time_ms": 814000
          }
        },
        "parent": "p3",
        "summary": "下一步工作将重点聚焦于MoE模型，探索其在多机多卡场景下的异构调度、AF分离（Attention和FFN/Expert分离）优化，以及Expert的分布式调度方案。"
      }
    ],
    "edges": [
      {
        "source": "p1",
        "target": "p2",
        "relation_type": "causal",
        "label": "催生解决方案"
      },
      {
        "source": "p2",
        "target": "p3",
        "relation_type": "sequential",
        "label": "展示成果"
      },
      {
        "source": "d1_1",
        "target": "d1_2",
        "relation_type": "parallel",
        "label": "并列痛点"
      },
      {
        "source": "d2_1",
        "target": "d2_2",
        "relation_type": "causal",
        "label": "是...的基础"
      },
      {
        "source": "d2_2",
        "target": "d2_3",
        "relation_type": "parallel",
        "label": "与...协同"
      },
      {
        "source": "d2_3_1",
        "target": "d2_3_2",
        "relation_type": "parallel",
        "label": "并列技术"
      },
      {
        "source": "d2_3_2",
        "target": "d2_3_3",
        "relation_type": "parallel",
        "label": "并列技术"
      },
      {
        "source": "d3_1",
        "target": "d3_2",
        "relation_type": "sequential",
        "label": "展望未来"
      }
    ]
  },
  "sections": [
    {
      "id": "sec_1",
      "index": 0,
      "title": "开场与主题介绍",
      "time_range": "00:00:00,000 - 00:00:28,075",
      "summary": "主持人邀请华为技术有限公司的AI工程师李宇振先生上台。讲者李宇振向观众问好，并介绍了本次分享的主题：《通算/智算大模型推理加速协同探索与实践》。",
      "refined_script": "# 通算/智算大模型推理加速协同探索与实践\n\n## 1. 主题介绍\n\n本次分享将探讨在通用计算（通算）与智能计算（智算）平台上，如何协同进行大模型推理的加速优化，并分享相关的探索与实践经验。",
      "original_script": "下面，有请华为技术有限公司AI工程师李宇振先生，为我们带来《通算/智算大模型推理加速协同探索与实践》。大家欢迎。\n大家好。\n我是来自华为的李宇振。今天给大家带来的是《通算/智算大模型推理加速协同探索与实践》。",
      "start_ms": 0,
      "end_ms": 28075,
      "material_ids": [
        "chunk_00/slide_001",
        "chunk_00/slide_002"
      ],
      "summary_hint": "主持人介绍讲者李宇振，随后讲者介绍本次分享的主题：《通算/智算大模型推理加速协同探索与实践》。",
      "lead_text": "下面，有请华为技术有限公司AI工程师李宇振先生，为我们带来《通算/智算大模型推理加速协同探索与实践》。大家欢迎。",
      "tail_text": "我是来自华为的李宇振。今天给大家带来的是《通算/智算大模型推理加速协同探索与实践》。",
      "text_span": {
        "start": 0,
        "end": 2
      }
    },
    {
      "id": "sec_2",
      "index": 1,
      "title": "业务背景与传统推理痛点",
      "time_range": "00:00:30,590 - 00:02:42,410",
      "summary": "在传统的推理场景中，计算主要集中在GPU上，导致CPU资源闲置，整机利用率低下。该模式存在两大痛点：一是XPU（GPU/NPU）的内存容量瓶颈限制了模型的上下文长度和并发量；二是CPU算力和内存被大量浪费。",
      "refined_script": "## 业务背景与传统推理痛点\n\n### 1. 传统推理模式概述\n\n在传统的推理场景中，由于GPU具备强大的并行计算能力，大部分模型计算任务（尤其是前向计算）都会被调度至GPU上执行。这种模式导致了以下问题：\n\n- **CPU资源闲置**：在GPU执行计算时，CPU的算力与内存资源处于低利用率状态，造成了显著的资源浪费。\n\n### 2. 主要痛点分析\n\n传统的GPU中心化推理模式主要存在两大痛点：\n\n#### 2.1 痛点一：XPU内存容量瓶颈\n\nXPU（指GPU/NPU等加速器）的显存容量是限制大模型推理性能的关键因素。\n\n- **显存限制**：市面上主流XPU的显存容量通常在16GB到80GB之间。\n- **性能影响**：有限的显存直接限制了模型能够处理的上下文长度（Context Length）和并发请求数量（Batch Size）。\n\n以Qwen-32B模型在10个并发请求（Batch=10）的场景为例，不同硬件平台可承载的KV Cache容量对比如下：\n\n| 硬件平台      | 内存/显存容量 | 可存储的KV Cache (Token) |\n| :------------ | :------------ | :----------------------- |\n| XPU (GPU/NPU) | 16GB - 80GB   | 3.2k - 16k               |\n| 鲲鹏920 CPU   | 512GB         | 1024k                    |\n\n如上表所示，CPU的大容量内存使其在处理长上下文请求时具备天然优势。\n\n#### 2.2 痛点二：整机资源利用率低\n\n在“CPU + XPU”的异构服务器中，传统推理模式导致整机利用率不佳。\n\n- **计算负载不均**：在前向计算过程中，XPU承担了超过95%的计算量。\n- **CPU闲置**：与此同时，CPU的算力和内存利用率极低，大部分时间处于等待状态，未能参与到有效的计算任务中。",
      "original_script": "首先，我们来看一下业务场景。\n在传统的推理场景中，大部分模型的计算都会落在GPU上。\n因为公众普遍认为CPU的算力较小，所以以往的推理一般都在GPU上进行。这时，CPU的算力和内存就会被闲置，也就是被浪费了。\n这个场景中有两个痛点。\n第一个痛点是XPU内存容量瓶颈。\n这里的XPU是指GPU和NPU。\nXPU的内存容量瓶颈如何理解？市面上常见的XPU，其内存容量通常是16GB到80GB。\n这个容量会限制模型推理的上下文长度和并发量。\n以Qwen-32B模型和10 batch的并发请求为例，这些XPU最多能存储3.2k到16k token的KV Cache。\n这意味着在一个请求中，它只能处理这么多token。\n而鲲鹏920 CPU的内存能达到512GB，最多能存储1024k token的KV Cache。由此可见，CPU的大内存可以处理更长的请求。\n第二个痛点是整机利用率不高。\n这里的整机是指CPU加XPU。利用率不高的主要原因是CPU在此过程中被闲置。\n例如，在前向计算过程中，XPU承担了95%以上的计算量，而CPU的算力和内存利用率都非常低。",
      "start_ms": 30590,
      "end_ms": 162410,
      "material_ids": [
        "chunk_00/slide_003"
      ],
      "summary_hint": "介绍传统推理场景中，计算集中于GPU导致CPU资源浪费，并指出XPU内存容量瓶颈是核心痛点。",
      "lead_text": "首先，我们来看一下业务场景。",
      "tail_text": "例如，在前向计算过程中，XPU承担了95%以上的计算量，而CPU的算力和内存利用率都非常低。",
      "text_span": {
        "start": 3,
        "end": 16
      }
    },
    {
      "id": "sec_3",
      "index": 2,
      "title": "CPU+XPU算力协同方案与PD分离",
      "time_range": "00:02:44,240 - 00:05:56,805",
      "summary": "为利用CPU算力，我们提出了CPU+XPU算力协同方案。该方案的核心是PD分离，即根据大模型推理中Prefill计算密集、Decode内存密集的特点，将Prefill放在XPU，而将部分Decode任务调度至CPU执行。此方案通过异构算力调度和通算推理加速技术，实现了对稠密和MoE模型的推理加速。",
      "refined_script": "## CPU+XPU 算力协同方案\n\n为充分利用系统中的 CPU 算力，我们设计了 CPU+XPU 算力协同方案，旨在通过异构计算加速大模型推理。\n\n### 核心概念：PD 分离\n\n方案的核心是 **PD 分离**（Prefill-Decode Separation），它基于大模型推理过程的两个不同阶段的特性进行任务划分。\n\n大模型推理包含两个核心阶段：\n- **Prefill**：处理输入 Prompt 并生成第一个 Token 的过程。\n- **Decode**：基于已生成的上下文，逐个生成后续新 Token 的过程。\n\n这两个阶段的计算特性差异显著：\n\n| 阶段    | 特点                         | 描述                                                                 |\n| :------ | :--------------------------- | :------------------------------------------------------------------- |\n| Prefill | **计算密集型** (Compute-intensive) | 涉及对整个输入序列的并行计算，计算量大。                             |\n| Decode  | **内存密集型** (Memory-intensive)  | 计算量小，但需频繁读写 KV Cache。Token 的生成过程为自回归的串行执行。 |\n\n由于 Decode 阶段的串行特性限制了 XPU 这类高并发算力硬件的有效利用率，使其出现“有力无处使”的情况。因此，将部分 Decode 任务卸载至 CPU 执行成为一种可行的优化路径。PD 分离正是利用此特性，将计算密集的 Prefill 任务固定在 XPU 执行，同时将部分 Decode 任务调度至 CPU，实现异构算力协同。\n\n### 方案架构\n\nCPU+XPU 算力协同方案主要由两部分组成：异构算力调度和通算推理加速。\n\n#### 1. 异构算力调度\n\n调度模块作为用户请求的代理，对推理任务进行 PD 分离处理：\n\n- **Prefill 调度**：将所有 Prefill 任务调度至 XPU 执行，以发挥其强大的并行计算能力。\n- **Decode 调度**：根据系统负载和任务特性，通过动态调度策略，将部分 Decode 任务分配至 CPU 执行，其余部分保留在 XPU 上，实现资源的灵活调度与负载均衡。\n\n#### 2. 通算推理加速\n\n为提升 CPU 在执行 Decode 任务时的性能，方案集成了针对通用计算（通算）场景的推理加速技术，主要包括：\n\n- **NUMA 亲和性 (NUMA Affinity)**：优化跨节点内存访问效率。\n- **多线程并行 (Multi-threaded Parallelism)**：在 CPU 核心上提升计算并行度。\n- **算子优化 (Operator Optimization)**：针对性优化关键计算算子。\n\n### 应用场景\n\n该协同方案可有效加速以下两类大模型的推理性能：\n\n- **稠密模型 (Dense Models)**\n- **混合专家模型 (MoE, Mixture-of-Experts Models)**",
      "original_script": "为了利用CPU的算力，我们提出了CPU+XPU算力协同方案。\n在此之前，我先介绍一个概念：PD分离。\n在大模型推理过程中，会涉及两个步骤：第一个是Prefill，第二个是Decode。\n文本生成的第一个token的处理过程就是Prefill，后续生成的每一个token的处理过程则对应Decode。\n在这个过程中，Prefill是计算密集型的，而Decode是内存密集型，其计算量非常小。根据这个特性，我们考虑将Prefill放在XPU上，而Decode可以放在CPU上，当然也可以放在XPU上。\n可能会有同学疑惑，CPU的算力明显小于XPU，用CPU辅助推理是否会导致整体吞吐量降低？关于这一点，举个例子，在峡谷中即使有十万大军，横向能展开的士兵也有限。这里同理。\n大模型推理的Decode过程所需计算量较小，且是串行执行的，即必须推理完一个token才能推理下一个。这使得算力强大的XPU在Decode过程中有点“有力无处使”，因此，在这个场景下使用CPU也是可行的。\n我们的CPU+XPU算力协同方案主要包含两部分。第一部分是异构算力调度。\n它会代理用户请求，当请求到达调度端后，我们会对其进行PD分离，即Prefill和Decode的分离。我们选择将Prefill放在XPU上，而将一部分Decode放在CPU上，另一部分放在XPU上。\n如何衡量哪些Decode应该放在CPU，哪些放在XPU，就涉及到动态调度。\n协同方案的第二部分是通算推理加速，主要使用了三项技术：NUMA亲和、多线程并行和算子优化。这项技术目前主要应用于两大类模型：稠密模型的推理加速和MoE模型的推理加速。",
      "start_ms": 164240,
      "end_ms": 356805,
      "material_ids": [
        "chunk_00/slide_004"
      ],
      "summary_hint": "为解决痛点，提出CPU+XPU算力协同方案。核心概念是PD分离，即将计算密集的Prefill置于XPU，内存密集的Decode置于CPU。",
      "lead_text": "为了利用CPU的算力，我们提出了CPU+XPU算力协同方案。",
      "tail_text": "协同方案的第二部分是通算推理加速，主要使用了三项技术：NUMA亲和、多线程并行和算子优化。这项技术目前主要应用于两大类模型：稠密模型的推理加速和MoE模型的推理加速。",
      "text_span": {
        "start": 17,
        "end": 27
      }
    },
    {
      "id": "sec_4",
      "index": 3,
      "title": "异构算力调度",
      "time_range": "00:06:00,320 - 00:09:12,115",
      "summary": "为平衡大模型推理服务中高并发与低时延的矛盾，我们设计了异构算力调度。通过优化vLLM以支持Prefill-Decode分离，并构建基于实时负载和历史数据预测的动态调度引擎，我们解决了KV Cache传输开销和动态任务分配的挑战。",
      "refined_script": "## 异构算力调度\n\n### 1. 背景与挑战\n\n在大模型推理服务中，服务提供商追求高并发以提升资源利用率，而用户则要求低时延以获得流畅的交互体验。为平衡高并发与低时延的矛盾，我们设计了异构算力调度方案。\n\n该方案主要面临两大技术挑战：\n\n1.  **KV Cache 传输开销**：Prefill-Decode 分离是优化推理性能的常用手段。在异构调度中，当 Prefill 步骤在 XPU 上完成后，其生成的 KV Cache 必须传输至 CPU，以便 CPU 接续执行 Decode 步骤，这一过程引入了额外的传输开销。\n2.  **动态任务分配决策**：如何动态地决定将 Decode 任务分配至 CPU 还是 XPU 是一个复杂问题。最优决策依赖于具体的硬件设备、实时负载及应用场景，需要一个智能的调度机制。\n\n### 2. 解决方案\n\n针对上述挑战，我们从以下两方面展开工作：\n\n-   **优化 vLLM 调度模块**：改造 vLLM，使其支持在异构算力（XPU 和 CPU）上实现 Prefill-Decode 分离。\n-   **构建动态调度引擎**：设计一个基于实时负载（如计算负载、显存占用）感知的动态调度引擎，以实现最优的任务分配。\n\n#### 2.1. 动态调度引擎设计\n\n动态调度引擎的核心是根据实时状态，决定将 Decode 任务分配到哪个计算单元能获得最大收益。其实现策略如下：\n\n##### 基础调度策略\n\n一种基础策略是基于实时吞吐量进行决策：\n\n-   **数据收集**：实时监测并收集 CPU 和 XPU 上每个批次（batch）的吞吐量数据。\n-   **调度决策**：当新的 Decode 请求到达时，将其分配给当前吞吐量更高的设备。\n-   **局限性**：当两个 Decode 请求到达的时间间隔极短时，调度器可能因来不及感知前一个任务带来的负载变化而做出次优决策，导致动态调度失效。\n\n##### 基于预测的增强调度策略\n\n为克服基础策略的局限性，我们引入了基于历史数据预测的增强策略：\n\n-   **历史数据收集**：持续记录历史调度信息，包括每次调度时的设备负载和最终实现的吞吐量。\n-   **模型预测**：利用收集到的历史数据训练一个预测模型。该模型能够根据系统当前状态，预测将一个 Decode 任务分配到特定设备后，该设备的吞吐量会如何变化。\n-   **精准调度**：基于模型的预测结果进行调度决策，从而实现更精准、更具前瞻性的动态任务分配。",
      "original_script": "我们先来看异构算力调度。\n为什么要进行异构算力调度？它能解决什么问题？\n大模型推理服务的提供商希望提高并发量，而用户则希望响应速度快，不能因为并发量提高而影响单个请求的时延。\n基于这个背景，我们设计了异构算力调度。\n它面临两大挑战：第一，Prefill-Decode分离会引入KV Cache的传输开销。因为在XPU上完成Prefill后，需要将KV Cache传输到CPU，CPU才能继续进行Decode推理。\n第二，如何动态决定将Decode任务分配到CPU还是XPU？这个问题比较复杂，因为它会因设备、应用场景的不同而变化。\n为解决这些挑战，我们做了两方面工作：首先，优化vLLM调度模块，使其支持在异构算力上进行PD分离。\n其次，构建基于实时负载感知的动态调度引擎。\n负载感知可以指计算负载、显存占用等。\n一个简单的做法是，实时收集CPU和XPU上每个batch的吞吐量，然后判断哪个设备的吞吐量更高，就将Decode请求分配到该设备，以期提高整体吞吐量。但这个方案存在一个问题：如果两个Decode请求到达的时间间隔过短，动态调度可能会因来不及反应而失效。\n为解决这个问题，我们可以收集历史调度信息，包括每次调度时的负载情况和吞吐量等。通过学习这些历史数据，我们可以预测在当前状态下，将Decode任务发送到某个设备后其吞吐量的变化，从而进行更精准的动态调度。",
      "start_ms": 360320,
      "end_ms": 552115,
      "material_ids": [
        "chunk_00/slide_005"
      ],
      "summary_hint": "为平衡高并发与低时延，设计了异构算力调度。通过动态调度，解决PD分离带来的KV Cache传输开销挑战。",
      "lead_text": "我们先来看异构算力调度。",
      "tail_text": "为解决这个问题，我们可以收集历史调度信息，包括每次调度时的负载情况和吞吐量等。通过学习这些历史数据，我们可以预测在当前状态下，将Decode任务发送到某个设备后其吞吐量的变化，从而进行更精准的动态调度。",
      "text_span": {
        "start": 28,
        "end": 38
      }
    },
    {
      "id": "sec_5",
      "index": 4,
      "title": "通算推理加速关键技术",
      "time_range": "00:09:17,320 - 00:12:37,085",
      "summary": "本节介绍了通算推理加速的三项关键技术：NUMA亲和、多线程并行和算子优化。通过将计算与数据本地化、利用多核并行处理及高级指令集，可显著提升CPU推理速度，并应用于稠密模型和MoE模型等场景。",
      "refined_script": "## 通算推理加速关键技术\n\n通算推理加速主要依赖三项关键技术：NUMA 亲和性、多线程并行和算子优化。这些技术旨在充分利用现代 CPU 架构的特性，提升大模型推理效率。\n\n### 1. NUMA 亲和性 (NUMA Affinity)\n\n- **背景**: 现代 CPU 普遍采用非统一内存访问（NUMA）架构。在此架构下，跨 NUMA 节点的内存访问延迟可能是节点内访问的 2 到 4 倍，开销巨大。\n- **优化策略**: 为降低在 CPU 端执行 Decode 任务时的访存开销，我们采取计算与数据本地化的策略：\n  1.  **权重分区**: 将完整的模型权重（例如，按 NUMA 节点数量进行纵向切分）预先加载。\n  2.  **数据本地化**: 将计算所需的 hidden state 和对应的权重分区拷贝到各个 NUMA 节点上。\n  3.  **本地计算**: 在每个 NUMA 节点内部独立执行计算任务。\n  4.  **结果聚合**: 计算完成后，合并各个 NUMA 节点的结果。\n\n### 2. 多线程并行 (Multi-threaded Parallelism)\n\n- **背景**: 现代 CPU 具备多核心设计，提供了强大的并行计算能力。\n- **优化策略**: 为了充分利用多核算力，我们采用多线程并行技术处理大模型推理中的计算密集型任务，例如对矩阵计算进行并行化处理。\n\n### 3. 算子优化 (Operator Optimization)\n\n- **背景**: 相比于一次只能处理单个运算的传统 CPU 指令，现代 CPU 架构（如 ARMv8 及之后版本）引入了可大幅加速计算的 SIMD（单指令多数据流）指令集。\n- **优化实例**:\n\n| 指令集 | 功能描述 |\n| :--- | :--- |\n| **NEON** | 一条指令可同时完成 4 次单精度浮点数乘法。 |\n| **I8MM** | 一条指令可计算 32 个 int8 整数的乘加运算。 |\n\n### 4. 应用场景\n\n通算推理加速技术主要应用于以下两种场景：\n\n- **稠密模型 (Dense Models)**\n  - **策略**: 当 Decode 阶段在 CPU 上执行时，将注意力（Attention）计算和前馈网络（FFN）计算全部保留在 CPU 端。\n  - **目的**: 避免 CPU 与 XPU 之间因频繁数据交换而产生的高昂开销。\n\n- **混合专家模型 (MoE Models)**\n  - **策略**: 采用混合计算模式：在 XPU 上执行注意力计算，同时在 CPU 上执行计算量巨大的专家（Expert）计算。\n  - **目的**: 发挥不同硬件的优势，将并行度高、计算密集的专家计算任务交由多核 CPU 高效处理。",
      "original_script": "关于通算推理加速，主要涉及三点：NUMA亲和、多线程并行和算子优化。首先是NUMA亲和。现在的CPU大多采用多NUMA内存架构，这种设计的一个特点是跨NUMA访存的开销可能是本NUMA内访存的2到4倍，非常昂贵。因此，当我们将Decode请求放在CPU端时，需要尽量降低跨NUMA访存的开销。我们的做法是将完整的模型权重（例如，纵向切割成四份，对应四个NUMA），以及计算所需的hidden state，分别拷贝到各个NUMA上进行本地化计算，最后再合并结果。其次是多线程并行。现代CPU多采用多核设计，提供了并行算力。为充分利用多核算力，我们采用多线程并行来处理大模型推理中的计算，例如对矩阵计算进行并行化处理。第三是算子优化。传统的CPU指令，一条乘法指令可能只能处理一次乘法。但在ARMv8架构之后，出现了许多可以加速计算的指令集。例如，NEON指令集中的一条指令可以一次完成四次浮点数乘法。而I8MM指令集扩展，其一条指令可以计算32个int8的乘法。这些都极大地加速了CPU的推理速度。通算推理加速目前主要应用于两个场景：稠密模型推理加速和MoE模型推理加速。对于稠密模型，如果我们将Decode放在CPU上执行，倾向于将注意力计算和FFN都放在CPU上完成，以避免与XPU之间频繁的数据交换产生开销。对于MoE模型，我们目前的策略是在XPU上完成注意力计算，而在CPU上进行大量的专家计算。",
      "start_ms": 557320,
      "end_ms": 757085,
      "material_ids": [
        "chunk_00/slide_006",
        "chunk_00/slide_007"
      ],
      "summary_hint": "详细介绍CPU端的推理加速技术，包括NUMA亲和、多线程并行和算子优化，并说明其在稠密模型和MoE模型中的具体应用。",
      "lead_text": "关于通算推理加速，主要涉及三点：NUMA亲和、多线程并行和算子优化。",
      "tail_text": "对于MoE模型，我们目前的策略是在XPU上完成注意力计算，而在CPU上进行大量的专家计算。",
      "text_span": {
        "start": 39,
        "end": 46
      }
    },
    {
      "id": "sec_6",
      "index": 5,
      "title": "业务效果展示",
      "time_range": "00:12:42,050 - 00:13:30,565",
      "summary": "在业务效果方面，该推理协同方案展现了显著的性能提升。在实时在线推理场景中，它能有效提升并发量；而在非实时离线批量推理场景下，则能大幅提高总吞吐量。",
      "refined_script": "# 业务效果展示\n\n与纯 XPU 原始方案相比，该推理协同方案在不同业务场景下均展现了显著的性能提升。\n\n### 1. 实时在线推理场景：提升并发量\n\n在此场景下，协同方案能够有效提升服务的并发处理能力。\n\n- **Qwen-32B 模型**: 并发量提升 **12.5%**\n- **Qwen-7B 模型**: 并发量提升 **8.8%**\n\n### 2. 非实时离线批量推理场景：提升总吞吐量\n\n在此场景下，协同方案能够大幅提高任务处理的总吞吐量。\n\n- **Qwen-32B 模型**: 总吞吐量提升 **20.1%**\n- **Qwen-7B 模型**: 总吞吐量提升 **7.1%**",
      "original_script": "关于业务效果，我们分两个场景来看。\n第一个是实时在线推理场景。在此场景下，与纯XPU的原始方案相比，我们的推理协同方案在Qwen-32B模型上可将并发量提升12.5%，在Qwen-7B模型上可提升8.8%。\n在非实时离线批量推理场景下，与原始方案相比，协同方案在Qwen-32B模型上可将总吞吐量提升20.1%，在Qwen-7B模型上可提升7.1%。",
      "start_ms": 762050,
      "end_ms": 810565,
      "material_ids": [
        "chunk_00/slide_008"
      ],
      "summary_hint": "展示协同方案带来的性能提升：在实时在线推理场景中并发量提升，在离线批量推理场景中总吞吐量提升。",
      "lead_text": "关于业务效果，我们分两个场景来看。",
      "tail_text": "在非实时离线批量推理场景下，与原始方案相比，协同方案在Qwen-32B模型上可将总吞吐量提升20.1%，在Qwen-7B模型上可提升7.1%。",
      "text_span": {
        "start": 47,
        "end": 49
      }
    },
    {
      "id": "sec_7",
      "index": 6,
      "title": "总结与未来工作展望",
      "time_range": "00:13:32,940 - 00:14:29,000",
      "summary": "演讲者总结了演讲内容，并展望了未来的工作方向。下一步将重点聚焦于MoE模型，探索其在多机多卡场景下的异构调度、AF分离（Attention和FFN/Expert分离）优化，以及Expert的分布式调度方案。",
      "refined_script": "# 总结与未来工作展望\n\n下一步的工作将重点聚焦于 MoE (Mixture-of-Experts) 模型，主要探索以下方向：\n\n*   **多机多卡异构调度**：研究 MoE 模型在多机多卡环境下的异构调度策略。\n*   **AF 分离优化**：针对 Attention 和 FFN/Expert 计算的分离（AF 分离）进行优化，以提升集群推理能力。\n*   **Expert 分布式调度**：探索 Expert 模块的分布式调度方案，以实现更高效的资源利用和性能。",
      "original_script": "我们下一步的工作将重点聚焦于MoE模型。我们会继续探索其在多机多卡场景下的异构调度，以及AF分离（Attention和FFN/Expert分离）的优化，还将探索Expert的分布式调度方案。好，谢谢大家，我的演讲完毕。感谢李老师为我们带来的通算与智算协同推理技术，非常精彩。刚才看到的性能提升效果令人印象深刻，我们特别期待后续在AF分离以及Expert分布式调度等面向集群能力方面的进展。好，谢谢。",
      "start_ms": 812940,
      "end_ms": 869000,
      "material_ids": [
        "chunk_00/slide_009",
        "chunk_00/slide_010"
      ],
      "summary_hint": "总结演讲内容，并提出下一步工作将聚焦于MoE模型的多机多卡异构调度、AF分离及Expert分布式调度。",
      "lead_text": "我们下一步的工作将重点聚焦于MoE模型。我们会继续探索其在多机多卡场景下的异构调度，以及AF分离（Attention和FFN/Expert分离）的优化，还将探索Expert的分布式调度方案。",
      "tail_text": "感谢李老师为我们带来的通算与智算协同推理技术，非常精彩。刚才看到的性能提升效果令人印象深刻，我们特别期待后续在AF分离以及Expert分布式调度等面向集群能力方面的进展。好，谢谢。",
      "text_span": {
        "start": 50,
        "end": 52
      }
    }
  ]
}