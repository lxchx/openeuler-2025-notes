{
  "id": "sec_2",
  "index": 1,
  "title": "业务背景与传统推理痛点",
  "time_range": "00:00:30,590 - 00:02:42,410",
  "script": "首先，我们来看一下业务场景。\n在传统的推理场景中，大部分模型的计算都会落在GPU上。\n因为公众普遍认为CPU的算力较小，所以以往的推理一般都在GPU上进行。这时，CPU的算力和内存就会被闲置，也就是被浪费了。\n这个场景中有两个痛点。\n第一个痛点是XPU内存容量瓶颈。\n这里的XPU是指GPU和NPU。\nXPU的内存容量瓶颈如何理解？市面上常见的XPU，其内存容量通常是16GB到80GB。\n这个容量会限制模型推理的上下文长度和并发量。\n以Qwen-32B模型和10 batch的并发请求为例，这些XPU最多能存储3.2k到16k token的KV Cache。\n这意味着在一个请求中，它只能处理这么多token。\n而鲲鹏920 CPU的内存能达到512GB，最多能存储1024k token的KV Cache。由此可见，CPU的大内存可以处理更长的请求。\n第二个痛点是整机利用率不高。\n这里的整机是指CPU加XPU。利用率不高的主要原因是CPU在此过程中被闲置。\n例如，在前向计算过程中，XPU承担了95%以上的计算量，而CPU的算力和内存利用率都非常低。",
  "summary": "在传统的推理场景中，计算主要集中在GPU上，导致CPU资源闲置，整机利用率低下。该模式存在两大痛点：一是XPU（GPU/NPU）的内存容量瓶颈限制了模型的上下文长度和并发量；二是CPU算力和内存被大量浪费。"
}