{
  "id": "sec_3",
  "index": 2,
  "title": "CPU+XPU算力协同方案与PD分离",
  "time_range": "00:02:44,240 - 00:05:56,805",
  "script": "为了利用CPU的算力，我们提出了CPU+XPU算力协同方案。\n在此之前，我先介绍一个概念：PD分离。\n在大模型推理过程中，会涉及两个步骤：第一个是Prefill，第二个是Decode。\n文本生成的第一个token的处理过程就是Prefill，后续生成的每一个token的处理过程则对应Decode。\n在这个过程中，Prefill是计算密集型的，而Decode是内存密集型，其计算量非常小。根据这个特性，我们考虑将Prefill放在XPU上，而Decode可以放在CPU上，当然也可以放在XPU上。\n可能会有同学疑惑，CPU的算力明显小于XPU，用CPU辅助推理是否会导致整体吞吐量降低？关于这一点，举个例子，在峡谷中即使有十万大军，横向能展开的士兵也有限。这里同理。\n大模型推理的Decode过程所需计算量较小，且是串行执行的，即必须推理完一个token才能推理下一个。这使得算力强大的XPU在Decode过程中有点“有力无处使”，因此，在这个场景下使用CPU也是可行的。\n我们的CPU+XPU算力协同方案主要包含两部分。第一部分是异构算力调度。\n它会代理用户请求，当请求到达调度端后，我们会对其进行PD分离，即Prefill和Decode的分离。我们选择将Prefill放在XPU上，而将一部分Decode放在CPU上，另一部分放在XPU上。\n如何衡量哪些Decode应该放在CPU，哪些放在XPU，就涉及到动态调度。\n协同方案的第二部分是通算推理加速，主要使用了三项技术：NUMA亲和、多线程并行和算子优化。这项技术目前主要应用于两大类模型：稠密模型的推理加速和MoE模型的推理加速。",
  "summary": "为利用CPU算力，我们提出了CPU+XPU算力协同方案。该方案的核心是PD分离，即根据大模型推理中Prefill计算密集、Decode内存密集的特点，将Prefill放在XPU，而将部分Decode任务调度至CPU执行。此方案通过异构算力调度和通算推理加速技术，实现了对稠密和MoE模型的推理加速。"
}