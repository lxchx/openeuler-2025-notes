{
  "id": "sec_4",
  "index": 3,
  "title": "异构算力调度",
  "time_range": "00:06:00,320 - 00:09:12,115",
  "script": "我们先来看异构算力调度。\n为什么要进行异构算力调度？它能解决什么问题？\n大模型推理服务的提供商希望提高并发量，而用户则希望响应速度快，不能因为并发量提高而影响单个请求的时延。\n基于这个背景，我们设计了异构算力调度。\n它面临两大挑战：第一，Prefill-Decode分离会引入KV Cache的传输开销。因为在XPU上完成Prefill后，需要将KV Cache传输到CPU，CPU才能继续进行Decode推理。\n第二，如何动态决定将Decode任务分配到CPU还是XPU？这个问题比较复杂，因为它会因设备、应用场景的不同而变化。\n为解决这些挑战，我们做了两方面工作：首先，优化vLLM调度模块，使其支持在异构算力上进行PD分离。\n其次，构建基于实时负载感知的动态调度引擎。\n负载感知可以指计算负载、显存占用等。\n一个简单的做法是，实时收集CPU和XPU上每个batch的吞吐量，然后判断哪个设备的吞吐量更高，就将Decode请求分配到该设备，以期提高整体吞吐量。但这个方案存在一个问题：如果两个Decode请求到达的时间间隔过短，动态调度可能会因来不及反应而失效。\n为解决这个问题，我们可以收集历史调度信息，包括每次调度时的负载情况和吞吐量等。通过学习这些历史数据，我们可以预测在当前状态下，将Decode任务发送到某个设备后其吞吐量的变化，从而进行更精准的动态调度。",
  "summary": "为平衡大模型推理服务中高并发与低时延的矛盾，我们设计了异构算力调度。通过优化vLLM以支持Prefill-Decode分离，并构建基于实时负载和历史数据预测的动态调度引擎，我们解决了KV Cache传输开销和动态任务分配的挑战。"
}