## CPU+XPU 算力协同方案

为充分利用系统中的 CPU 算力，我们设计了 CPU+XPU 算力协同方案，旨在通过异构计算加速大模型推理。

### 核心概念：PD 分离

方案的核心是 **PD 分离**（Prefill-Decode Separation），它基于大模型推理过程的两个不同阶段的特性进行任务划分。

大模型推理包含两个核心阶段：
- **Prefill**：处理输入 Prompt 并生成第一个 Token 的过程。
- **Decode**：基于已生成的上下文，逐个生成后续新 Token 的过程。

这两个阶段的计算特性差异显著：

| 阶段    | 特点                         | 描述                                                                 |
| :------ | :--------------------------- | :------------------------------------------------------------------- |
| Prefill | **计算密集型** (Compute-intensive) | 涉及对整个输入序列的并行计算，计算量大。                             |
| Decode  | **内存密集型** (Memory-intensive)  | 计算量小，但需频繁读写 KV Cache。Token 的生成过程为自回归的串行执行。 |

由于 Decode 阶段的串行特性限制了 XPU 这类高并发算力硬件的有效利用率，使其出现“有力无处使”的情况。因此，将部分 Decode 任务卸载至 CPU 执行成为一种可行的优化路径。PD 分离正是利用此特性，将计算密集的 Prefill 任务固定在 XPU 执行，同时将部分 Decode 任务调度至 CPU，实现异构算力协同。

### 方案架构

CPU+XPU 算力协同方案主要由两部分组成：异构算力调度和通算推理加速。

#### 1. 异构算力调度

调度模块作为用户请求的代理，对推理任务进行 PD 分离处理：

- **Prefill 调度**：将所有 Prefill 任务调度至 XPU 执行，以发挥其强大的并行计算能力。
- **Decode 调度**：根据系统负载和任务特性，通过动态调度策略，将部分 Decode 任务分配至 CPU 执行，其余部分保留在 XPU 上，实现资源的灵活调度与负载均衡。

#### 2. 通算推理加速

为提升 CPU 在执行 Decode 任务时的性能，方案集成了针对通用计算（通算）场景的推理加速技术，主要包括：

- **NUMA 亲和性 (NUMA Affinity)**：优化跨节点内存访问效率。
- **多线程并行 (Multi-threaded Parallelism)**：在 CPU 核心上提升计算并行度。
- **算子优化 (Operator Optimization)**：针对性优化关键计算算子。

### 应用场景

该协同方案可有效加速以下两类大模型的推理性能：

- **稠密模型 (Dense Models)**
- **混合专家模型 (MoE, Mixture-of-Experts Models)**