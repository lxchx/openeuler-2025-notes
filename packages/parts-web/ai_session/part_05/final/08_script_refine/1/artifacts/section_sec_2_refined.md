## 业务背景与传统推理痛点

### 1. 传统推理模式概述

在传统的推理场景中，由于GPU具备强大的并行计算能力，大部分模型计算任务（尤其是前向计算）都会被调度至GPU上执行。这种模式导致了以下问题：

- **CPU资源闲置**：在GPU执行计算时，CPU的算力与内存资源处于低利用率状态，造成了显著的资源浪费。

### 2. 主要痛点分析

传统的GPU中心化推理模式主要存在两大痛点：

#### 2.1 痛点一：XPU内存容量瓶颈

XPU（指GPU/NPU等加速器）的显存容量是限制大模型推理性能的关键因素。

- **显存限制**：市面上主流XPU的显存容量通常在16GB到80GB之间。
- **性能影响**：有限的显存直接限制了模型能够处理的上下文长度（Context Length）和并发请求数量（Batch Size）。

以Qwen-32B模型在10个并发请求（Batch=10）的场景为例，不同硬件平台可承载的KV Cache容量对比如下：

| 硬件平台      | 内存/显存容量 | 可存储的KV Cache (Token) |
| :------------ | :------------ | :----------------------- |
| XPU (GPU/NPU) | 16GB - 80GB   | 3.2k - 16k               |
| 鲲鹏920 CPU   | 512GB         | 1024k                    |

如上表所示，CPU的大容量内存使其在处理长上下文请求时具备天然优势。

#### 2.2 痛点二：整机资源利用率低

在“CPU + XPU”的异构服务器中，传统推理模式导致整机利用率不佳。

- **计算负载不均**：在前向计算过程中，XPU承担了超过95%的计算量。
- **CPU闲置**：与此同时，CPU的算力和内存利用率极低，大部分时间处于等待状态，未能参与到有效的计算任务中。