## 异构算力调度

### 1. 背景与挑战

在大模型推理服务中，服务提供商追求高并发以提升资源利用率，而用户则要求低时延以获得流畅的交互体验。为平衡高并发与低时延的矛盾，我们设计了异构算力调度方案。

该方案主要面临两大技术挑战：

1.  **KV Cache 传输开销**：Prefill-Decode 分离是优化推理性能的常用手段。在异构调度中，当 Prefill 步骤在 XPU 上完成后，其生成的 KV Cache 必须传输至 CPU，以便 CPU 接续执行 Decode 步骤，这一过程引入了额外的传输开销。
2.  **动态任务分配决策**：如何动态地决定将 Decode 任务分配至 CPU 还是 XPU 是一个复杂问题。最优决策依赖于具体的硬件设备、实时负载及应用场景，需要一个智能的调度机制。

### 2. 解决方案

针对上述挑战，我们从以下两方面展开工作：

-   **优化 vLLM 调度模块**：改造 vLLM，使其支持在异构算力（XPU 和 CPU）上实现 Prefill-Decode 分离。
-   **构建动态调度引擎**：设计一个基于实时负载（如计算负载、显存占用）感知的动态调度引擎，以实现最优的任务分配。

#### 2.1. 动态调度引擎设计

动态调度引擎的核心是根据实时状态，决定将 Decode 任务分配到哪个计算单元能获得最大收益。其实现策略如下：

##### 基础调度策略

一种基础策略是基于实时吞吐量进行决策：

-   **数据收集**：实时监测并收集 CPU 和 XPU 上每个批次（batch）的吞吐量数据。
-   **调度决策**：当新的 Decode 请求到达时，将其分配给当前吞吐量更高的设备。
-   **局限性**：当两个 Decode 请求到达的时间间隔极短时，调度器可能因来不及感知前一个任务带来的负载变化而做出次优决策，导致动态调度失效。

##### 基于预测的增强调度策略

为克服基础策略的局限性，我们引入了基于历史数据预测的增强策略：

-   **历史数据收集**：持续记录历史调度信息，包括每次调度时的设备负载和最终实现的吞吐量。
-   **模型预测**：利用收集到的历史数据训练一个预测模型。该模型能够根据系统当前状态，预测将一个 Decode 任务分配到特定设备后，该设备的吞吐量会如何变化。
-   **精准调度**：基于模型的预测结果进行调度决策，从而实现更精准、更具前瞻性的动态任务分配。