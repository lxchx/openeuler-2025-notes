## 通算推理加速关键技术

通算推理加速主要依赖三项关键技术：NUMA 亲和性、多线程并行和算子优化。这些技术旨在充分利用现代 CPU 架构的特性，提升大模型推理效率。

### 1. NUMA 亲和性 (NUMA Affinity)

- **背景**: 现代 CPU 普遍采用非统一内存访问（NUMA）架构。在此架构下，跨 NUMA 节点的内存访问延迟可能是节点内访问的 2 到 4 倍，开销巨大。
- **优化策略**: 为降低在 CPU 端执行 Decode 任务时的访存开销，我们采取计算与数据本地化的策略：
  1.  **权重分区**: 将完整的模型权重（例如，按 NUMA 节点数量进行纵向切分）预先加载。
  2.  **数据本地化**: 将计算所需的 hidden state 和对应的权重分区拷贝到各个 NUMA 节点上。
  3.  **本地计算**: 在每个 NUMA 节点内部独立执行计算任务。
  4.  **结果聚合**: 计算完成后，合并各个 NUMA 节点的结果。

### 2. 多线程并行 (Multi-threaded Parallelism)

- **背景**: 现代 CPU 具备多核心设计，提供了强大的并行计算能力。
- **优化策略**: 为了充分利用多核算力，我们采用多线程并行技术处理大模型推理中的计算密集型任务，例如对矩阵计算进行并行化处理。

### 3. 算子优化 (Operator Optimization)

- **背景**: 相比于一次只能处理单个运算的传统 CPU 指令，现代 CPU 架构（如 ARMv8 及之后版本）引入了可大幅加速计算的 SIMD（单指令多数据流）指令集。
- **优化实例**:

| 指令集 | 功能描述 |
| :--- | :--- |
| **NEON** | 一条指令可同时完成 4 次单精度浮点数乘法。 |
| **I8MM** | 一条指令可计算 32 个 int8 整数的乘加运算。 |

### 4. 应用场景

通算推理加速技术主要应用于以下两种场景：

- **稠密模型 (Dense Models)**
  - **策略**: 当 Decode 阶段在 CPU 上执行时，将注意力（Attention）计算和前馈网络（FFN）计算全部保留在 CPU 端。
  - **目的**: 避免 CPU 与 XPU 之间因频繁数据交换而产生的高昂开销。

- **混合专家模型 (MoE Models)**
  - **策略**: 采用混合计算模式：在 XPU 上执行注意力计算，同时在 CPU 上执行计算量巨大的专家（Expert）计算。
  - **目的**: 发挥不同硬件的优势，将并行度高、计算密集的专家计算任务交由多核 CPU 高效处理。