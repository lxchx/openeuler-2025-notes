1
00:00:00,000 --> 00:00:04,790
下面有请华为技术有限公司的软件工程师郑晨卉女士，

2
00:00:04,810 --> 00:00:17,000
为我们带来ANNC，基于XLA的深度学习图编译器的技术在搜推场景的探索与应用分享，欢迎。

3
00:00:22,610 --> 00:00:23,749
大家好。

4
00:00:23,750 --> 00:00:27,130
我是来自华为技术有限公司的软件工程师郑晨卉。

5
00:00:27,410 --> 00:00:34,229
今天想给大家分享一下我们团队最新的一个研究成果，就是ANNC。

6
00:00:34,230 --> 00:00:40,205
这是一款面向CPU的，加速深度学习模型推理的一款AI编译器。

7
00:00:41,290 --> 00:00:44,570
首先让我们从一个简单的问题作为导入。

8
00:00:44,710 --> 00:00:53,910
可能很多人都很好奇，在GPU、NPU等异构算力蓬勃发展的今天，我们为什么还要在CPU上进行AI推理？

9
00:00:54,330 --> 00:00:58,889
那我们从性能和成本的两个方面给出了我们的一个分析。

10
00:00:58,890 --> 00:01:00,730
首先在性能层面，

11
00:01:00,810 --> 00:01:11,680
CPU在处理某些特定场景时，它相较于AI专用加速器是没有性能劣势的，甚至会有一些它的性能优势。

12
00:01:11,700 --> 00:01:28,840
像在搜推、广告场景中比较常用的一些操作，像embedding查表、序列解码等等，那这些操作的特点就是比较稀疏，然后它的数据访问的规则是不规则的，也是计算密度比较低。

13
00:01:29,060 --> 00:01:38,580
那CPU的一个多层级cache的优势，就可以让它在处理这类数据时，它的表现会比GPU更为出色。

14
00:01:39,020 --> 00:01:58,090
然后另一类的话就是对端到端时延要求比较高的场景，比如说广告、实时推荐等，那纯CPU推理的话就减少了模型下发、数据拷贝等操作，并且也不需要在CPU和xPU之间进行一个通信，减少了这样一个通信开销。

15
00:01:58,130 --> 00:02:04,269
特别是当模型比较小、计算量比较小的时候，这种优势就会更加的明显。

16
00:02:04,270 --> 00:02:12,859
那从成本的角度上面可以看到，就是说在低算力要求的一个情况下，CPU的它的硬件成本会更低。

17
00:02:12,860 --> 00:02:22,180
以及它在软件栈的角度来说，它会比较成熟，在后续的一个开发和维护的成本也相对要更低。

18
00:02:22,220 --> 00:02:35,662
然后基于此，我们也发现，很多的互联网公司，尤其是在搜推、粗排以及广告等这些核心的业务中，普遍都有一些非常可观的纯CPU的算力部署。

19
00:02:36,390 --> 00:02:43,649
基于这样的一个背景，我们就构建了一套面向CPU的神经网络推理加速编译器ANNC。

20
00:02:43,650 --> 00:02:47,570
然后可以看一下，左边就是我们ANNC的一个整体架构。

21
00:02:47,590 --> 00:02:56,655
然后我们是ANNC是对上向上去对接TensorFlow、PyTorch等一系列的开源框架，向下是亲和我们的鲲鹏硬件。

22
00:02:57,540 --> 00:03:05,240
那我们的编译器，它的优化是覆盖了框架层和编译层，实现了一个端到端的一个优化的闭环。

23
00:03:05,740 --> 00:03:11,600
我们在框架层的话主要是一些包括自定义和通用的一些图算融合优化。

24
00:03:12,020 --> 00:03:21,880
这个主要是通过算法等价的方式去将多个独立的晓算子去融合成一个大的算子，来减少计算的开销以及内存的占用。

25
00:03:22,640 --> 00:03:36,090
那在编译部分的前端是主要是一些与硬件无关的一些图优化，包括冗余算子的一个消除、CPU感知的图优化以及多内核搜索等等策略。

26
00:03:36,350 --> 00:03:53,390
然后后端主要是一些算子层面的与硬件亲和的一些优化，包括像自动生成算子、代码的代码的指令级别的优化，以及我们也可以支持对接一些开源的成熟的一些算子库。

27
00:03:53,950 --> 00:04:01,780
那在实际应用中的话，我们的ANNC在多个开源的推荐模型下获得了20%的一个性能提升。

28
00:04:02,280 --> 00:04:08,575
然后在某个客户推荐的一个模型下，能够获得一个25%的性能提升。

29
00:04:12,330 --> 00:04:17,070
然后具体来介绍一下我们ANNC的几个关键的一些优化特性。

30
00:04:17,250 --> 00:04:20,169
首先是针对这个矩阵乘算子的数据布局优化。

31
00:04:20,690 --> 00:04:29,870
那在传统CPU在处理这个矩阵乘算子的时候，传统的做法是需要在运行时对输入数据进行一个重排打包。

32
00:04:30,630 --> 00:04:39,210
这个目的就是为了让数据去适配CPU的它的一个缓存大小以及它的一个内存访问的模式。

33
00:04:39,630 --> 00:04:43,720
那这个过程，这个动态的过程就会带来一定的运行时性能开销。

34
00:04:44,220 --> 00:04:49,660
那我们的优化就是将这个过程去前移到编译期。

35
00:04:49,960 --> 00:05:01,980
在对于就是常量输入，就是类似矩阵的权重，这类常量输入，我们直接在编译期对它进行一个静态的一个数据打包。

36
00:05:02,080 --> 00:05:11,945
然后同时在对多个连续的一个矩阵乘算子来说，我们会在它们之间进行一个最优的布局传递。

37
00:05:12,470 --> 00:05:30,539
就像右边这个图所示，比如说有这样子ABC三个矩阵乘算子，像B的输出就是C的输入，那我们就可以在B的输出写入内存的时候，直接就按照C的最佳的输入布局去进行一个写入。

38
00:05:30,540 --> 00:05:35,300
那这样子的话，我们的运行时就无需进行任何的一些动态重排操作。

39
00:05:35,420 --> 00:05:49,588
这样子所有的数据已经以最适合当前CPU硬件的布局准备好了，这样子就不需要再去进行一个重排操作，这样子就显著地降低了一个访存开销。

40
00:05:50,338 --> 00:05:55,095
那我们这个优化的话，是在开源推荐模型当中提升了5%的一个吞吐率。

41
00:05:57,050 --> 00:06:08,790
然后这是一个是面向利用了CPU的一些cache line、指令集等特征，去设计的一个自适应的算子优化和快速生成的一个技术。

42
00:06:08,970 --> 00:06:20,689
然后我们是主要是对MatMul等关键算子进行了一个模板化抽象，通过一些高级别的一个模板编译，去快速生成适配当前硬件的不同形状的一个最优算子。

43
00:06:20,690 --> 00:06:23,409
然后我们这个方案的话主要是有三大特点。

44
00:06:23,410 --> 00:06:25,610
首先是硬件感知优化。

45
00:06:25,770 --> 00:06:36,180
我们的编译器后端可以感知到CPU的一些硬件信息，去自动生成当前对算子生成的一个代码进行一个针对性的优化。

46
00:06:36,760 --> 00:06:51,060
然后第二个是中端的一个智能的一个优化，我们是基于MLIR去完成了一个对像tiling、packing、buffer复用等的一些数据流与控制流的一些优化。

47
00:06:51,680 --> 00:07:02,105
第三个是通过模板化设计的话，我们可以快速地修改、调整算子的各种参数，来生成一个最佳的一个融合算子。

48
00:07:03,370 --> 00:07:12,190
那实测表明的话，我们生成的算子对OpenBLAS可以获得一个9%到123%的一个性能收益。

49
00:07:12,630 --> 00:07:20,090
并且由于我们这个是模板化的一个算子生成，可以大幅度地提高一个调试以及快速迭代的一个效率。

50
00:07:20,450 --> 00:07:30,745
那未来我们这个相关的些pass也会在openEuler社区开源，然后下一步的话我们也是计划去结合Triton去提供一个更强的算子生成能力。

51
00:07:32,950 --> 00:07:40,310
然后接下来我将结合具体的一个场景，就是分享几个我们在客户场景中成功落地的两个案例。

52
00:07:40,890 --> 00:07:44,290
那首先是针对这个embedding层的一个图算融合的优化。

53
00:07:44,730 --> 00:07:52,730
我们发现embedding层在我们客户的广告模型当中，占比高达30%到40%，这个优化空间还是比较大的。

54
00:07:53,070 --> 00:08:06,560
然后embedding层的这个特点就是说，它有包含大量的小算子以及频繁的内存操作，它的性能的瓶颈不在于它的计算，而在于它的那个调度和访存。

55
00:08:06,920 --> 00:08:17,235
然后由于它的那个逻辑结构会比较复杂，所以通用的图优化难度会比较大，那这里的话我们就是采用了一个人工的一个图融合的一个方案。

56
00:08:18,320 --> 00:08:38,059
我们会基于就是profiling采集的一些信息，去定位到embedding层的embedding层的性能瓶颈的一个部分，然后，再去基于算法原理去拆解出可融合并且高频可复用的一些pattern。

57
00:08:38,060 --> 00:08:42,400
然后具体的实现上来说的话，主要是分为两个关键的步骤。

58
00:08:42,540 --> 00:08:57,645
首先是在图层面，我们会对节点进行一个融合，在原生的TF框架中我们去插入一个自定义的图重写模块，让去让图去匹配我们设定的一些融合pattern，并将其改写成一个大的一个融合算子。

59
00:08:58,790 --> 00:09:14,650
然后第二个的话就是去为每一个融合pattern去手写一个高性能的一个融合算子kernel，通过鲲鹏CPU的一些特殊的向量化指令去优化，去充分发挥我们的鲲鹏CPU的一个硬件性能。

60
00:09:14,850 --> 00:09:20,945
那这项优化的话也在客户模型当层面去提升了一个5%的推理性能。

61
00:09:23,500 --> 00:09:29,680
第二个案例是针对这个MatMul加BiasAdd和BatchNorm的一个常量折叠优化。

62
00:09:29,700 --> 00:09:50,280
我们发现，由于就是客户模型的构图问题，BatchNorm它这个算子被拆分成了多个细粒度的算子，那原生TF它是无法识别到这类就是拆分后的一些细粒度算子的一个整体语义，所以就无法去使能它的一个原生一个融合特性。

63
00:09:50,520 --> 00:10:13,090
那我们从算，我们从数学原理上可以分析到，就是当矩阵乘的权重、BiasAdd的偏置以及BatchNorm的参数都是常量的时候，这些操作其实可以通过一个数学的线性变换去做一个等效变换，融合后的话就会等价成一个MatMul加BiasAdd的一个操作。

64
00:10:13,770 --> 00:10:22,069
这个当中就相当于去省掉了一个BatchNorm的一个操作，这个操作的话实际就包含了有七个细碎的小op。

65
00:10:22,070 --> 00:10:26,730
然后在具体实现过程中的话，我们是对TF做了一个轻量式的一个修改。

66
00:10:27,170 --> 00:10:38,480
我们需要在去在模型加载阶段去获取到所有的我们需要的张量数据，然后在后续的图优化阶段去读取并完成我们的折叠操作。

67
00:10:38,800 --> 00:10:44,255
那这个优化的话是最终在单个模型上的推理时延降低了10%左右。

68
00:10:46,140 --> 00:10:53,360
那基于客户的反馈以及实际落地中的痛点，我们也梳理了下一步的重点的一个工作方向。

69
00:10:53,800 --> 00:11:13,160
首先我们是希望能够从手工融合走向一个自动融合，去结合我们之前已经积累的大量的人工调优的一个经验，去共建更通用的一个自动图融合的策略，来提升我们融合的一个泛化能力，去灵活地覆盖到更多场景的一个融合诉求。

70
00:11:13,840 --> 00:11:24,329
第二点的话就是去提升我们编译器对动态batch的一个支持能力，来适应更好的适应就是当前真实业务中的一个流量波动。

71
00:11:24,330 --> 00:11:38,670
因为我们当，我们我们现在的编译器是只支持静态shape，它在面对变长输入的时候，它会再次进行触发编译，这个过程就会增加我们的编译耗时以及它会生成一些大量冗余的一个编译结果。

72
00:11:39,170 --> 00:11:43,470
那第三点的话就是去增强算子算子间并行的一个优化。

73
00:11:43,830 --> 00:11:58,240
我们当前的图算融合会把多个小的算子去融合成一个单个算子的一个调用，那这个过程虽然减少了调度的开销，但是也降低了一个inter-op级别的一个并行性。

74
00:11:58,440 --> 00:12:10,750
那我们未来将去解决这个问题，去充分地去发挥这个多核CPU的一个并发性能，让我们的融合算子也能在CPU中进行一个并发的一个执行。

75
00:12:11,270 --> 00:12:19,515
然后最后在量化方面的话，我们也是计划去打造一套面向CPU的高精度搜推场景的自动PTQ工具链。

76
00:12:22,360 --> 00:12:37,080
最后的话就是我们ANNC的源代码以及相关的文档已经在openEuler社区开源了，然后欢迎大家去下载使用，也给我们提出一些宝贵的意见，那我们未来后续的一些优化也会继续贡献到openEuler社区。

77
00:12:37,620 --> 00:12:40,000
以上就是我的分享，谢谢大家。

78
00:12:44,550 --> 00:12:47,049
好，那个非常感谢郑老师的这个精彩分享。

79
00:12:47,050 --> 00:13:00,910
就是其实大家可以看到我们在这个搜推广告的技术场景，从编译的方式就可以获得这个不错的性能，这个性能并且还不需要去，那个编译器能够自动完成这些相关的工作。

80
00:13:01,210 --> 00:13:11,750
那包括刚才提到的那个融合算子、常量折叠的两个相关案例，给我们得到那个吞吐的提升，非常棒的一个分享啊。像，好。
