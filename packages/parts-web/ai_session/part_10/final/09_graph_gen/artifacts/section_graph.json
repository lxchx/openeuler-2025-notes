{
  "graph_data": {
    "nodes": [
      {
        "id": "p1",
        "label": "CPU推理的动机",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_003"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_003",
            "primary_time_ts": "00:00:42,000",
            "primary_time_ms": 42000
          }
        },
        "parent": null,
        "summary": "分析在专用AI加速器蓬勃发展的背景下，继续在CPU上进行AI推理的必要性，主要从性能（稀疏数据处理、低延迟）和成本（硬件、开发维护）两个维度展开。"
      },
      {
        "id": "d1_1",
        "label": "性能优势",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_003"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_003",
            "primary_time_ts": "00:00:52,000",
            "primary_time_ms": 52000
          }
        },
        "parent": "p1",
        "summary": "CPU的多层级缓存使其在处理搜推、广告等场景的稀疏、不规则数据访问时更具优势，且能为小模型提供更低的端到端时延，避免了与加速器之间的通信和数据拷贝开销。"
      },
      {
        "id": "d1_2",
        "label": "成本优势",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_2"
          ],
          "material_ids": [
            "chunk_00/slide_003"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_003",
            "primary_time_ts": "00:01:33,000",
            "primary_time_ms": 93000
          }
        },
        "parent": "p1",
        "summary": "在低算力需求下，CPU硬件成本更低，且其成熟的软件栈降低了应用的开发、部署和长期维护成本。"
      },
      {
        "id": "p2",
        "label": "ANNC 核心架构与技术",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_1",
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_002",
            "chunk_00/slide_004",
            "chunk_00/slide_005",
            "chunk_00/slide_006"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_004",
            "primary_time_ts": "00:02:37,000",
            "primary_time_ms": 157000
          }
        },
        "parent": null,
        "summary": "介绍AI编译器ANNC的整体架构，涵盖框架层和编译层的优化，并深入讲解两大关键技术：矩阵乘的数据布局优化和自适应的算子生成技术。"
      },
      {
        "id": "d2_1",
        "label": "ANNC 简介",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_1"
          ],
          "material_ids": [
            "chunk_00/slide_001",
            "chunk_00/slide_002"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_002",
            "primary_time_ts": "00:00:08,000",
            "primary_time_ms": 8000
          }
        },
        "parent": "p2",
        "summary": "ANNC是一款基于XLA的AI编译器，专为加速CPU上的深度学习模型推理而设计，尤其关注搜索与推荐等应用场景。"
      },
      {
        "id": "d2_2",
        "label": "ANNC 整体架构",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_004"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_004",
            "primary_time_ts": "00:02:40,000",
            "primary_time_ms": 160000
          }
        },
        "parent": "p2",
        "summary": "ANNC架构贯穿框架层（图算融合）和编译层（图优化、算子优化），向上对接主流框架，向下亲和鲲鹏硬件，实现端到端优化闭环。"
      },
      {
        "id": "d2_3",
        "label": "矩阵乘数据布局优化",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_005"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_005",
            "primary_time_ts": "00:04:12,000",
            "primary_time_ms": 252000
          }
        },
        "parent": "p2",
        "summary": "通过编译期静态打包权重和算子间最优布局传递，将数据重排（Repack）操作从运行时前移到编译期，消除运行时开销，显著降低访存压力，提升5%吞吐率。"
      },
      {
        "id": "d2_4",
        "label": "自适应算子优化与生成",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_3"
          ],
          "material_ids": [
            "chunk_00/slide_006"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_006",
            "primary_time_ts": "00:05:57,000",
            "primary_time_ms": 357000
          }
        },
        "parent": "p2",
        "summary": "利用硬件感知优化、基于MLIR的智能中端优化和模板化设计，为MatMul等关键算子快速生成适配特定硬件和形状（Shape）的高性能代码，性能优于OpenBLAS。"
      },
      {
        "id": "p3",
        "label": "客户模型调优案例",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_4"
          ],
          "material_ids": [
            "chunk_00/slide_007",
            "chunk_00/slide_008"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_007",
            "primary_time_ts": "00:07:33,000",
            "primary_time_ms": 453000
          }
        },
        "parent": null,
        "summary": "分享两个在客户实际业务中落地的调优案例：针对广告模型的Embedding层融合优化，以及通过常量折叠优化MatMul、BiasAdd和BatchNorm组合。"
      },
      {
        "id": "d3_1",
        "label": "案例一：Embedding层图算融合",
        "category": "Result",
        "references": {
          "section_ids": [
            "sec_4"
          ],
          "material_ids": [
            "chunk_00/slide_007"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_007",
            "primary_time_ts": "00:07:33,000",
            "primary_time_ms": 453000
          }
        },
        "parent": "p3",
        "summary": "针对广告模型中计算占比高的Embedding层，通过人工识别融合模式并手写高性能Kernel，将大量小算子融合成一个大算子，提升了5%的推理性能。"
      },
      {
        "id": "d3_2",
        "label": "案例二：常量折叠优化",
        "category": "Result",
        "references": {
          "section_ids": [
            "sec_4"
          ],
          "material_ids": [
            "chunk_00/slide_008"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_008",
            "primary_time_ts": "00:09:24,000",
            "primary_time_ms": 564000
          }
        },
        "parent": "p3",
        "summary": "利用数学等效变换，将MatMul、BiasAdd及被拆分的BatchNorm操作折叠为一个MatMul加BiasAdd，省去多个小算子计算，使单个模型推理时延降低10%。"
      },
      {
        "id": "p4",
        "label": "未来规划与开源",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_5",
            "sec_6"
          ],
          "material_ids": [
            "chunk_00/slide_009",
            "chunk_00/slide_010",
            "chunk_00/slide_011"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_009",
            "primary_time_ts": "00:10:47,000",
            "primary_time_ms": 647000
          }
        },
        "parent": null,
        "summary": "基于落地痛点，规划了ANNC的下一步工作，包括自动图融合、支持动态Batch、优化并行执行和构建PTQ量化工具链，并宣布项目已在openEuler社区开源。"
      },
      {
        "id": "d4_1",
        "label": "下一步工作方向",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_5"
          ],
          "material_ids": [
            "chunk_00/slide_009"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_009",
            "primary_time_ts": "00:10:47,000",
            "primary_time_ms": 647000
          }
        },
        "parent": "p4",
        "summary": "重点工作包括：从手工融合走向自动融合，增强对动态Batch的支持，优化算子间并行执行以发挥多核性能，以及打造面向CPU的自动PTQ量化工具链。"
      },
      {
        "id": "d4_2",
        "label": "开源与资源",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_6"
          ],
          "material_ids": [
            "chunk_00/slide_010",
            "chunk_00/slide_011"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_010",
            "primary_time_ts": "00:12:23,000",
            "primary_time_ms": 743000
          }
        },
        "parent": "p4",
        "summary": "ANNC的源代码及相关文档已在openEuler社区开源，欢迎社区下载使用、提出意见，未来优化成果也将持续贡献至社区。"
      }
    ],
    "edges": [
      {
        "source": "p1",
        "target": "p2",
        "relation_type": "sequential",
        "label": "引出"
      },
      {
        "source": "p2",
        "target": "p3",
        "relation_type": "sequential",
        "label": "应用于"
      },
      {
        "source": "p3",
        "target": "p4",
        "relation_type": "sequential",
        "label": "展望未来"
      },
      {
        "source": "d1_1",
        "target": "d1_2",
        "relation_type": "parallel",
        "label": "并列优势"
      },
      {
        "source": "d2_1",
        "target": "d2_2",
        "relation_type": "compositional",
        "label": "展开为"
      },
      {
        "source": "d2_2",
        "target": "d2_3",
        "relation_type": "compositional",
        "label": "包含关键技术"
      },
      {
        "source": "d2_3",
        "target": "d2_4",
        "relation_type": "parallel",
        "label": "并列关键技术"
      },
      {
        "source": "d3_1",
        "target": "d3_2",
        "relation_type": "parallel",
        "label": "并列案例"
      },
      {
        "source": "d4_1",
        "target": "d4_2",
        "relation_type": "parallel",
        "label": "并列主题"
      }
    ]
  },
  "sections": [
    {
      "id": "sec_1",
      "index": 0,
      "title": "ANNC：加速深度学习模型推理的AI编译器",
      "time_range": "00:00:00,000 - 00:00:40,205",
      "summary": "华为软件工程师郑晨卉介绍了团队的最新研究成果 ANNC。ANNC 是一款面向 CPU、用于加速深度学习模型推理的 AI 编译器。",
      "refined_script": "# ANNC：加速深度学习模型推理的AI编译器\n\n## 1. 概述\n\nANNC (ANNC: An XLA-based Deep Learning Graph Compiler) 是华为推出的一款 AI 编译器，旨在加速深度学习模型在 CPU 上的推理性能。\n\n### 核心特性\n\n- **定位**：基于 XLA 的深度学习图编译器。\n- **目标**：加速深度学习模型的推理（Inference）过程。\n- **目标平台**：CPU。\n- **应用场景**：专注于搜索与推荐（搜推）等场景的探索与应用。",
      "original_script": "下面有请华为技术有限公司的软件工程师郑晨卉女士，为我们带来ANNC，基于XLA的深度学习图编译器的技术在搜推场景的探索与应用分享，欢迎。\n大家好。\n我是来自华为技术有限公司的软件工程师郑晨卉。\n今天想给大家分享一下我们团队最新的一个研究成果，就是ANNC。\n这是一款面向CPU的，加速深度学习模型推理的一款AI编译器。",
      "start_ms": 0,
      "end_ms": 40205,
      "material_ids": [
        "chunk_00/slide_001",
        "chunk_00/slide_002"
      ],
      "summary_hint": "讲者介绍了本次分享的主题 ANNC，一款面向CPU、加速深度学习模型推理的AI编译器。",
      "lead_text": "下面有请华为技术有限公司的软件工程师郑晨卉女士，",
      "tail_text": "这是一款面向CPU的，加速深度学习模型推理的一款AI编译器。",
      "text_span": {
        "start": 0,
        "end": 5
      }
    },
    {
      "id": "sec_2",
      "index": 1,
      "title": "为何在CPU上进行AI推理？",
      "time_range": "00:00:41,290 - 00:02:35,662",
      "summary": "该部分从性能和成本两个角度，分析了在CPU上进行AI推理的必要性。性能上，CPU的多层级缓存使其在处理搜推、广告等场景的稀疏、不规则数据访问时更具优势，且能为小模型提供更低的端到端时延。成本上，CPU在低算力需求下硬件成本更低，软件栈更成熟，从而降低了开发和维护成本。",
      "refined_script": "## 为何在 CPU 上进行 AI 推理？\n\n在 GPU、NPU 等专用 AI 加速器蓬勃发展的背景下，CPU 依然是 AI 推理的重要算力平台。其必要性主要体现在性能和成本两个方面。\n\n### 性能优势\n\n在特定场景下，CPU 进行 AI 推理相较于专用 AI 加速器具备独特的性能优势。\n\n*   **高效处理稀疏与不规则数据访问**\n    *   **应用场景**：搜索、推荐、广告等业务中的 Embedding 查表、序列解码等操作。\n    *   **技术特点**：这类操作具有数据访问稀疏、模式不规则、计算密度低的特点。\n    *   **CPU 优势**：CPU 的多层级缓存（Multi-level Cache）架构非常适合处理此类数据访问模式，其性能表现通常优于 GPU 等加速器。\n\n*   **低端到端时延**\n    *   **应用场景**：对时延要求极高的应用，如广告竞价、实时推荐等。\n    *   **CPU 优势**：纯 CPU 推理路径更短，避免了模型下发、内存拷贝（CPU 与加速器之间）以及通信等额外开销。\n    *   **适用模型**：当模型规模和计算量较小时，这种时延优势尤为显著。\n\n### 成本优势\n\n从总体拥有成本（TCO）来看，CPU 在某些情况下是更经济的选择。\n\n*   **硬件成本**：在低算力需求的场景下，CPU 的单位硬件成本低于专用 AI 加速器。\n*   **开发与维护成本**：CPU 拥有非常成熟的软件栈，降低了应用的开发、部署和长期维护成本。\n\n### 结论与行业应用\n\n综合性能与成本两大因素，许多互联网公司的核心业务（如搜索、推荐系统粗排、广告等）中，均部署了规模可观的纯 CPU AI 推理算力。",
      "original_script": "首先让我们从一个简单的问题作为导入。\n可能很多人都很好奇，在GPU、NPU等异构算力蓬勃发展的今天，我们为什么还要在CPU上进行AI推理？\n那我们从性能和成本的两个方面给出了我们的一个分析。\n首先在性能层面，\nCPU在处理某些特定场景时，它相较于AI专用加速器是没有性能劣劣势的，甚至会有一些它的性能优势。\n像在搜推、广告场景中比较常用的一些操作，像embedding查表、序列解码等等，那这些操作的特点就是比较稀疏，然后它的数据访问的规则是不规则的，也是计算密度比较低。\n那CPU的一个多层级cache的优势，就可以让它在处理这类数据时，它的表现会比GPU更为出色。\n然后另一类的话就是对端到端时延要求比较高的场景，比如说广告、实时推荐等，那纯CPU推理的话就减少了模型下发、数据拷贝等操作，并且也不需要在CPU和xPU之间进行一个通信，减少了这样一个通信开销。\n特别是当模型比较小、计算量比较小的时候，这种优势就会更加的明显。\n那从成本的角度上面可以看到，就是说在低算力要求的一个情况下，CPU的它的硬件成本会更低。\n以及它在软件栈的角度来说，它会比较成熟，在后续的一个开发和维护的成本也相对要更低。\n然后基于此，我们也发现，很多的互联网公司，尤其是在搜推、粗排以及广告等这些核心的业务中，普遍都有一些非常可观的纯CPU的算力部署。",
      "start_ms": 41290,
      "end_ms": 155662,
      "material_ids": [
        "chunk_00/slide_003"
      ],
      "summary_hint": "从性能和成本两个方面分析了在CPU上进行AI推理的优势，特别是在搜推、广告等场景。",
      "lead_text": "首先让我们从一个简单的问题作为导入。",
      "tail_text": "然后基于此，我们也发现，很多的互联网公司，尤其是在搜推、粗排以及广告等这些核心的业务中，普遍都有一些非常可观的纯CPU的算力部署。",
      "text_span": {
        "start": 6,
        "end": 17
      }
    },
    {
      "id": "sec_3",
      "index": 2,
      "title": "ANNC 整体架构与关键技术",
      "time_range": "00:02:36,390 - 00:07:30,745",
      "summary": "本节介绍了面向CPU的神经网络推理加速编译器ANNC的整体架构，其优化覆盖框架层和编译层，实现了端到端的优化闭环。接着，详细阐述了两大关键技术：一是针对矩阵乘算子的数据布局优化，通过编译期静态打包和最优布局传递，显著降低访存开销；二是一种自适应的算子优化和快速生成技术，通过硬件感知的模板化设计，可生成对特定硬件和形状最优的高性能算子。",
      "refined_script": "# ANNC 整体架构与关键技术\n\nANNC 是一款面向 CPU 的神经网络推理加速编译器，向上对接 TensorFlow、PyTorch 等主流开源框架，向下亲和鲲鹏硬件。其优化覆盖框架层与编译层，实现了端到端的优化闭环。\n\n在实际应用中，ANNC 在多个开源推荐模型上获得了 20% 的性能提升，在某客户的推荐模型下获得了 25% 的性能提升。\n\n## 一、整体架构\n\nANNC 的优化贯穿框架层与编译层，各层级的主要工作如下：\n\n- **框架层**：进行图算融合优化，通过算法等价的方式将多个独立的小算子融合成一个大算子，以减少计算开销和内存占用。\n- **编译层前端**：执行与硬件无关的图优化，包括：\n  - 冗余算子消除\n  - CPU 感知的图优化\n  - 多内核搜索策略\n- **编译层后端**：进行与硬件亲和的算子级优化，包括：\n  - 自动生成高性能算子\n  - 指令级代码优化\n  - 支持对接成熟的开源算子库\n\n## 二、关键技术\n\n### 1. 矩阵乘算子的数据布局优化\n\n传统的矩阵乘（MatMul）算子需要在运行时对输入数据进行重排（Repack），以适配 CPU 的缓存大小和内存访问模式，但这会引入额外的性能开销。\n\nANNC 的优化方案是将数据重排前移到编译期，从而消除运行时开销：\n\n- **编译期静态打包**：对于权重等常量输入，在编译阶段直接完成静态数据打包。\n- **最优布局传递**：对于连续的多个矩阵乘算子（例如 A->B->C），在前一个算子（B）的输出直接按下一个算子（C）的最佳输入布局写入内存。 \n\n通过该优化，运行时无需进行任何动态重排操作，数据已为硬件准备就绪，显著降低了访存开销。此项优化在开源推荐模型中提升了 5% 的吞吐率。\n\n### 2. 自适应的算子优化与快速生成\n\n该技术利用 CPU 的 Cache Line、指令集等硬件特征，设计了一套自适应的算子优化与快速生成方案。我们对 MatMul 等关键算子进行模板化抽象，通过高级别的模板编译，快速生成适配特定硬件与不同形状（Shape）的最优算子。\n\n该方案具备三大特点：\n\n1.  **硬件感知优化**：编译器后端能够感知 CPU 硬件信息，为生成的算子代码进行针对性优化。\n2.  **智能中端优化**：基于 MLIR 完成数据流与控制流的优化，如 Tiling、Packing、Buffer 复用等。\n3.  **模板化设计**：可快速修改和调整算子参数，以生成性能最佳的融合算子。\n\n实测表明，ANNC 生成的算子相比 OpenBLAS 可获得 9% 至 123% 的性能收益。同时，模板化的生成方式也大幅提高了调试和快速迭代的效率。\n\n## 三、未来规划\n\n- **社区开源**：计划在 openEuler 社区开源相关的优化 Pass。\n- **能力增强**：计划结合 Triton，提供更强的算子生成能力。",
      "original_script": "基于这样的一个背景，我们就构建了一套面向CPU的神经网络推理加速编译器ANNC。\n然后可以看一下，左边就是我们ANNC的一个整体架构。\n然后我们是ANNC是对上向上去对接TensorFlow、PyTorch等一系列的开源框架，向下是亲和我们的鲲鹏硬件。\n那我们的编译器，它的优化是覆盖了框架层和编译层，实现了一个端到端的一个优化的闭环。\n我们在框架层的话主要是一些包括自定义和通用的一些图算融合优化。\n这个主要是通过算法等价的方式去将多个独立的晓算子去融合成一个大的算子，来减少计算的开销以及内存的占用。\n那在编译部分的前端是主要是一些与硬件无关的一些图优化，包括冗余算子的一个消除、CPU感知的图优化以及多内核搜索等等策略。\n然后后端主要是一些算子层面的与硬件亲和的一些优化，包括像自动生成算子、代码的代码的指令级别的优化，以及我们也可以支持对接一些开源的成熟的一些算子库。\n那在实际应用中的话，我们的ANNC在多个开源的推荐模型下获得了20%的一个性能提升。\n然后在某个客户推荐的一个模型下，能够获得一个25%的性能提升。\n然后具体来介绍一下我们ANNC的几个关键的一些优化特性。\n首先是针对这个矩阵乘算子的数据布局优化。\n那在传统CPU在处理这个矩阵乘算子的时候，传统的做法是需要在运行时对输入数据进行一个重排打包。\n这个目的就是为了让数据去适配CPU的它的一个缓存大小以及它的一个内存访问的模式。\n那这个过程，这个动态的过程就会带来一定的运行时性能开销。\n那我们的优化就是将这个过程去前移到编译期。\n在对于就是常量输入，就是类似矩阵的权重，这类常量输入，我们直接在编译期对它进行一个静态的一个数据打包。\n然后同时在对多个连续的一个矩阵乘算子来说，我们会在它们之间进行一个最优的布局传递。\n就像右边这个图所示，比如说有这样子ABC三个矩阵乘算子，像B的输出就是C的输入，那我们就可以在B的输出写入内存的时候，直接就按照C的最佳的输入布局去进行一个写入。\n那这样子的话，我们的运行时就无需进行任何的一些动态重排操作。\n这样子所有的数据已经以最适合当前CPU硬件的布局准备好了，这样子就不需要再去进行一个重排操作，这样子就显著地降低了一个访存开销。\n那我们这个优化的话，是在开源推荐模型当中提升了5%的一个吞吐率。\n然后这是一个是面向利用了CPU的一些cache line、指令集等特征，去设计的一个自适应的算子优化和快速生成的一个技术。\n然后我们是主要是对MatMul等关键算子进行了一个模板化抽象，通过一些高级别的一个模板编译，去快速生成适配当前硬件的不同形状的一个最优算子。\n然后我们这个方案的话主要是有三大特点。\n首先是硬件感知优化。\n我们的编译器后端可以感知到CPU的一些硬件信息，去自动生成当前对算子生成的一个代码进行一个针对性的优化。\n然后第二个是中端的一个智能的一个优化，我们是基于MLIR去完成了一个对像tiling、packing、buffer复用等的一些数据流与控制流的一些优化。\n第三个是通过模板化设计的话，我们可以快速地修改、调整算子的各种参数，来生成一个最佳的一个融合算子。\n那实测表明的话，我们生成的算子对OpenBLAS可以获得一个9%到123%的一个性能收益。\n并且由于我们这个是模板化的一个算子生成，可以大幅度地提高一个调试以及快速迭代的一个效率。\n那未来我们这个相关的些pass也会在openEuler社区开源，然后下一步的话我们也是计划去结合Triton去提供一个更强的算子生成能力。",
      "start_ms": 156390,
      "end_ms": 450745,
      "material_ids": [
        "chunk_00/slide_004",
        "chunk_00/slide_005",
        "chunk_00/slide_006"
      ],
      "summary_hint": "介绍了ANNC的整体架构，并详细阐述了其关键技术，包括图优化、高性能算子库、GEMM算子数据布局优化以及CPU感知的自适应算子优化。",
      "lead_text": "基于这样的一个背景，我们就构建了一套面向CPU的神经网络推理加速编译器ANNC。",
      "tail_text": "那未来我们这个相关的些pass也会在openEuler社区开源，然后下一步的话我们也是计划去结合Triton去提供一个更强的算子生成能力。",
      "text_span": {
        "start": 18,
        "end": 49
      }
    },
    {
      "id": "sec_4",
      "index": 3,
      "title": "客户模型调优案例分享",
      "time_range": "00:07:32,950 - 00:10:44,255",
      "summary": "本节分享了两个客户模型调优案例。一是针对广告模型中占比高的Embedding层，通过人工图融合与手写高性能Kernel，将大量小算子融合成一个大算子，提升了5%的推理性能。二是通过常量折叠优化，将MatMul、BiasAdd及被拆分的BatchNorm操作等效变换为一个MatMul加BiasAdd操作，使单个模型推理时延降低了10%。",
      "refined_script": "# 客户模型调优案例分享\n\n本文分享两个在客户场景中成功落地的模型调优案例，分别针对 Embedding 层优化和常量折叠优化。\n\n## 案例一：Embedding 层图算融合优化\n\n### 1. 问题背景\n\n在客户的广告推荐模型中，Embedding 层的计算占比高达 30% 至 40%，存在较大的优化空间。其性能瓶颈主要表现为：\n\n- **包含大量小算子**：导致调度开销大。\n- **频繁的内存操作**：性能瓶颈在于访存而非计算。\n- **逻辑结构复杂**：通用的图优化方法难以生效。\n\n### 2. 解决方案\n\n我们采用了一种人工图融合的方案，结合手写高性能 Kernel 进行优化。具体步骤如下：\n\n1.  **瓶颈定位与模式识别**\n    -   通过 Profiling 工具采集的信息，精确定位 Embedding 层的性能瓶颈。\n    -   基于算法原理，拆解并识别出可融合、高频复用的计算模式（Pattern）。\n\n2.  **图层面融合**\n    -   在原生 TensorFlow 框架中插入一个自定义的图重写模块。\n    -   该模块负责匹配预设的融合 Pattern，并将其改写成一个大的融合算子。\n\n3.  **手写高性能 Kernel**\n    -   为每个融合 Pattern 手动编写一个高性能的融合算子 Kernel。\n    -   利用鲲鹏 CPU 的向量化指令进行深度优化，以充分发挥硬件性能。\n\n### 3. 优化效果\n\n此项优化使客户模型的**推理性能提升了 5%**。\n\n---\n\n## 案例二：MatMul、BiasAdd 与 BatchNorm 的常量折叠优化\n\n### 1. 问题背景\n\n由于客户模型的构图方式，`BatchNorm` 算子被拆分成了多个细粒度的算子。原生 TensorFlow 无法识别这些拆分后算子的整体语义，导致其自带的融合特性失效。\n\n### 2. 解决方案\n\n我们从数学原理出发，实现了常量折叠优化。\n\n-   **数学原理**：当矩阵乘（`MatMul`）的权重、`BiasAdd` 的偏置以及 `BatchNorm` 的参数均为常量时，这些连续的操作可以通过一个线性的数学变换，等效为一个 `MatMul` 加 `BiasAdd` 操作。此举相当于省去了一个 `BatchNorm` 操作（实际包含了 7 个细碎的小算子）。\n\n-   **实现方式**：对 TensorFlow 框架进行了轻量级修改。\n    1.  在模型加载阶段，获取并缓存所有需要的常量张量数据。\n    2.  在后续的图优化阶段，读取缓存数据并完成算子的折叠变换。\n\n### 3. 优化效果\n\n该优化使单个模型的**推理时延降低了约 10%**。",
      "original_script": "然后接下来我将结合具体的一个场景，就是分享几个我们在客户场景中成功落地的两个案例。\n那首先是针对这个embedding层的一个图算融合的优化。\n我们发现embedding层在我们客户的广告模型当中，占比高达30%到40%，这个优化空间还是比较大的。\n然后embedding层的这个特点就是说，它有包含大量的小算子以及频繁的内存操作，它的性能的瓶颈不在于它的计算，而在于它的那个调度和访存。\n然后由于它的那个逻辑结构会比较复杂，所以通用的图优化难度会比较大，那这里的话我们就是采用了一个人工的一个图融合的一个方案。\n我们会基于就是profiling采集的一些信息，去定位到embedding层的embedding层的性能瓶颈的一个部分，然后，再去基于算法原理去拆解出可融合并且高频可复用的一些pattern。\n然后具体的实现上来说的话，主要是分为两个关键的步骤。\n首先是在图层面，我们会对节点进行一个融合，在原生的TF框架中我们去插入一个自定义的图重写模块，让去让图去匹配我们设定的一些融合pattern，并将其改写成一个大的一个融合算子。\n然后第二个的话就是去为每一个融合pattern去手写一个高性能的一个融合算子kernel，通过鲲鹏CPU的一些特殊的向量化指令去优化，去充分发挥我们的鲲鹏CPU的一个硬件性能。\n那这项优化的话也在客户模型当层面去提升了一个5%的推理性能。\n第二个案例是针对这个MatMul加BiasAdd和BatchNorm的一个常量折叠优化。\n我们发现，由于就是客户模型的构图问题，BatchNorm它这个算子被拆分成了多个细粒度的算子，那原生TF它是无法识别到这类就是拆分后的一些细粒度算子的一个整体语义，所以就无法去使能它的一个原生一个融合特性。\n那我们从算，我们从数学原理上可以分析到，就是当矩阵乘的权重、BiasAdd的偏置以及BatchNorm的参数都是常量的时候，这些操作其实可以通过一个数学的线性变换去做一个等效变换，融合后的话就会等价成一个MatMul加BiasAdd的一个操作。\n这个当中就相当于去省掉了一个BatchNorm的一个操作，这个操作的话实际就包含了有七个细碎的小op。\n然后在具体实现过程中的话，我们是对TF做了一个轻量式的一个修改。\n我们需要在去在模型加载阶段去获取到所有的我们需要的张量数据，然后在后续的图优化阶段去读取并完成我们的折叠操作。\n那这个优化的话是最终在单个模型上的推理时延降低了10%左右。",
      "start_ms": 452950,
      "end_ms": 644255,
      "material_ids": [
        "chunk_00/slide_007",
        "chunk_00/slide_008"
      ],
      "summary_hint": "分享了两个在客户场景中成功落地的优化案例：针对搜推场景的Embedding融合算子优化，以及针对BatchNorm层的常量折叠优化。",
      "lead_text": "然后接下来我将结合具体的一个场景，就是分享几个我们在客户场景中成功落地的两个案例。",
      "tail_text": "那这个优化的话是最终在单个模型上的推理时延降低了10%左右。",
      "text_span": {
        "start": 50,
        "end": 66
      }
    },
    {
      "id": "sec_5",
      "index": 4,
      "title": "下一步工作方向",
      "time_range": "00:10:46,140 - 00:12:19,515",
      "summary": "基于客户反馈和落地痛点，我们梳理了下一步的重点工作方向。这包括从手工融合走向自动融合，增强编译器对动态batch的支持，优化算子间的并行执行以发挥多核CPU性能，以及打造面向CPU的自动PTQ量化工具链。",
      "refined_script": "## 下一步工作方向\n\n基于客户反馈与实际落地中的痛点，我们梳理了下一步的重点工作方向，旨在提升编译器性能与易用性。\n\n### 1. 自动图融合\n\n- **现状**：依赖基于经验积累的手工融合策略。\n- **目标**：结合已有的人工调优经验，共建更通用的自动图融合策略。\n- **价值**：提升融合能力的泛化性，灵活覆盖更多场景的融合诉求。\n\n### 2. 增强对动态 Batch 的支持\n\n- **现状**：当前编译器仅支持静态 Shape。\n- **痛点**：\n  - 面对变长输入（动态 Batch）时，会重复触发编译。\n  - 该过程不仅增加了编译耗时，还会生成大量冗余的编译结果。\n- **目标**：提升编译器对动态 Batch 的支持能力。\n- **价值**：更好地适应真实业务场景中的流量波动。\n\n### 3. 优化算子间并行执行\n\n- **现状**：图算融合将多个小算子融合成单个大算子，减少了调度开销，但也降低了 `inter-op` 级别的并行性。\n- **目标**：解决融合带来的并行性下降问题，让融合后的大算子也能在多核 CPU 中并发执行。\n- **价值**：充分发挥多核 CPU 的并发性能。\n\n### 4. 构建自动 PTQ 量化工具链\n\n- **目标**：计划打造一套面向 CPU 的自动 Post-Training Quantization (PTQ) 工具链。\n- **应用场景**：重点服务于高精度的搜索与推荐场景。",
      "original_script": "那基于客户的反馈以及实际落地中的痛点，我们也梳理了下一步的重点的一个工作方向。首先我们是希望能够从手工融合走向一个自动融合，去结合我们之前已经积累的大量的人工调优的一个经验，去共建更通用的一个自动图融合的策略，来提升我们融合的一个泛化能力，去灵活地覆盖到更多场景的一个融合诉求。第二点的话就是去提升我们编译器对动态batch的一个支持能力，来适应更好的适应就是当前真实业务中的一个流量波动。因为我们当，我们我们现在的编译器是只支持静态shape，它在面对变长输入的时候，它会再次进行触发编译，这个过程就会增加我们的编译耗时以及它会生成一些大量冗余的一个编译结果。那第三点的话就是去增强算子算子间并行的一个优化。我们当前的图算融合会把多个小的算子去融合成一个单个算子的一个调用，那这个过程虽然减少了调度的开销，但是也降低了一个inter-op级别的一个并行性。那我们未来将去解决这个问题，去充分地去发挥这个多核CPU的一个并发性能，让我们的融合算子也能在CPU中进行一个并发的一个执行。然后最后在量化方面的话，我们也是计划去打造一套面向CPU的高精度搜推场景的自动PTQ工具链。",
      "start_ms": 646140,
      "end_ms": 739515,
      "material_ids": [
        "chunk_00/slide_009"
      ],
      "summary_hint": "阐述了ANNC未来的重点工作方向，包括从手工融合走向自动融合、支持动态Batch以及并行优化。",
      "lead_text": "那基于客户的反馈以及实际落地中的痛点，我们也梳理了下一步的重点的一个工作方向。",
      "tail_text": "然后最后在量化方面的话，我们也是计划去打造一套面向CPU的高精度搜推场景的自动PTQ工具链。",
      "text_span": {
        "start": 67,
        "end": 74
      }
    },
    {
      "id": "sec_6",
      "index": 5,
      "title": "总结与资源获取",
      "time_range": "00:12:22,360 - 00:13:11,750",
      "summary": "讲者宣布 ANNC 的源代码及文档已在 openEuler 社区开源，欢迎大家下载使用并提出宝贵意见。主持人随后总结了分享内容，强调通过编译器自动完成的融合算子、常量折叠等优化，可以在搜推广告等场景中获得显著的性能提升。",
      "refined_script": "# 总结与资源获取\n\n## 开源资源\n\nANNC 项目的源代码及相关技术文档现已在 openEuler 社区正式开源。\n\n- **获取方式**: 欢迎前往 openEuler 社区下载使用。\n- **社区贡献**: 我们鼓励用户提供宝贵的反馈意见，未来的优化成果也将持续贡献至 openEuler 社区。\n\n## 核心优势总结\n\n通过采用编译技术，ANNC 能够在搜索、推荐、广告等业务场景中带来显著的性能提升。其核心优势在于，所有优化工作均由编译器自动完成，无需人工干预。\n\n关键优化技术包括：\n\n- **融合算子 (Operator Fusion)**\n- **常量折叠 (Constant Folding)**\n\n这些自动化优化最终实现了服务吞吐能力的显著提升。",
      "original_script": "最后的话就是我们ANNC的源代码以及相关的文档已经在openEuler社区开源了，然后欢迎大家去下载使用，也给我们提出一些宝贵的意见，那我们未来后续的一些优化也会继续贡献到openEuler社区。\n以上就是我的分享，谢谢大家。\n好，那个非常感谢郑老师的这个精彩分享。\n就是其实大家可以看到我们在这个搜推广告的技术场景，从编译的方式就可以获得这个不错的性能，这个性能并且还不需要去，那个编译器能够自动完成这些相关的工作。\n那包括刚才提到的那个融合算子、常量折叠的两个相关案例，给我们得到那个吞吐的提升，非常棒的一个分享啊。像，好。",
      "start_ms": 742360,
      "end_ms": 791750,
      "material_ids": [
        "chunk_00/slide_010",
        "chunk_00/slide_011"
      ],
      "summary_hint": "总结分享内容，并提供了ANNC在openEuler社区的开源代码和文档资源。",
      "lead_text": "最后的话就是我们ANNC的源代码以及相关的文档已经在openEuler社区开源了，然后欢迎大家去下载使用，也给我们提出一些宝贵的意见，那我们未来后续的一些优化也会继续贡献到openEuler社区。",
      "tail_text": "那包括刚才提到的那个融合算子、常量折叠的两个相关案例，给我们得到那个吞吐的提升，非常棒的一个分享啊。像，好。",
      "text_span": {
        "start": 75,
        "end": 79
      }
    }
  ]
}