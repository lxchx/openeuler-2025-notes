{
  "id": "sec_3",
  "index": 2,
  "title": "ANNC 整体架构与关键技术",
  "time_range": "00:02:36,390 - 00:07:30,745",
  "script": "基于这样的一个背景，我们就构建了一套面向CPU的神经网络推理加速编译器ANNC。\n然后可以看一下，左边就是我们ANNC的一个整体架构。\n然后我们是ANNC是对上向上去对接TensorFlow、PyTorch等一系列的开源框架，向下是亲和我们的鲲鹏硬件。\n那我们的编译器，它的优化是覆盖了框架层和编译层，实现了一个端到端的一个优化的闭环。\n我们在框架层的话主要是一些包括自定义和通用的一些图算融合优化。\n这个主要是通过算法等价的方式去将多个独立的晓算子去融合成一个大的算子，来减少计算的开销以及内存的占用。\n那在编译部分的前端是主要是一些与硬件无关的一些图优化，包括冗余算子的一个消除、CPU感知的图优化以及多内核搜索等等策略。\n然后后端主要是一些算子层面的与硬件亲和的一些优化，包括像自动生成算子、代码的代码的指令级别的优化，以及我们也可以支持对接一些开源的成熟的一些算子库。\n那在实际应用中的话，我们的ANNC在多个开源的推荐模型下获得了20%的一个性能提升。\n然后在某个客户推荐的一个模型下，能够获得一个25%的性能提升。\n然后具体来介绍一下我们ANNC的几个关键的一些优化特性。\n首先是针对这个矩阵乘算子的数据布局优化。\n那在传统CPU在处理这个矩阵乘算子的时候，传统的做法是需要在运行时对输入数据进行一个重排打包。\n这个目的就是为了让数据去适配CPU的它的一个缓存大小以及它的一个内存访问的模式。\n那这个过程，这个动态的过程就会带来一定的运行时性能开销。\n那我们的优化就是将这个过程去前移到编译期。\n在对于就是常量输入，就是类似矩阵的权重，这类常量输入，我们直接在编译期对它进行一个静态的一个数据打包。\n然后同时在对多个连续的一个矩阵乘算子来说，我们会在它们之间进行一个最优的布局传递。\n就像右边这个图所示，比如说有这样子ABC三个矩阵乘算子，像B的输出就是C的输入，那我们就可以在B的输出写入内存的时候，直接就按照C的最佳的输入布局去进行一个写入。\n那这样子的话，我们的运行时就无需进行任何的一些动态重排操作。\n这样子所有的数据已经以最适合当前CPU硬件的布局准备好了，这样子就不需要再去进行一个重排操作，这样子就显著地降低了一个访存开销。\n那我们这个优化的话，是在开源推荐模型当中提升了5%的一个吞吐率。\n然后这是一个是面向利用了CPU的一些cache line、指令集等特征，去设计的一个自适应的算子优化和快速生成的一个技术。\n然后我们是主要是对MatMul等关键算子进行了一个模板化抽象，通过一些高级别的一个模板编译，去快速生成适配当前硬件的不同形状的一个最优算子。\n然后我们这个方案的话主要是有三大特点。\n首先是硬件感知优化。\n我们的编译器后端可以感知到CPU的一些硬件信息，去自动生成当前对算子生成的一个代码进行一个针对性的优化。\n然后第二个是中端的一个智能的一个优化，我们是基于MLIR去完成了一个对像tiling、packing、buffer复用等的一些数据流与控制流的一些优化。\n第三个是通过模板化设计的话，我们可以快速地修改、调整算子的各种参数，来生成一个最佳的一个融合算子。\n那实测表明的话，我们生成的算子对OpenBLAS可以获得一个9%到123%的一个性能收益。\n并且由于我们这个是模板化的一个算子生成，可以大幅度地提高一个调试以及快速迭代的一个效率。\n那未来我们这个相关的些pass也会在openEuler社区开源，然后下一步的话我们也是计划去结合Triton去提供一个更强的算子生成能力。",
  "summary": "本节介绍了面向CPU的神经网络推理加速编译器ANNC的整体架构，其优化覆盖框架层和编译层，实现了端到端的优化闭环。接着，详细阐述了两大关键技术：一是针对矩阵乘算子的数据布局优化，通过编译期静态打包和最优布局传递，显著降低访存开销；二是一种自适应的算子优化和快速生成技术，通过硬件感知的模板化设计，可生成对特定硬件和形状最优的高性能算子。"
}