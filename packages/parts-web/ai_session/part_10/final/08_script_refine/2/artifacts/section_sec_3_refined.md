# ANNC 整体架构与关键技术

ANNC 是一款面向 CPU 的神经网络推理加速编译器，向上对接 TensorFlow、PyTorch 等主流开源框架，向下亲和鲲鹏硬件。其优化覆盖框架层与编译层，实现了端到端的优化闭环。

在实际应用中，ANNC 在多个开源推荐模型上获得了 20% 的性能提升，在某客户的推荐模型下获得了 25% 的性能提升。

## 一、整体架构

ANNC 的优化贯穿框架层与编译层，各层级的主要工作如下：

- **框架层**：进行图算融合优化，通过算法等价的方式将多个独立的小算子融合成一个大算子，以减少计算开销和内存占用。
- **编译层前端**：执行与硬件无关的图优化，包括：
  - 冗余算子消除
  - CPU 感知的图优化
  - 多内核搜索策略
- **编译层后端**：进行与硬件亲和的算子级优化，包括：
  - 自动生成高性能算子
  - 指令级代码优化
  - 支持对接成熟的开源算子库

## 二、关键技术

### 1. 矩阵乘算子的数据布局优化

传统的矩阵乘（MatMul）算子需要在运行时对输入数据进行重排（Repack），以适配 CPU 的缓存大小和内存访问模式，但这会引入额外的性能开销。

ANNC 的优化方案是将数据重排前移到编译期，从而消除运行时开销：

- **编译期静态打包**：对于权重等常量输入，在编译阶段直接完成静态数据打包。
- **最优布局传递**：对于连续的多个矩阵乘算子（例如 A->B->C），在前一个算子（B）的输出直接按下一个算子（C）的最佳输入布局写入内存。 

通过该优化，运行时无需进行任何动态重排操作，数据已为硬件准备就绪，显著降低了访存开销。此项优化在开源推荐模型中提升了 5% 的吞吐率。

### 2. 自适应的算子优化与快速生成

该技术利用 CPU 的 Cache Line、指令集等硬件特征，设计了一套自适应的算子优化与快速生成方案。我们对 MatMul 等关键算子进行模板化抽象，通过高级别的模板编译，快速生成适配特定硬件与不同形状（Shape）的最优算子。

该方案具备三大特点：

1.  **硬件感知优化**：编译器后端能够感知 CPU 硬件信息，为生成的算子代码进行针对性优化。
2.  **智能中端优化**：基于 MLIR 完成数据流与控制流的优化，如 Tiling、Packing、Buffer 复用等。
3.  **模板化设计**：可快速修改和调整算子参数，以生成性能最佳的融合算子。

实测表明，ANNC 生成的算子相比 OpenBLAS 可获得 9% 至 123% 的性能收益。同时，模板化的生成方式也大幅提高了调试和快速迭代的效率。

## 三、未来规划

- **社区开源**：计划在 openEuler 社区开源相关的优化 Pass。
- **能力增强**：计划结合 Triton，提供更强的算子生成能力。