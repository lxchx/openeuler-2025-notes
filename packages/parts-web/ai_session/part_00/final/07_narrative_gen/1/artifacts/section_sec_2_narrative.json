{
  "id": "sec_2",
  "index": 1,
  "title": "AI Agent时代背景与OS维护挑战",
  "time_range": "00:02:22,500 - 00:08:43,659",
  "script": "各位来宾,各位老师,大家下午好。\n非常荣幸能有机会到这里, 与大家分享我们团队在操作系统与 AI 结合领域的一些关键实践和阶段性成果。\n首先, 我们回顾一下, 在2024年, 可以称为是 AI Agent 的应用元年。\n包括 MCP 协议, 其实给我们带来了非常多的改变, 比如可以将一些多任务、复杂性程度比较高、不确定性程度比较高的一些长链条的处理任务, 给它拆解成不同的专家角色, 让它们能够实现人机协同的方式, 帮我们完成工作。\n那么在这个背景之下, openEuler intelligence 团队与我们做了一些联创, 把它这一套基础设施通过集成到我们产品开发的过程中去, 然后把我们的整个的基础设施实现了一个打通。\n提供了一些 MCP 的关键的服务节点, 以及一些 Agent 的对接的方式, 然后我们的开发的流程也比较顺畅地往下跑, 包括集成到我们公司的自研的一些 Codelet 之类的开发工具里面去。\n接下来我给大家做一些简单介绍。\n在我们的 OS 团队中, 其实有几个非常大的命题, 就是关于操作系统维护的。\n首先第一个就是我们的操作系统需要非常大量地跟社区进行紧密配合, 其中一个点就是我们的 Patch Merging。\nPatch Merging 就是我们需要频繁地从社区合入一些补丁或者 Bug Fix。每次的合入规模可能是千级别, 甚至是如果我们大批量组件合入的时候, 都是成千上万的补丁分析的规模。\n每次分析, 工程师的耗时每千条补丁可能会耗时80+到100+小时, 也就是我们投入一个资深的内核开发工程师以及对应的组件开发工程师, 每个组件它的分析工作量大概就是两周以上。\n这只是单纯做分析, 还没有说代码的合入。\n然后在我们合入代码的过程中, 还会出现非常多的代码冲突报错, 这个时候就需要我们人为地介入, 做一个代码补丁的分析和合入, 这个地方可能占比在10%以上。\n最后就是还有一个关键的点, 就是我们给客户提供操作系统的时候, 还需要给客户承诺一个高危漏洞的响应周期, 就是在14天以内可能要完成中高危漏洞的修复, 并且推送到客户端。\n这是一个典型的 Patch Merging 的一个场景。\n然后第二个场景的话, 就是我们在合入代码过程中还需要做一些集成测试。\n集成测试也是一样的, 就是我们可能根据社区的合入建议, 我们需要自己自身去做一个分析和测试合入。\n第一个就是分析的量级跟补丁的规模其实是比较对应的, 那我们可能会做一些融合, 比如说按模块去区分, 但是其实并没有一个明显的降低, 也是数千级别的一次每次的合入。\n第二块就是对测试能力的要求, 其实相对来说是比较高的, 因为我们内核代码的合入或者说一些关键中间件的合入, 它需要对我们的系统测试非常非常精通, 可能是一个比较高、比较精通这里的一个测试专家。\n最终的覆盖效果, 其实我们认为, 我们从我们的实际的产出来看, 如果社区测不出来这个问题, 那我们其实自己去测, 即使是我们做了非常多的 Patch Merging 之后的一个合入的话, 它的那个测试效果其实也是聊胜于无的, 可以认为就是测试的效果并不是非常的好, 因为因为它比较依赖于我们那个测试的专家以及他的经验。\n好。\n那第三个场景的话, 就是在我们的开发运维的一个过程当中, 可能需要对我们非常多的设备去做一个实时的监控, 然后当它出现一些问题的时候, 我们需要把这些问题上报到我们的巡检平台, 或者是进行一个 Bug 的记录。\n那这个地方我们内部的操作系统团队这边维护了上百套的自动化的设备, 包括那个虚拟化部署的、物理机部署的, 以及运行到一些可能小、小规模的一些嵌入式设备之类的, 这里面搭载了上百套的一个设备在上面去。那么我们的运维人员每天就需要对这个设备进行一个定期的检查和巡检。\n如果说它出现问题的话, 那就需要记录, 然后找开发工程师去做一个分析。\n那这里的那个设备数量的话就是上百套, 然后但是出现问题的时候, 那对于这个测试专家来说, 或者说这个运维专家来说, 他需要对这些信息做初步的分析和 Bug 的上报, 这个 Bug 上报过程就会需要很多的信息的记录。那其实我们发现, 在这个他们提交的缺陷里面, 然后我们去回溯它的是否有效的一个情况来看的话, 可能有15%以上都是无效的 Bug, 或者是信息不全的一些 Bug。\n那这样的话对于我们的操作系统团队去维护这些 Bug, 或者说修复这些 Bug 来说, 它其实并没有起到正向的作用, 反而产生了非常多的无效的工作量。\n然后呢, 重复问题的占比也是非常非常高的, 就是因为非常多的设备, 它可能具备一定的相似性, 或者说它很多问题会在不同的设备上面出现。那出现这样问题的时候呢, 我们也做了一个分析, 大概是30%以上的问题都是重复性的, 也就是不是第一次首次提交的一些问题, 这里也会消耗非常多的工作量。\n每天的重复工作几乎是接近100%, 因为这个同学他天天就需要去对设备去做巡检, 然后呢去做 Bug 的上报, 去做技术分析。\n然后最后一个场景的话, 就是我们对于一些维护内核的同事可能比较熟悉的, 就是对那个 Vmcore 做一个分析, 就是去查一些内核的 Bug, 然后修复内核的 Bug。这里要求的话是非常高的, 就是对这个技术人员的要求是非常高的, 它是一个需要一个资深的内核的研发, 并且要熟悉各个内核的子系统。\n否则的话, 就是每个 Bug 就需要拉上一个整个团队的不同方向的, 比如文件存储啊, 然后呢 IO 之类的工程师去一起分析, 所以对这个专业性要求是非常高的。\n然后解决耗时的话其实也是比较高的, 因为我们经过大量测试之后发布的一个问题的话, 它相对来说比较少, 然后在这个情况出现 Bug 的话, 它的概率相对来说比较低, 然后难度就会比较高。\n修复的周期的话都会相对很长。\n但这个过程的自动化占比其实是非常非常低的, 在我们落地之前其实非常非常低的, 可以说全靠人工的经验和现有的一些 GCC、Vmcore 分析的一些工具。\n我们怎么解决的呢?",
  "summary": "在AI Agent的应用元年背景下，讲者指出了操作系统（OS）维护面临的四大核心挑战。这些挑战包括：需要耗费大量人力的补丁合入（Patch Merging）与集成测试；设备日常巡检与运维中存在大量无效和重复的Bug上报；以及对专家经验依赖极高、自动化程度低的内核崩溃（Vmcore）分析。"
}