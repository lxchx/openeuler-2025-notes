{
  "graph_data": {
    "nodes": [
      {
        "id": "p1",
        "label": "研究背景与存储困境",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_001",
            "sec_002",
            "sec_003"
          ],
          "material_ids": [
            "chunk_00/slide_001",
            "chunk_00/slide_002",
            "chunk_00/slide_003",
            "chunk_00/slide_004",
            "chunk_00/slide_005",
            "chunk_00/slide_006",
            "chunk_00/slide_007",
            "chunk_00/slide_008",
            "chunk_00/slide_009",
            "chunk_00/slide_010",
            "chunk_00/slide_011"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_006",
            "primary_time_ts": "00:04:25,300",
            "primary_time_ms": 265300
          }
        },
        "parent": null,
        "summary": "介绍推荐大模型在现代数据中心的核心地位，分析其独特的嵌入层结构，并指出当前面临的容量墙与带宽墙挑战，以及多层级存储硬件的演进趋势。"
      },
      {
        "id": "p2",
        "label": "多层级存储优化技术",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_004",
            "sec_005",
            "sec_006",
            "sec_003"
          ],
          "material_ids": [
            "chunk_00/slide_012",
            "chunk_00/slide_013",
            "chunk_00/slide_014",
            "chunk_00/slide_015",
            "chunk_00/slide_016",
            "chunk_00/slide_017",
            "chunk_00/slide_018",
            "chunk_00/slide_019",
            "chunk_00/slide_020",
            "chunk_00/slide_021",
            "chunk_00/slide_022",
            "chunk_00/slide_023",
            "chunk_00/slide_024",
            "chunk_00/slide_025",
            "chunk_00/slide_026",
            "chunk_00/slide_027",
            "chunk_00/slide_028",
            "chunk_00/slide_029",
            "chunk_00/slide_030",
            "chunk_00/slide_031",
            "chunk_00/slide_032",
            "chunk_00/slide_033",
            "chunk_00/slide_034",
            "chunk_00/slide_035",
            "chunk_00/slide_036",
            "chunk_00/slide_037",
            "chunk_00/slide_038"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_012",
            "primary_time_ts": "00:08:14,300",
            "primary_time_ms": 494300
          }
        },
        "parent": null,
        "summary": "针对 GPU、PM 和 SSD 三个存储层级的瓶颈，分别提出了 Fleche 查询融合、PetPS 网卡聚集和 MaxEmbed 选择性副本三项核心技术。"
      },
      {
        "id": "p3",
        "label": "系统落地与未来展望",
        "category": "Pillar",
        "references": {
          "section_ids": [
            "sec_007",
            "sec_008"
          ],
          "material_ids": [
            "chunk_00/slide_039",
            "chunk_00/slide_040",
            "chunk_00/slide_041",
            "chunk_00/slide_042"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_039",
            "primary_time_ts": "00:19:58,300",
            "primary_time_ms": 1198300
          }
        },
        "parent": null,
        "summary": "整合优化成果构建 RecStore 系统，分享在快手等企业的落地成效，并探讨生成式推荐模型（GR）带来的新挑战及后 Optane 时代的硬件替代方案。"
      },
      {
        "id": "d1_1",
        "label": "嵌入层 (Embedding Layer) 瓶颈",
        "category": "Problem",
        "references": {
          "section_ids": [
            "sec_002"
          ],
          "material_ids": [
            "chunk_00/slide_005",
            "chunk_00/slide_006"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_006",
            "primary_time_ts": "00:04:25,300",
            "primary_time_ms": 265300
          }
        },
        "parent": "p1",
        "summary": "推荐模型中嵌入层参数量巨大且涉及海量随机访存，导致严重的容量墙（12万亿级参数）与带宽墙（占运行时间70%以上）问题。"
      },
      {
        "id": "d1_2",
        "label": "多层级存储硬件趋势",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_003"
          ],
          "material_ids": [
            "chunk_00/slide_008",
            "chunk_00/slide_009",
            "chunk_00/slide_010",
            "chunk_00/slide_011"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_008",
            "primary_time_ts": "00:05:36,300",
            "primary_time_ms": 336300
          }
        },
        "parent": "p1",
        "summary": "分析了 HBM（高性能层）、PM（容量层）和 SSD（存储层）的特性，指出软件在利用这些异构介质时存在交互效率低、延迟高及读放大等核心科学问题。"
      },
      {
        "id": "d2_1",
        "label": "Fleche: GPU 查询融合",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_004"
          ],
          "material_ids": [
            "chunk_00/slide_017",
            "chunk_00/slide_018"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_017",
            "primary_time_ts": "00:09:53,300",
            "primary_time_ms": 593300
          }
        },
        "parent": "p2",
        "summary": "针对 GPU 软硬交互开销大的问题，提出自识别 Kernel 融合方法，通过单体大 Kernel 处理海量动态形状的小查询任务。"
      },
      {
        "id": "d2_2",
        "label": "PetPS: PM 网卡聚集机制",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_005"
          ],
          "material_ids": [
            "chunk_00/slide_024",
            "chunk_00/slide_025"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_024",
            "primary_time_ts": "00:12:57,300",
            "primary_time_ms": 777300
          }
        },
        "parent": "p2",
        "summary": "利用网卡的 SG-DMA 功能代替 CPU 完成参数序列化，规避了持久性内存（PM）高延迟导致的 CPU 瓶颈，实现协同访存。"
      },
      {
        "id": "d2_3",
        "label": "MaxEmbed: SSD 选择性副本",
        "category": "Technology",
        "references": {
          "section_ids": [
            "sec_006"
          ],
          "material_ids": [
            "chunk_00/slide_035",
            "chunk_00/slide_036"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_035",
            "primary_time_ts": "00:18:02,300",
            "primary_time_ms": 1082300
          }
        },
        "parent": "p2",
        "summary": "针对 SSD 读放大问题，通过为热点参数创建选择性副本，以空间换带宽，提升了 SSD 在处理稀疏参数读取时的硬件利用率。"
      },
      {
        "id": "d3_1",
        "label": "RecStore 系统落地成效",
        "category": "Result",
        "references": {
          "section_ids": [
            "sec_007"
          ],
          "material_ids": [
            "chunk_00/slide_039",
            "chunk_00/slide_040"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_040",
            "primary_time_ts": "00:20:13,300",
            "primary_time_ms": 1213300
          }
        },
        "parent": "p3",
        "summary": "RecStore 在快手落地，节省约一亿元成本，稳定服务亿级用户，并获得 Intel 等厂商的认可。"
      },
      {
        "id": "d3_2",
        "label": "生成式推荐 (GR) 展望",
        "category": "Concept",
        "references": {
          "section_ids": [
            "sec_008",
            "sec_007"
          ],
          "material_ids": [
            "chunk_00/slide_041"
          ],
          "highlight": {
            "primary_material_id": "chunk_00/slide_041",
            "primary_time_ts": "00:20:29,300",
            "primary_time_ms": 1229300
          }
        },
        "parent": "p3",
        "summary": "探讨推荐模型从 DLRM 向 GR 的范式转移，分析其“大稀疏+大稠密”的特征，并提出“以存代算”的优化思路。"
      }
    ],
    "edges": [
      {
        "source": "p1",
        "target": "p2",
        "relation_type": "sequential",
        "label": "针对挑战提出方案"
      },
      {
        "source": "p2",
        "target": "p3",
        "relation_type": "sequential",
        "label": "方案整合与落地"
      },
      {
        "source": "d1_1",
        "target": "d1_2",
        "relation_type": "causal",
        "label": "驱动硬件架构演进"
      },
      {
        "source": "d2_1",
        "target": "d2_2",
        "relation_type": "parallel",
        "label": "并列技术方案"
      },
      {
        "source": "d2_2",
        "target": "d2_3",
        "relation_type": "parallel",
        "label": "并列技术方案"
      },
      {
        "source": "d2_1",
        "target": "d3_1",
        "relation_type": "compositional",
        "label": "整合进系统"
      },
      {
        "source": "d2_2",
        "target": "d3_1",
        "relation_type": "compositional",
        "label": "整合进系统"
      },
      {
        "source": "d2_3",
        "target": "d3_1",
        "relation_type": "compositional",
        "label": "整合进系统"
      },
      {
        "source": "d3_1",
        "target": "d3_2",
        "relation_type": "sequential",
        "label": "面向未来演进"
      }
    ]
  },
  "sections": [
    {
      "id": "sec_001",
      "index": 0,
      "title": "报告开场与讲者介绍个人背景及研究方向介绍，包括面向 AI 的存储（Storage for AI）领域的工作总览。介绍报告题目《面向推荐大模型的参数存储系统研究》及讲者谢明晖老师的学术背景与研究方向。",
      "time_range": "00:00:09,360 - 00:01:26,765",
      "summary": "",
      "refined_script": "# 报告概览：面向推荐大模型的参数存储系统研究\n\n## 1. 讲者简介\n**谢明晖**，现任职于中国人民大学，专注于机器学习系统与存储系统的交叉研究。\n\n*   **教育背景**：\n    *   **博士**：清华大学计算机系（师从陆游游教授与舒继武教授）。\n    *   **本科**：南京大学。\n*   **学术荣誉**：\n    *   ACM 中国优秀博士学位论文奖（ACM China Doctoral Dissertation Award）。\n    *   入选北京市青年人才托举工程（青托计划）。\n*   **科研实践**：拥有丰富的学术积累与企业项目合作经验。\n\n## 2. 核心研究方向：面向 AI 的存储 (Storage for AI)\n研究工作主要围绕 **机器学习系统 (MLSys)** 展开，深耕于存储系统与机器学习的交叉领域，即 **Storage for AI**。该方向旨在通过重构底层存储架构，解决大规模 AI 模型在训练与推理过程中的数据吞吐与参数管理瓶颈。\n\n## 3. 报告主题：推荐大模型参数存储系统\n本次技术分享的主题为**《面向推荐大模型的参数存储系统研究》**。\n\n*   **行业背景**：随着推荐模型规模的爆炸式增长，海量参数的存储与高效访问已成为当前人工智能领域的关键技术难题。\n*   **核心议题**：报告将深入探讨如何针对推荐大模型的特性，设计并优化参数存储系统，以应对高并发、低延迟及大规模扩展性的挑战。",
      "original_script": "接下来，让我们有请中国人民大学的谢明晖老师，带来《面向推荐大模型的参数存储系统研究》的分享。\n推荐大模型的参数存储是当前人工智能领域的关键难题，期待谢老师的分享能为我们解开疑惑。掌声欢迎！\n谢谢主持人的介绍，也谢谢各位领导和老师。\n我是来自中国人民大学的谢明晖，本次汇报的题目是《面向推荐大模型的参数存储系统研究》。\n首先对我个人情况做个简单的介绍。\n我本科就读于南京大学，去年在清华大学计算机系陆游游老师和舒继武老师指导下获得博士学位，毕业后加入中国人民大学。\n曾获得ACM优博奖、北京市青托计划等荣誉，并承担了一系列企业项目。\n我的研究方向主要围绕机器学习系统，具体涉及存储系统与机器学习系统的交叉领域，即面向 AI 的存储（Storage for AI）。",
      "start_ms": 9360,
      "end_ms": 86765,
      "material_ids": [
        "chunk_00/slide_001",
        "chunk_00/slide_002"
      ],
      "notes": "校准至字幕第2句开始，第9句结束。",
      "lead_text": "接下来，让我们有请中国人民大学的谢明晖老师，带来《面向推荐大模型的参数存储系统研究》的分享。",
      "tail_text": "我的研究方向主要围绕机器学习系统，具体涉及存储系统与机器学习系统的交叉领域，即面向 AI 的存储（Storage for AI）。",
      "text_span": {
        "start": 1,
        "end": 8
      }
    },
    {
      "id": "sec_002",
      "index": 1,
      "title": "研究背景：推荐模型的结构与存储困境阐述推荐模型中嵌入层（Embedding Layer）的重要性，以及当前面临的容量墙和带宽墙挑战。阐述推荐模型中嵌入层（Embedding Layer）的重要性，以及当前面临的容量墙和带宽墙挑战。",
      "time_range": "00:01:27,780 - 00:05:35,435",
      "summary": "",
      "refined_script": "# 研究背景：推荐模型的结构演进与存储困境\n\n## 1. AI 存储的三层架构体系\n在当前人工智能领域，存储系统根据其承载的功能可分为三个核心层次：\n- **参数存储 (Parameter Storage)**：模型底层权重的存储，是推荐模型等大规模系统的基石。\n- **向量存储 (Vector Storage)**：将现实世界实体转化为高维向量后的索引与检索层。\n- **记忆存储 (Memory Storage)**：赋予智能体（Agent）长期记忆能力的关键，涵盖 RAG（检索增强生成）或参数化记忆方案，是区分简单聊天机器人与智能体的核心指标。\n\n本文重点探讨**推荐模型中的参数存储**问题。\n\n## 2. 推荐模型在现代数据中心的核心地位\n随着互联网 UGC（用户生成内容）爆发式增长，推荐模型已成为筛选海量信息的关键基础设施。其重要性体现在：\n- **业务占比高**：以 Facebook 为例，推荐模型在 AI 集群服务耗时中的占比已高达 **80%**。\n- **算力载体**：推荐模型已成为现代数据中心 AI 服务的主要负载类型。\n\n## 3. 推荐模型结构：嵌入层 (Embedding Layer) 的作用\n与传统的 CV（计算机视觉）或 NLP（自然语言处理）模型不同，推荐模型具有独特的结构特征：\n\n### 3.1 输入特征：高维稀疏 ID\n推荐模型的输入通常是城市、用户、视频等离散的 ID 信息。由于深度神经网络（DNN）无法直接处理这些高维稀疏数据，必须通过嵌入表进行转化。\n\n### 3.2 核心组件：嵌入表 (Embedding Table)\n- **功能**：将高维稀疏 ID 映射为低维稠密向量（Dense Vector）。\n- **运行机制**：例如，根据用户观看的视频 ID，从嵌入表中检索对应的向量行，随后输入神经网络进行计算。\n- **访存特性**：嵌入层的操作本质上是**访存密集型 (Memory-intensive)**，而非计算密集型。\n\n## 4. 推荐模型面临的“两堵墙”挑战\n\n### 4.1 容量墙 (Capacity Wall)\n- **规模激增**：由于特征交叉等技术的应用，嵌入层参数量已达到 **12 万亿级**。\n- **精度耦合**：工业界实践证明，模型精度的提升高度依赖于参数规模的同步增长，这对存储容量提出了极高要求。\n\n### 4.2 带宽墙 (Bandwidth Wall)\n- **性能瓶颈**：嵌入层涉及海量的随机访存操作。根据阿里巴巴的统计数据，嵌入操作占端到端模型运行时间的 **70% 以上**。\n- **算力受限**：访存延迟严重制约了计算单元（如 GPU/NPU）的效能发挥。\n\n## 5. 工业界现状与成本压力\n受限于容量与带宽的双重压力，目前的解决方案往往依赖于大规模硬件堆叠：\n- **部署规模**：快手等头部企业需部署**万台规模**的内存参数服务器仅用于存储模型。\n- **经济成本**：此类集群的机器成本已达到**十亿元人民币**级别。\n\n**结论**：存储系统已取代计算单元，成为制约现代推荐模型演进与落地的核心瓶颈。",
      "original_script": "我们目前的工作主要围绕参数存储、向量存储和记忆存储三个层次展开。\n随着人工智能模型规模的扩大，模型“脑容量”增加，对应的参数量也随之激增。\n其次，AI 模型观察万物的方式是将世界万物表达为向量，因此向量存储也是我们需要考虑的关键组件。\n第三块是记忆存储。区分聊天机器人与真正智能体的关键在于智能体是否拥有记忆。\n如果没有记忆，它只是一个简单的 Chatbot。因此，我们也面向特定场景研究大语言模型（LLM）的记忆方案，包括 RAG 或参数化记忆方案。\n回到本次演讲的主题，我们主要针对推荐模型的参数存储，即刚才提到的最底层部分。\n当前互联网高度发达，每天产生海量的视频、推文等 UGC 内容。面对海量信息，用户无法全部处理，因此需要推荐模型负责筛选并呈现。\n在 Facebook 的 AI 集群中，推荐模型在 AI 服务耗时中的占比已达到 80%。因此，推荐模型已成为现代数据中心 AI 服务的主要载体。\n具体看推荐模型的结构，它与传统的 CV 或 NLP 模型不同，拥有一个非常庞大的嵌入层（Embedding Layer）。\n其中包含许多嵌入表，如城市、用户、视频的嵌入表。\n推荐模型的输入不是像 ResNet 那样的稠密图像，而是高维稀疏的 ID。\n由于 DNN 难以直接处理高维稀疏 ID，因此使用嵌入表将其转化为低维稠密向量。\n例如，根据用户最近观看的三个视频 ID，从表中查出对应的三行向量，再喂给神经网络。这部分稀疏参数访问是典型的访存密集型而非计算密集型。\n当前推荐模型面临两方面的存储困境。第一是容量墙，参数量巨大。\n由于 ID 交叉等技术，嵌入层参数量可达 12 万亿级。工业实践证明，模型精度提升需要参数量的同步提升。\n第二是带宽墙。嵌入层涉及大量访存操作。根据阿里的统计，嵌入操作占端到端模型时间的 70% 以上。\n这严重限制了算力的发挥。受这两堵墙叠加影响，快手目前部署了万台内存参数服务器，仅用于存储模型。\n其机器成本极高，达到了十亿元级别。\n因此，存储系统已成为制约推荐模型发展的核心因素。",
      "start_ms": 87780,
      "end_ms": 335435,
      "material_ids": [
        "chunk_00/slide_003",
        "chunk_00/slide_004",
        "chunk_00/slide_005",
        "chunk_00/slide_006",
        "chunk_00/slide_007"
      ],
      "notes": "校准至字幕第10句开始，第28句结束。",
      "lead_text": "我们目前的工作主要围绕参数存储、向量存储和记忆存储三个层次展开。",
      "tail_text": "因此，存储系统已成为制约推荐模型发展的核心因素。",
      "text_span": {
        "start": 9,
        "end": 27
      }
    },
    {
      "id": "sec_003",
      "index": 2,
      "title": "硬件趋势与核心科学问题分析多层级存储（HBM, PM, SSD）的特性，指出软件在利用这些硬件时存在的交互效率低、延迟高及读放大等问题。分析多层级存储（HBM, PM, SSD）的特性，指出软件在利用这些硬件时存在的交互效率低、延迟高及读放大等问题。",
      "time_range": "00:05:37,700 - 00:08:28,745",
      "summary": "",
      "refined_script": "# 多层级存储系统的硬件趋势与核心挑战分析\n\n## 1. 背景与研究目标\n随着硬件技术的演进，存储架构正向多层级化发展。研究的核心思路在于**混合利用多层次存储介质（HBM、PM、SSD）构建存储系统**，通过异构介质的特性实现容量与带宽的互补，以应对日益增长的数据处理需求。\n\n## 2. 多层级存储架构概览\n\n| 存储层级 | 硬件介质 | 定位 | 核心优势 | 访问粒度 |\n| :--- | :--- | :--- | :--- | :--- |\n| **性能层** | HBM (High Bandwidth Memory) | 缓存/加速 | 极高带宽，缓解“带宽墙” | 字节级 |\n| **容量层** | PM (Persistent Memory) | 内存扩容 | 成本低于 DRAM，具备持久化特性 | 字节级 (Load/Store) |\n| **存储层** | SSD (Solid State Drive) | 大规模存储 | 低成本、大容量 | 块级 (4KB Page) |\n\n## 3. 核心技术挑战与瓶颈分析\n\n尽管多层级存储提供了理想的理论性能，但在实际软件利用过程中，不同硬件特性的巨大差异带来了三大核心挑战：\n\n### 3.1 GPU HBM 层：软硬交互效率低下\n*   **现象**：在利用 HBM 作为缓存进行数据查询时，系统开销并非主要来自数据传输，而是来自控制面。\n*   **瓶颈**：CPU 启动 Kernel 调度 GPU 执行的开销巨大。实验数据显示，**CPU 调用 GPU 的指令开销占比可高达 70%**，严重掩盖了 HBM 的高带宽优势。\n\n### 3.2 DRAM/PM 层：介质延迟导致流水线停顿\n*   **现象**：PM 虽然支持 Load/Store 指令访问，但其延迟远高于 DRAM。\n*   **瓶颈**：PM 的访问延迟约为 DRAM 的 3 倍。在参数服务器场景下，直接将 DRAM 替换为 PM 会导致 **CPU 流水线频繁停顿（Stall）**，造成**系统吞吐量下降 1.9 倍**，抵消了硬件成本带来的收益。\n\n### 3.3 SSD 层：访问粒度不匹配导致的读放大\n*   **现象**：内存级介质（HBM/DRAM/PM）支持字节级寻址，而 SSD 仅支持块级寻址。\n*   **瓶颈**：在处理小参数访问（如 64B 至 512B）时，SSD 必须以 4KB Page 为最小单位进行读取。这种粒度不匹配导致了严重的**读放大**现象。实验表明，即使是高性能 SSD，在随机小对象访问场景下的**实际有效带宽仅为 6% 左右**。\n\n## 4. 研究成果与学术产出\n针对上述三个层级的瓶颈，本研究团队开展了系统性工作，相关成果已发表于计算机系统顶级会议：\n\n*   **GPU 层优化 (Fleche)**：针对软硬交互开销大的问题，提出了高效的调度与交互机制（发表于 **EuroSys**）。\n*   **内存与存储层优化**：针对 PM 延迟与 SSD 读放大问题的研究成果已发表于 **VLDB** 和 **ASPLOS** 等会议。",
      "original_script": "幸运的是，随着硬件发展，出现了不同层次的存储。在性能层，有集成在 GPU 或新一代 CPU 中的 HBM。\n我们可以利用 HBM 做缓存来缓解带宽墙。在容量层，有持久性内存（PM），它成本更低，可用于内存扩容。\n我们研究的核心思路是混合利用多层次内存构建存储系统，实现容量和带宽的互补。\n但实现这一目标极具挑战，主要问题在于不同层次硬件的特性差异巨大。\n例如，在 GPU HBM 层，软硬交互效率较低。使用 HBM 做缓存查询时，需要 CPU 启动 Kernel 让 GPU 执行，而 CPU 调用 GPU 的开销占比可达 70%。\n在 DRAM 和 PM 层，介质性能难以发挥。PM 延迟较高，且属于 Load/Store 介质。\n如果 PM 延迟是 DRAM 的三倍，会严重卡住 CPU 流水线。将参数服务器的 DRAM 换成 PM 后，吞吐量下降了 1.9 倍，抵消了硬件成本优势。\n在 SSD 层，参数读放大现象严重。内存介质是字节粒度寻址，而 SSD 是块寻址。\n即使只访问 64B 到 512B 的参数，也必须读取整个 4KB 的 Page。这种粒度不匹配导致 SSD 读放大严重。\n即使是高性能 SSD，其实际有效带宽也仅有 6% 左右。\n针对这三个层次的问题，我们分别开展了工作，成果发表在 EuroSys、VLDB 和 ASPLOS 等会议上。\n首先是 GPU 层的工作 Fleche，解决软硬交互开销大的问题。",
      "start_ms": 337700,
      "end_ms": 508745,
      "material_ids": [
        "chunk_00/slide_008",
        "chunk_00/slide_009",
        "chunk_00/slide_010",
        "chunk_00/slide_011",
        "chunk_00/slide_012",
        "chunk_00/slide_013"
      ],
      "notes": "校准至字幕第29句开始，第39句结束。原计划尾句与下节首句重复，此处切分在具体工作介绍之前。",
      "lead_text": "幸运的是，随着硬件发展，出现了不同层次的存储。在性能层，有集成在 GPU 或新一代 CPU 中的 HBM。",
      "tail_text": "首先是 GPU 层的工作 Fleche，解决软硬交互开销大的问题。",
      "text_span": {
        "start": 28,
        "end": 39
      }
    },
    {
      "id": "sec_004",
      "index": 3,
      "title": "GPU层参数存储：Fleche查询融合针对GPU片上内存软硬交互开销大的问题，提出自识别Kernel融合方法，减少零碎小Kernel产生的开销。针对GPU片上内存软硬交互开销大的问题，提出自识别Kernel融合方法，减少零碎小Kernel产生的开销。",
      "time_range": "00:08:30,180 - 00:11:51,235",
      "summary": "",
      "refined_script": "# Fleche: 针对 GPU 参数存储的自识别查询融合技术\n\n## 1. 背景与挑战\n在推荐系统等深度学习场景中，GPU 层参数存储面临严重的**软硬交互开销**问题。其核心矛盾在于：\n\n*   **内核碎片化**：模型通常包含数百个嵌入表（Embedding Tables）和数千个特征，导致系统生成海量零碎的小 Kernel。\n*   **传统编译器的局限性**：\n    *   **TVM/XLA 的局限**：传统深度学习编译器擅长处理输入输出形状固定的张量（如 AlexNet）。\n    *   **动态性挑战**：推荐模型中的张量形状具有高度动态性（例如不同用户观看视频数量不一，Batch 拼接后长度不等），导致传统 Kernel Fusion 难以奏效。\n\n## 2. 核心设计理念：查询融合 (Query Fusion)\nFleche 的核心观察是：**虽然小 Kernel 数量众多，但其执行的逻辑大多源自同一个缓存查询函数，仅参数不同。**\n\n基于此，Fleche 放弃了为每个任务发射独立 Kernel 的做法，转而采用**单体大 Kernel** 的策略：\n*   **发射机制**：发射一个包含足够多线程的巨大单体 Kernel。\n*   **自识别处理**：让大 Kernel 内部的线程自动识别并领取原属于多个小 Kernel 的任务。\n\n## 3. 关键技术实现\n\n### 3.1 三阶段自识别 Kernel 融合方法\n为了实现高效的任务分发与执行，Fleche 引入了以下机制：\n1.  **前缀和记录**：在 CPU 端记录所有原小 Kernel 所需线程数的前缀和数组（Prefix Sum Array）。\n2.  **二分查找定位**：大 Kernel 启动后，内部的每个线程通过对前缀和数组进行**二分查找**，快速识别自己归属于哪一个原始查询任务。\n3.  **执行查询**：识别归属关系后，线程直接执行对应的缓存查询逻辑。\n\n### 3.2 CPU-GPU 数据拷贝优化\n针对小块数据从 CPU 到 GPU 的拷贝瓶颈，Fleche 借鉴了网卡（NIC）的特性：\n*   **Load/Store 直接拷贝**：利用 CPU 的 Load/Store 指令直接操作显存，优化了零碎数据的传输性能，进一步降低了软硬交互延迟。\n\n## 4. 性能表现\n实验数据表明，Fleche 成功解决了延迟随任务数线性增长的问题：\n*   **延迟稳定性**：随着 Kernel 个数（特征数量）的增加，Fleche 的处理延迟基本保持稳定，不再随 Kernel 数量线性上升。\n*   **系统收益**：有效缓解了 GPU 片上内存软硬交互的开销，提升了整体查询效率。",
      "original_script": "首先是 GPU 层的工作 Fleche，解决软硬交互开销大的问题。\n开销大的本质原因是产生了大量零碎的小 Kernel。一个模型可能有几百个嵌入表，对应产生几百个小 Kernel；如果有几千个特征，则会产生几千个小 Kernel。\n海量小 Kernel 并非新问题。传统深度学习编译器（如 TVM、XLA）会通过 Kernel Fusion（内核融合）来缓解。\n但我们发现，传统编译器并不适合缓存查询的 Kernel 融合。\n因为它们适合处理输入输出形状固定的张量（如 AlexNet），而推荐模型中的张量形状是非常动态的。\n例如，不同用户观看视频的数量不同，拼接成 Batch 后长度不一。这对 TVM 等编译器非常不友好，不利于融合。\n我们工作的核心观察是：虽然有很多小 Kernel，但它们大多来自同一个缓存查询函数，只是参数不同。\n因此，我们可以发射一个巨大的单体 Kernel，内部包含足够多的线程。\n让这些线程自识别并处理原来多个小 Kernel 的任务，实现查询融合。\n为此，我们提出了“三阶段自识别 Kernel 融合方法”。\n核心思想是发射一个大 Kernel，同时记录原小 Kernel 线程数的前缀和数组。大 Kernel 内部的每个线程通过二分查找识别其归属关系，再进行缓存查询。\n我们还利用原属于网卡的特性，让 CPU 实现 Load/Store 直接拷贝显存，优化了 CPU 到 GPU 的小块数据拷贝性能。\n性能评估显示，与现有系统相比，Fleche 的延迟不随 Kernel 个数增加而线性上升，基本保持稳定。\n小结一下，针对 GPU 片上内存软硬交互开销大的问题，我们的核心思想是查询融合。",
      "start_ms": 510180,
      "end_ms": 711235,
      "material_ids": [
        "chunk_00/slide_014",
        "chunk_00/slide_015",
        "chunk_00/slide_016",
        "chunk_00/slide_017",
        "chunk_00/slide_018",
        "chunk_00/slide_019",
        "chunk_00/slide_020"
      ],
      "notes": "校准至字幕第40句开始，第53句结束。",
      "lead_text": "首先是 GPU 层的工作 Fleche，解决软硬交互开销大的问题。",
      "tail_text": "小结一下，针对 GPU 片上内存软硬交互开销大的问题，我们的核心思想是查询融合。",
      "text_span": {
        "start": 39,
        "end": 52
      }
    },
    {
      "id": "sec_005",
      "index": 4,
      "title": "PM层参数存储：PetPS网卡聚集机制利用网卡的SG-DMA功能代替CPU完成参数序列化，规避持久性内存（PM）的高延迟导致的CPU瓶颈。利用网卡的SG-DMA功能代替CPU完成参数序列化，规避持久性内存（PM）的高延迟导致的CPU瓶颈。",
      "time_range": "00:11:52,760 - 00:15:30,665",
      "summary": "",
      "refined_script": "# PetPS：基于网卡 SG-DMA 的高性能持久性内存（PM）参数存储机制\n\n## 1. 背景与挑战\n在参数服务器（Parameter Server, PS）架构中，引入持久性内存（Persistent Memory, PM）旨在平衡存储容量与成本。然而，实际部署中发现系统吞吐量受限于 CPU 瓶颈（利用率达 100%），而非网络或存储带宽。\n\n### 1.1 核心痛点：序列化开销\n*   **资源消耗**：参数的序列化与聚集（Aggregation）过程占据了约 35% 的 CPU 资源。\n*   **性能衰减诱因**：在 DRAM 环境下，序列化开销尚可接受；但在 PM 环境下，由于 PM 的访问延迟高于 DRAM，CPU 在执行逐个参数拷贝至缓冲区的过程中，高加载延迟导致 CPU 耗时显著增加，形成严重的性能瓶颈。\n\n## 2. 核心设计：网卡聚集机制 (NIC Aggregation)\n为了规避 CPU 直接访问 PM 带来的延迟瓶颈，PetPS 利用商用网卡中常被忽视的 **SG-DMA（Scatter-Gather DMA）** 功能，将参数聚集任务从 CPU 卸载（Offload）至网卡。\n\n### 2.1 工作流程\n1.  **描述符发射**：CPU 不再直接执行内存拷贝，而是构建包含参数源地址（PM 空间）和目的地址的描述符（Descriptors）。\n2.  **异步聚集**：CPU 将描述符提交给网卡后即可释放，由网卡硬件自动完成内存数据的拉取与聚集。\n3.  **协同访存**：整个过程实现了 CPU 与网卡的协同工作，CPU 无需直接参与 PM 数据的搬运，从而有效规避了 PM 的高加载延迟。\n\n## 3. 数据一致性保障\n由于网卡异步拉取数据与 CPU 更新参数可能存在并发冲突（即网卡读取时参数版本发生更新），PetPS 设计了专门的一致性维护机制：\n\n*   **写时拷贝 (Copy-on-Write, CoW)**：当参数需要更新时，不直接覆盖原数据，确保网卡读取的是一致的历史版本。\n*   **空间回收**：引入**基于 Epoch 的空间回收机制**，确保已失效的旧版本参数在所有读取操作完成后能被安全回收。\n\n## 4. 性能表现与价值\n实验数据表明，PetPS 在保持高性能的同时显著优化了成本结构：\n\n| 维度 | 表现描述 |\n| :--- | :--- |\n| **吞吐量** | 较经典 PS 系统有大幅提升 |\n| **延迟** | 与纯 DRAM 系统基本持平 |\n| **成本** | 相比纯 DRAM 方案降低了约 30% |\n\n## 5. 小结\nPetPS 通过利用网卡的硬件 DMA 能力，成功解决了 PM 存储层中的 CPU 瓶颈问题。这种“协同访存”的设计思路，为高延迟存储介质在高性能计算场景中的应用提供了有效的参考路径。",
      "original_script": "在 PM 层，第二个工作 PetPS 针对参数服务器换用 PM 后的 CPU 瓶颈问题。\n我们发现 CPU 利用率达到 100%，成为瓶颈。分析 PS 的工作流发现，参数的序列化和聚集过程消耗了大量 CPU 资源，占比约 35%。\n为什么在 DRAM 上这不是问题，换成 PM 就成了问题？\n因为 PM 较慢，在序列化过程中，CPU 需要逐个将参数拷贝到缓冲区，高延迟导致 CPU 耗时显著增加。\n我们的核心想法是利用商用网卡中常被忽视的 SG-DMA（Scatter-Gather DMA）功能。\n它可以代替 CPU 完成 PM 上参数的序列化。CPU 只需向网卡提交包含源地址和目的地址的描述符，网卡即可自动完成内存聚集。\n基于此，我们设计了“网卡聚集机制”。\nCPU 只需发射描述符，由网卡负责拉取参数。整个过程中 CPU 无需直接访问 PM，从而规避了 PM 的高加载延迟，提升了吞吐量。\n当然，这也会带来参数一致性问题。例如网卡正在拉取参数时，参数版本发生了更新。\n网卡对此并不知情，可能导致拉取的数据新旧版本混杂。\n我们通过写时拷贝（Copy-on-Write）和基于 Epoch 的空间回收机制来维护参数一致性，此处不再展开。\n性能测试显示，PetPS 相比经典 PS 系统吞吐量大幅提升，且相比纯 DRAM 系统，延迟基本相当，同时成本降低了 30%。\n小结：针对 PM 延迟导致的 CPU 瓶颈，我们利用网卡 DMA 能力实现了协同访存。",
      "start_ms": 712760,
      "end_ms": 930665,
      "material_ids": [
        "chunk_00/slide_021",
        "chunk_00/slide_022",
        "chunk_00/slide_023",
        "chunk_00/slide_024",
        "chunk_00/slide_025",
        "chunk_00/slide_026",
        "chunk_00/slide_027",
        "chunk_00/slide_028",
        "chunk_00/slide_029"
      ],
      "notes": "校准至字幕第54句开始，第66句结束。",
      "lead_text": "在 PM 层，第二个工作 PetPS 针对参数服务器换用 PM 后的 CPU 瓶颈问题。",
      "tail_text": "小结：针对 PM 延迟导致的 CPU 瓶颈，我们利用网卡 DMA 能力实现了协同访存。",
      "text_span": {
        "start": 53,
        "end": 65
      }
    },
    {
      "id": "sec_006",
      "index": 5,
      "title": "SSD层参数存储：MaxEmbed选择性副本针对SSD读放大问题，通过热点参数的选择性副本策略，利用空间换带宽，提升硬件利用率。针对SSD读放大问题，通过热点参数的选择性副本策略，利用空间换带宽，提升硬件利用率。",
      "time_range": "00:15:32,760 - 00:19:56,565",
      "summary": "",
      "refined_script": "# MaxEmbed: 针对 SSD 参数存储的选择性副本策略\n\n## 1. 背景与挑战\n在基于 SSD 的大规模推荐系统参数存储中，**粒度不匹配**是导致读放大（Read Amplification）的核心原因。由于参数（Embedding）通常远小于 SSD 的物理页面（Page），单次读取往往包含大量无效数据。\n\n### 1.1 现有技术的局限性\n现有的优化方案（如 Facebook 的 SHP）尝试通过**超图划分（Hypergraph Partitioning）**算法，将具有共现关系的参数排布在同一个 SSD Page 中。然而，这种方式存在明显瓶颈：\n- **硬件利用率低**：即使经过精细排布，有效带宽利用率仅能从 6% 提升至 8%。\n- **归属冲突**：超图划分假设一个参数仅能归属于一个子图（即一个 Page）。但在实际场景中，**超过 30% 的参数存在于多组共现关系中**。例如，“iPad” 既属于“平板电脑”类别，也属于“苹果生态”或“数码配件”类别。单副本存储无法同时满足多种共现组合的读取效率。\n\n## 2. 核心设计理念：空间换带宽\nMaxEmbed 提出了**选择性副本（Selective Replication）**策略。其核心逻辑是利用廉价的 SSD 存储空间换取昂贵的读取带宽，通过为热点参数创建额外副本，尽可能覆盖更多的共现组合，从而减少总体的 IO 次数。\n\n## 3. 关键技术实现\n实现该策略面临两个核心挑战，MaxEmbed 分别从算法和系统架构层面给出了解决方案：\n\n### 3.1 离线阶段：副本选择算法\n*   **挑战**：在给定的空间预算下，选择哪些参数进行副本化以最大化收益是一个 **NP-Hard** 问题。\n*   **对策**：MaxEmbed 提出了一套启发式解法，通过建模参数间的关联强度，离线计算最优的副本分布方案。\n\n### 3.2 在线阶段：最优请求拆分与流水线优化\n*   **挑战**：面对在线请求，如何利用副本将请求拆分为最优的页面组合（类似于“集合覆盖问题”），且不能增加系统延迟。\n*   **系统级优化**：\n    1.  **流水线化（Pipelining）**：将 SSD IO 与计算任务并行化。\n    2.  **两阶段决策**：系统首先识别并发出“必须读取”的页面请求；在等待 SSD 返回数据的空档期，利用 CPU 计算后续的最优读取策略，从而抵消算法开销。\n\n## 4. 实验结果与结论\n实验数据表明，MaxEmbed 能够显著提升硬件资源利用率：\n- **空间开销**：仅需 20% 的额外空间放大。\n- **性能提升**：系统吞吐率（Throughput）提升了 **30%**。\n\n**总结**：MaxEmbed 通过选择性副本策略，打破了传统单副本存储的限制，有效解决了 SSD 在处理稀疏参数读取时的带宽瓶颈问题。",
      "original_script": "最后来到 SSD 层的工作 MaxEmbed，解决粒度不匹配导致的读放大问题。\n现有工作尝试将容易共现的参数放在同一个 SSD Page 中，以增加单次读取的有效数据量。\n但我们发现这还不够。Facebook 的 SHP 工作即使采用精巧的排布，有效带宽也仅能从 6% 提升到 8%，硬件利用率依然极低。\n分析原因发现，现有工作将参数共现关系视为超图中的边，并利用超图划分算法将其切分为子图。\n每个子图内的参数数量少于一个 SSD Block。\n虽然这能捕获部分共现关系，但由于一个参数在磁盘上只存一份，它只能归属于一个子图。\n然而现实中，一个参数往往存在于多组共现关系中。统计显示，这种情况占比超过 30%。\n举个生活化的例子：我想买平板，可能关注华为 MatePad、iPad Air 或小米平板。\n如果我只想买 iPad，可能关注 iPad Air、iPad mini 及其壳膜。\n如果是“果粉”，可能关注 iPad、MacBook 和 iPhone。\n这三组关系叠加后，iPad mini 和 iPad Air 就成了热点。现有的划分方式只能捕获其中一类共现关系，不够高效。\n我们的核心想法是利用廉价的磁盘空间换取带宽。\n具体而言，为热点参数创建额外的副本，尽可能多地捕获组合关系。这样原本需要三次读取的操作，现在可能只需两次。\n实现这一目标涉及两个复杂问题：一是离线选择哪些参数做副本，这是一个 NP-Hard 的数学问题，我们提出了相应的解法。\n二是面对在线请求，如何利用副本将其拆分为最优的页面组合。这看似是集合覆盖问题，但在线系统没有时间运行复杂算法。\n我们将其转化为系统问题，通过流水线化 SSD IO 和计算，先确定必须读取的页面并发出请求，在等待过程中再计算后续的最优读取策略。\n实验结果显示，在 20% 的空间放大下，吞吐率提升了 30%，更高效地利用了 SSD 带宽。\n小结：针对磁盘读放大，我们采取了选择性副本策略。",
      "start_ms": 932760,
      "end_ms": 1196565,
      "material_ids": [
        "chunk_00/slide_030",
        "chunk_00/slide_031",
        "chunk_00/slide_032",
        "chunk_00/slide_033",
        "chunk_00/slide_034",
        "chunk_00/slide_035",
        "chunk_00/slide_036",
        "chunk_00/slide_037",
        "chunk_00/slide_038"
      ],
      "notes": "校准至字幕第67句开始，第84句结束。",
      "lead_text": "最后来到 SSD 层的工作 MaxEmbed，解决粒度不匹配导致的读放大问题。",
      "tail_text": "小结：针对磁盘读放大，我们采取了选择性副本策略。",
      "text_span": {
        "start": 66,
        "end": 83
      }
    },
    {
      "id": "sec_007",
      "index": 6,
      "title": "RecStore系统研发与产业落地介绍整合三层优化工作的RecStore系统及其在快手等企业的实际应用效果。介绍整合三层优化工作的RecStore系统及其在快手等企业的实际应用效果。",
      "time_range": "00:19:58,080 - 00:20:29,410",
      "summary": "",
      "refined_script": "# RecStore：工业级推荐系统存储优化方案与落地实践\n\n## 1. 系统概述\n**RecStore** 是一个旨在整合多层优化技术并实现统一管理的推荐系统存储引擎。该系统目前已正式开源，其核心目标是通过对底层存储逻辑的深度解构与重组，为上层多样化的算法框架提供高性能、高可靠的存储支撑。\n\n## 2. 核心架构设计\nRecStore 的设计理念围绕“统一性”与“兼容性”展开：\n- **三层架构整合**：将三个维度的优化工作（如索引优化、内存管理、I/O 调度等）整合进统一的系统框架内，实现资源的高效协同。\n- **多框架支持**：系统向上提供标准化的接口，能够无缝对接并支持多种主流的深度学习与推荐计算框架。\n\n## 3. 产业落地与应用成效\nRecStore 已在头部互联网企业（如快手）实现大规模生产环境落地，并取得了显著的经济与技术效益：\n\n### A. 业务价值\n- **成本优化**：通过高效的存储管理与资源调度，帮助企业节省了约 **1 亿元人民币** 的硬件及运营成本。\n- **高并发支撑**：系统表现出极强的稳定性，成功支撑了**亿级用户量**的实时访问与服务需求。\n\n### B. 行业认可\n- **生态合作**：RecStore 的技术突破与落地成果已获得 **Intel** 等国际领先硬件厂商的官方报道与技术认可。",
      "original_script": "我们将这三个层次的工作整合为 RecStore 系统并已开源。我们希望实现各层次的统一管理，并向上支持多个框架。\n部分工作已在快手落地，帮助其节省了约一亿元成本，稳定服务了亿级用户。Intel 公司也对此进行了报道。",
      "start_ms": 1198080,
      "end_ms": 1229410,
      "material_ids": [
        "chunk_00/slide_039",
        "chunk_00/slide_040"
      ],
      "notes": "校准至字幕第85句开始，第86句结束。",
      "lead_text": "我们将这三个层次的工作整合为 RecStore 系统并已开源。我们希望实现各层次的统一管理，并向上支持多个框架。",
      "tail_text": "部分工作已在快手落地，帮助其节省了约一亿元成本，稳定服务了亿级用户。Intel 公司也对此进行了报道。",
      "text_span": {
        "start": 84,
        "end": 85
      }
    },
    {
      "id": "sec_008",
      "index": 7,
      "title": "未来展望与总结探讨生成式推荐模型（GR）带来的新挑战，并对全篇内容进行总结及现场问答。探讨生成式推荐模型（GR）带来的新挑战，并对全篇内容进行总结及现场问答。",
      "time_range": "00:20:30,730 - 00:24:42,519",
      "summary": "",
      "refined_script": "# 生成式推荐系统（GR）的演进趋势与存储架构优化实践\n\n## 1. 产业落地成果\n目前，相关研究成果已在**快手**等头部互联网企业成功落地，取得了显著的经济与技术效益：\n- **成本优化**：通过架构改良，累计为企业节省约一亿元的算力与存储成本。\n- **服务规模**：稳定支撑亿级用户量的实时推荐需求。\n- **行业认可**：该实践案例已获得 Intel 等国际主流硬件厂商的官方报道与关注。\n\n## 2. 推荐模型的新范式：从 DLRM 到 GR\n随着大模型技术的渗透，推荐系统正经历从传统深度学习推荐模型（DLRM）向**生成式推荐模型（Generative Recommendation, GR）**的范式转移。\n\n### 2.1 负载特征演进\n- **传统 DLRM**：呈现“大稀疏（Large Sparse）+ 小稠密（Small Dense）”的特征，计算压力主要集中在 Embedding 层。\n- **生成式 GR**：演变为“**大稀疏（Large Sparse）+ 大稠密（Large Dense）**”的结构。由于引入了更深层的 Transformer 等结构，稠密计算部分的参数量与计算量大幅增加。\n\n### 2.2 存储与 OS 的优化机遇\n面对 GR 带来的算力成本挑战，存储与操作系统领域的研究者可以借鉴大语言模型（LLM）的优化思路：\n- **以存代算**：利用类似 **KV Cache** 的缓存机制，通过增加存储层级的投入来换取计算资源的释放，从而降低整体部署成本。\n\n## 3. 硬件生态演进：后 Optane 时代的替代方案\n针对 Intel Optane（持久化内存）停产后的技术路径选择，研究重点应聚焦于“**慢速、字节粒度寻址、廉价**”的存储介质，而非绑定特定硬件。\n\n| 方案类型 | 技术特点 | 产业现状 |\n| :--- | :--- | :--- |\n| **Memory Semantic SSD** | 三星等厂商推出，支持字节粒度寻址。 | 处于应用探索期。 |\n| **CXL 内存扩展卡** | 基于 CXL 1.1+ 协议，插在主机上实现内存扩展，成本低于传统 DRAM。 | 快手等企业已开始尝试用于替代 Optane。 |\n| **CXL 远端内存池** | 通过网络/总线聚合远端内存资源。 | 未来具备大规模扩展潜力，适配现有的网卡聚集机制。 |\n\n## 4. RecStore 框架现状\n**RecStore** 是由中国人民大学团队主导并持续迭代的参数存储系统框架：\n- **功能状态**：目前核心功能已基本完备。\n- **优化方向**：团队正集中于性能调优与长期的社区维护，旨在为大模型时代下的推荐系统提供坚实的底层支撑。",
      "original_script": "部分工作已在快手落地，帮助其节省了约一亿元成本，稳定服务了亿级用户。Intel 公司也对此进行了报道。\n最后做一点未来展望。大模型对推荐模型也产生了冲击，出现了生成式推荐模型（GR）。\n我们发现它与传统 DLRM 不同，从“大稀疏+小稠密”演变为“大稀疏+大稠密”。\n对于存储 and OS 研究者来说，这充满了机遇。企业部署生成式推荐模型面临巨大的算力成本压力，我们可以借鉴 LLM 的 KV Cache 等思路，通过“以存代算”降低算力成本。\n以上就是我今天分享的全部内容，谢谢大家。\n感谢谢老师的深入解析。请您留步，现场的各位嘉宾朋友，有没有想要提问的？我们可以畅所欲言。\n要抓住这个机会。这位先生，请工作人员把麦克风递给后面这位先生。\n我不问学术问题，问个产业问题。持久化内存我研究过一段时间，觉得应用场景很好，但 Intel 把这条产线砍掉了，国内有没有可替代的方案？\n谢谢，非常好的问题。持久性内存是一个泛指的概念，并不非要绑定 Intel 的 Optane。现在三星等公司推出了 Memory Semantic SSD，即字节粒度寻址的磁盘。\n此外，还有 CXL 内存扩展卡，插在主机上，特性类似于 PM，且成本更低。\n快手当时买了很多 Optane，确实很有用。Intel 停产后，他们开始尝试 CXL 1.1 的内存扩展卡来替代。未来 CXL 扩展出的内存池会有更多机会。\n本质上我们的研究是针对慢速、字节粒度寻址的廉价内存，并不局限于 Optane。例如 CXL 远端内存，我们的网卡聚集机制依然能发挥作用。\n好的，谢谢。我们再加一位。\nRecStore 框架目前支持哪些模型？RecStore 主要是由我在人大带的学生在持续迭代。\n目前功能已基本完备，性能上还在进一步调优，我们一直在持续更新。\n再次感谢谢老师。参数存储系统的优化不仅提升了大模型的运行效率，更为人机智能技术的落地提供了坚实支撑。",
      "start_ms": 1230730,
      "end_ms": 1482519,
      "material_ids": [
        "chunk_00/slide_041",
        "chunk_00/slide_042"
      ],
      "notes": "校准至字幕第87句开始，第101句结束。",
      "lead_text": "部分工作已在快手落地，帮助其节省了约一亿元成本，稳定服务了亿级用户。Intel 公司也对此进行了报道。",
      "tail_text": "再次感谢谢老师。参数存储系统的优化不仅提升了大模型的运行效率，更为人机智能技术的落地提供了坚实支撑。",
      "text_span": {
        "start": 85,
        "end": 100
      }
    }
  ]
}