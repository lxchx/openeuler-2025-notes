1
00:00:00,360 --> 00:00:09,160
从高速互联总线的技术突破，到异构池化系统的实践探索，每一个细节都展现了前沿技术的创新魅力。

2
00:00:09,360 --> 00:00:18,140
接下来，让我们有请中国人民大学的谢明晖老师，带来《面向推荐大模型的参数存储系统研究》的分享。

3
00:00:18,420 --> 00:00:27,695
推荐大模型的参数存储是当前人工智能领域的关键难题，期待谢老师的分享能为我们解开疑惑。掌声欢迎！

4
00:00:36,080 --> 00:00:40,040
谢谢主持人的介绍，也谢谢各位领导和老师。

5
00:00:40,820 --> 00:00:48,305
我是来自中国人民大学的谢明晖，本次汇报的题目是《面向推荐大模型的参数存储系统研究》。

6
00:00:49,050 --> 00:00:54,030
首先对我个人情况做个简单的介绍。

7
00:00:54,450 --> 00:01:03,950
我本科就读于南京大学，去年在清华大学计算机系陆游游老师和舒继武老师指导下获得博士学位，毕业后加入中国人民大学。

8
00:01:05,650 --> 00:01:12,990
曾获得ACM优博奖、北京市青托计划等荣誉，并承担了一系列企业项目。

9
00:01:12,990 --> 00:01:26,765
我的研究方向主要围绕机器学习系统，具体涉及存储系统与机器学习系统的交叉领域，即面向 AI 的存储（Storage for AI）。

10
00:01:27,780 --> 00:01:36,360
我们目前的工作主要围绕参数存储、向量存储和记忆存储三个层次展开。

11
00:01:36,720 --> 00:01:45,600
随着人工智能模型规模的扩大，模型“脑容量”增加，对应的参数量也随之激增。

12
00:01:45,960 --> 00:02:00,110
其次，AI 模型观察万物的方式是将世界万物表达为向量，因此向量存储也是我们需要考虑的关键组件。

13
00:02:00,450 --> 00:02:13,070
第三块是记忆存储。区分聊天机器人与真正智能体的关键在于智能体是否拥有记忆。

14
00:02:13,090 --> 00:02:28,610
如果没有记忆，它只是一个简单的 Chatbot。因此，我们也面向特定场景研究大语言模型（LLM）的记忆方案，包括 RAG 或参数化记忆方案。

15
00:02:28,950 --> 00:02:38,890
回到本次演讲的主题，我们主要针对推荐模型的参数存储，即刚才提到的最底层部分。

16
00:02:39,310 --> 00:03:01,600
当前互联网高度发达，每天产生海量的视频、推文等 UGC 内容。面对海量信息，用户无法全部处理，因此需要推荐模型负责筛选并呈现。

17
00:03:02,060 --> 00:03:18,575
在 Facebook 的 AI 集群中，推荐模型在 AI 服务耗时中的占比已达到 80%。因此，推荐模型已成为现代数据中心 AI 服务的主要载体。

18
00:03:19,650 --> 00:03:35,220
具体看推荐模型的结构，它与传统的 CV 或 NLP 模型不同，拥有一个非常庞大的嵌入层（Embedding Layer）。

19
00:03:35,440 --> 00:03:41,500
其中包含许多嵌入表，如城市、用户、视频的嵌入表。

20
00:03:41,940 --> 00:03:54,755
推荐模型的输入不是像 ResNet 那样的稠密图像，而是高维稀疏的 ID。

21
00:03:55,450 --> 00:04:07,670
由于 DNN 难以直接处理高维稀疏 ID，因此使用嵌入表将其转化为低维稠密向量。

22
00:04:07,670 --> 00:04:25,830
例如，根据用户最近观看的三个视频 ID，从表中查出对应的三行向量，再喂给神经网络。这部分稀疏参数访问是典型的访存密集型而非计算密集型。

23
00:04:26,130 --> 00:04:33,450
当前推荐模型面临两方面的存储困境。第一是容量墙，参数量巨大。

24
00:04:34,370 --> 00:04:49,445
由于 ID 交叉等技术，嵌入层参数量可达 12 万亿级。工业实践证明，模型精度提升需要参数量的同步提升。

25
00:04:50,250 --> 00:05:09,750
第二是带宽墙。嵌入层涉及大量访存操作。根据阿里的统计，嵌入操作占端到端模型时间的 70% 以上。

26
00:05:10,150 --> 00:05:23,690
这严重限制了算力的发挥。受这两堵墙叠加影响，快手目前部署了万台内存参数服务器，仅用于存储模型。

27
00:05:24,110 --> 00:05:27,025
其机器成本极高，达到了十亿元级别。

28
00:05:28,470 --> 00:05:35,435
因此，存储系统已成为制约推荐模型发展的核心因素。

29
00:05:37,700 --> 00:05:58,900
幸运的是，随着硬件发展，出现了不同层次的存储。在性能层，有集成在 GPU 或新一代 CPU 中的 HBM。

30
00:05:58,900 --> 00:06:09,170
我们可以利用 HBM 做缓存来缓解带宽墙。在容量层，有持久性内存（PM），它成本更低，可用于内存扩容。

31
00:06:09,530 --> 00:06:19,785
我们研究的核心思路是混合利用多层次内存构建存储系统，实现容量和带宽的互补。

32
00:06:20,910 --> 00:06:30,750
但实现这一目标极具挑战，主要问题在于不同层次硬件的特性差异巨大。

33
00:06:30,770 --> 00:06:56,545
例如，在 GPU HBM 层，软硬交互效率较低。使用 HBM 做缓存查询时，需要 CPU 启动 Kernel 让 GPU 执行，而 CPU 调用 GPU 的开销占比可达 70%。

34
00:06:57,440 --> 00:07:11,030
在 DRAM 和 PM 层，介质性能难以发挥。PM 延迟较高，且属于 Load/Store 介质。

35
00:07:11,330 --> 00:07:28,175
如果 PM 延迟是 DRAM 的三倍，会严重卡住 CPU 流水线。将参数服务器的 DRAM 换成 PM 后，吞吐量下降了 1.9 倍，抵消了硬件成本优势。

36
00:07:28,950 --> 00:07:46,170
在 SSD 层，参数读放大现象严重。内存介质是字节粒度寻址，而 SSD 是块寻址。

37
00:07:46,170 --> 00:07:57,530
即使只访问 64B 到 512B 的参数，也必须读取整个 4KB 的 Page。这种粒度不匹配导致 SSD 读放大严重。

38
00:07:57,850 --> 00:08:12,925
即使是高性能 SSD，其实际有效带宽也仅有 6% 左右。

39
00:08:14,090 --> 00:08:28,745
针对这三个层次的问题，我们分别开展了工作，成果发表在 EuroSys、VLDB 和 ASPLOS 等会议上。

40
00:08:30,180 --> 00:08:35,940
首先是 GPU 层的工作 Fleche，解决软硬交互开销大的问题。

41
00:08:36,540 --> 00:08:56,205
开销大的本质原因是产生了大量零碎的小 Kernel。一个模型可能有几百个嵌入表，对应产生几百个小 Kernel；如果有几千个特征，则会产生几千个小 Kernel。

42
00:08:57,810 --> 00:09:12,610
海量小 Kernel 并非新问题。传统深度学习编译器（如 TVM、XLA）会通过 Kernel Fusion（内核融合）来缓解。

43
00:09:12,870 --> 00:09:22,650
但我们发现，传统编译器并不适合缓存查询的 Kernel 融合。

44
00:09:23,070 --> 00:09:38,880
因为它们适合处理输入输出形状固定的张量（如 AlexNet），而推荐模型中的张量形状是非常动态的。

45
00:09:38,880 --> 00:09:52,365
例如，不同用户观看视频的数量不同，拼接成 Batch 后长度不一。这对 TVM 等编译器非常不友好，不利于融合。

46
00:09:53,250 --> 00:10:13,420
我们工作的核心观察是：虽然有很多小 Kernel，但它们大多来自同一个缓存查询函数，只是参数不同。

47
00:10:13,840 --> 00:10:28,140
因此，我们可以发射一个巨大的单体 Kernel，内部包含足够多的线程。

48
00:10:28,280 --> 00:10:29,545
让这些线程自识别并处理原来多个小 Kernel 的任务，实现查询融合。

49
00:10:30,880 --> 00:10:35,620
为此，我们提出了“三阶段自识别 Kernel 融合方法”。

50
00:10:36,120 --> 00:10:58,975
核心思想是发射一个大 Kernel，同时记录原小 Kernel 线程数的前缀和数组。大 Kernel 内部的每个线程通过二分查找识别其归属关系，再进行缓存查询。

51
00:10:59,720 --> 00:11:29,075
我们还利用原属于网卡的特性，让 CPU 实现 Load/Store 直接拷贝显存，优化了 CPU 到 GPU 的小块数据拷贝性能。

52
00:11:29,670 --> 00:11:40,245
性能评估显示，与现有系统相比，Fleche 的延迟不随 Kernel 个数增加而线性上升，基本保持稳定。

53
00:11:41,890 --> 00:11:51,235
小结一下，针对 GPU 片上内存软硬交互开销大的问题，我们的核心思想是查询融合。

54
00:11:52,760 --> 00:12:01,220
在 PM 层，第二个工作 PetPS 针对参数服务器换用 PM 后的 CPU 瓶颈问题。

55
00:12:01,780 --> 00:12:32,325
我们发现 CPU 利用率达到 100%，成为瓶颈。分析 PS 的工作流发现，参数的序列化和聚集过程消耗了大量 CPU 资源，占比约 35%。

56
00:12:33,770 --> 00:12:41,070
为什么在 DRAM 上这不是问题，换成 PM 就成了问题？

57
00:12:41,310 --> 00:12:56,505
因为 PM 较慢，在序列化过程中，CPU 需要逐个将参数拷贝到缓冲区，高延迟导致 CPU 耗时显著增加。

58
00:12:57,710 --> 00:13:12,450
我们的核心想法是利用商用网卡中常被忽视的 SG-DMA（Scatter-Gather DMA）功能。

59
00:13:12,450 --> 00:13:31,445
它可以代替 CPU 完成 PM 上参数的序列化。CPU 只需向网卡提交包含源地址和目的地址的描述符，网卡即可自动完成内存聚集。

60
00:13:32,750 --> 00:13:37,730
基于此，我们设计了“网卡聚集机制”。

61
00:13:38,150 --> 00:14:06,340
CPU 只需发射描述符，由网卡负责拉取参数。整个过程中 CPU 无需直接访问 PM，从而规避了 PM 的高加载延迟，提升了吞吐量。

62
00:14:07,020 --> 00:14:21,080
当然，这也会带来参数一致性问题。例如网卡正在拉取参数时，参数版本发生了更新。

63
00:14:21,520 --> 00:14:32,910
网卡对此并不知情，可能导致拉取的数据新旧版本混杂。

64
00:14:34,220 --> 00:14:48,335
我们通过写时拷贝（Copy-on-Write）和基于 Epoch 的空间回收机制来维护参数一致性，此处不再展开。

65
00:14:49,780 --> 00:15:15,465
性能测试显示，PetPS 相比经典 PS 系统吞吐量大幅提升，且相比纯 DRAM 系统，延迟基本相当，同时成本降低了 30%。

66
00:15:16,760 --> 00:15:30,665
小结：针对 PM 延迟导致的 CPU 瓶颈，我们利用网卡 DMA 能力实现了协同访存。

67
00:15:32,760 --> 00:15:39,400
最后来到 SSD 层的工作 MaxEmbed，解决粒度不匹配导致的读放大问题。

68
00:15:39,860 --> 00:15:52,160
现有工作尝试将容易共现的参数放在同一个 SSD Page 中，以增加单次读取的有效数据量。

69
00:15:53,820 --> 00:16:15,215
但我们发现这还不够。Facebook 的 SHP 工作即使采用精巧的排布，有效带宽也仅能从 6% 提升到 8%，硬件利用率依然极低。

70
00:16:16,240 --> 00:16:36,540
分析原因发现，现有工作将参数共现关系视为超图中的边，并利用超图划分算法将其切分为子图。

71
00:16:36,840 --> 00:16:44,260
每个子图内的参数数量少于一个 SSD Block。

72
00:16:44,820 --> 00:17:04,555
虽然这能捕获部分共现关系，但由于一个参数在磁盘上只存一份，它只能归属于一个子图。

73
00:17:05,500 --> 00:17:18,815
然而现实中，一个参数往往存在于多组共现关系中。统计显示，这种情况占比超过 30%。

74
00:17:19,520 --> 00:17:32,015
举个生活化的例子：我想买平板，可能关注华为 MatePad、iPad Air 或小米平板。

75
00:17:32,750 --> 00:17:39,965
如果我只想买 iPad，可能关注 iPad Air、iPad mini 及其壳膜。

76
00:17:40,700 --> 00:17:44,435
如果是“果粉”，可能关注 iPad、MacBook 和 iPhone。

77
00:17:45,110 --> 00:18:02,450
这三组关系叠加后，iPad mini 和 iPad Air 就成了热点。现有的划分方式只能捕获其中一类共现关系，不够高效。

78
00:18:02,810 --> 00:18:14,805
我们的核心想法是利用廉价的磁盘空间换取带宽。

79
00:18:15,960 --> 00:18:31,485
具体而言，为热点参数创建额外的副本，尽可能多地捕获组合关系。这样原本需要三次读取的操作，现在可能只需两次。

80
00:18:32,250 --> 00:18:48,410
实现这一目标涉及两个复杂问题：一是离线选择哪些参数做副本，这是一个 NP-Hard 的数学问题，我们提出了相应的解法。

81
00:18:48,830 --> 00:19:06,580
二是面对在线请求，如何利用副本将其拆分为最优的页面组合。这看似是集合覆盖问题，但在线系统没有时间运行复杂算法。

82
00:19:06,660 --> 00:19:38,020
我们将其转化为系统问题，通过流水线化 SSD IO 和计算，先确定必须读取的页面并发出请求，在等待过程中再计算后续的最优读取策略。

83
00:19:38,480 --> 00:19:45,875
实验结果显示，在 20% 的空间放大下，吞吐率提升了 30%，更高效地利用了 SSD 带宽。

84
00:19:48,060 --> 00:19:56,565
小结：针对磁盘读放大，我们采取了选择性副本策略。

85
00:19:58,080 --> 00:20:12,615
我们将这三个层次的工作整合为 RecStore 系统并已开源。我们希望实现各层次的统一管理，并向上支持多个框架。

86
00:20:14,300 --> 00:20:29,410
部分工作已在快手落地，帮助其节省了约一亿元成本，稳定服务了亿级用户。Intel 公司也对此进行了报道。

87
00:20:30,730 --> 00:20:38,555
最后做一点未来展望。大模型对推荐模型也产生了冲击，出现了生成式推荐模型（GR）。

88
00:20:39,460 --> 00:20:47,855
我们发现它与传统 DLRM 不同，从“大稀疏+小稠密”演变为“大稀疏+大稠密”。

89
00:20:48,610 --> 00:21:12,635
对于存储 and OS 研究者来说，这充满了机遇。企业部署生成式推荐模型面临巨大的算力成本压力，我们可以借鉴 LLM 的 KV Cache 等思路，通过“以存代算”降低算力成本。

90
00:21:14,040 --> 00:21:19,000
以上就是我今天分享的全部内容，谢谢大家。

91
00:21:20,900 --> 00:21:33,705
感谢谢老师的深入解析。请您留步，现场的各位嘉宾朋友，有没有想要提问的？我们可以畅所欲言。

92
00:21:34,670 --> 00:21:44,190
要抓住这个机会。这位先生，请工作人员把麦克风递给后面这位先生。

93
00:21:50,580 --> 00:22:13,705
我不问学术问题，问个产业问题。持久化内存我研究过一段时间，觉得应用场景很好，但 Intel 把这条产线砍掉了，国内有没有可替代的方案？

94
00:22:15,060 --> 00:22:42,700
谢谢，非常好的问题。持久性内存是一个泛指的概念，并不非要绑定 Intel 的 Optane。现在三星等公司推出了 Memory Semantic SSD，即字节粒度寻址的磁盘。

95
00:22:43,060 --> 00:23:01,230
此外，还有 CXL 内存扩展卡，插在主机上，特性类似于 PM，且成本更低。

96
00:23:01,650 --> 00:23:26,095
快手当时买了很多 Optane，确实很有用。Intel 停产后，他们开始尝试 CXL 1.1 的内存扩展卡来替代。未来 CXL 扩展出的内存池会有更多机会。

97
00:23:27,000 --> 00:23:55,240
本质上我们的研究是针对慢速、字节粒度寻址的廉价内存，并不局限于 Optane。例如 CXL 远端内存，我们的网卡聚集机制依然能发挥作用。

98
00:23:55,580 --> 00:24:07,000
好的，谢谢。我们再加一位。

99
00:24:07,240 --> 00:24:22,620
RecStore 框架目前支持哪些模型？RecStore 主要是由我在人大带的学生在持续迭代。

100
00:24:22,820 --> 00:24:34,000
目前功能已基本完备，性能上还在进一步调优，我们一直在持续更新。

101
00:24:34,680 --> 00:24:42,519
再次感谢谢老师。参数存储系统的优化不仅提升了大模型的运行效率，更为人机智能技术的落地提供了坚实支撑。
