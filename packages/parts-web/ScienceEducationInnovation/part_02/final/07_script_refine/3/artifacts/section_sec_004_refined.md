# Fleche: 针对 GPU 参数存储的自识别查询融合技术

## 1. 背景与挑战
在推荐系统等深度学习场景中，GPU 层参数存储面临严重的**软硬交互开销**问题。其核心矛盾在于：

*   **内核碎片化**：模型通常包含数百个嵌入表（Embedding Tables）和数千个特征，导致系统生成海量零碎的小 Kernel。
*   **传统编译器的局限性**：
    *   **TVM/XLA 的局限**：传统深度学习编译器擅长处理输入输出形状固定的张量（如 AlexNet）。
    *   **动态性挑战**：推荐模型中的张量形状具有高度动态性（例如不同用户观看视频数量不一，Batch 拼接后长度不等），导致传统 Kernel Fusion 难以奏效。

## 2. 核心设计理念：查询融合 (Query Fusion)
Fleche 的核心观察是：**虽然小 Kernel 数量众多，但其执行的逻辑大多源自同一个缓存查询函数，仅参数不同。**

基于此，Fleche 放弃了为每个任务发射独立 Kernel 的做法，转而采用**单体大 Kernel** 的策略：
*   **发射机制**：发射一个包含足够多线程的巨大单体 Kernel。
*   **自识别处理**：让大 Kernel 内部的线程自动识别并领取原属于多个小 Kernel 的任务。

## 3. 关键技术实现

### 3.1 三阶段自识别 Kernel 融合方法
为了实现高效的任务分发与执行，Fleche 引入了以下机制：
1.  **前缀和记录**：在 CPU 端记录所有原小 Kernel 所需线程数的前缀和数组（Prefix Sum Array）。
2.  **二分查找定位**：大 Kernel 启动后，内部的每个线程通过对前缀和数组进行**二分查找**，快速识别自己归属于哪一个原始查询任务。
3.  **执行查询**：识别归属关系后，线程直接执行对应的缓存查询逻辑。

### 3.2 CPU-GPU 数据拷贝优化
针对小块数据从 CPU 到 GPU 的拷贝瓶颈，Fleche 借鉴了网卡（NIC）的特性：
*   **Load/Store 直接拷贝**：利用 CPU 的 Load/Store 指令直接操作显存，优化了零碎数据的传输性能，进一步降低了软硬交互延迟。

## 4. 性能表现
实验数据表明，Fleche 成功解决了延迟随任务数线性增长的问题：
*   **延迟稳定性**：随着 Kernel 个数（特征数量）的增加，Fleche 的处理延迟基本保持稳定，不再随 Kernel 数量线性上升。
*   **系统收益**：有效缓解了 GPU 片上内存软硬交互的开销，提升了整体查询效率。